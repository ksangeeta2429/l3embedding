{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Activation, Lambda\n",
    "import keras.regularizers as regularizers\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files(iterable):\n",
    "    lst = list(iterable)\n",
    "    random.shuffle(lst)\n",
    "    return iter(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_data_generator(data_dir, batch_size=512, random_state=None, start_batch_idx=None):\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "        \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        #shortlist_files.append(data_batch_path)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = np.load(data_batch_path)\n",
    "        blob_size = len(data_blob['db_mels'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['db_mels'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['db_mels'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    #saved audio files are already in float so need not convert to float32\n",
    "                    X = [batch[i] for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    return batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None\n",
    "\n",
    "def single_epoch_test_data_generator(file_list, batch_size=64, start_batch_idx=None):\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    for fname in file_list:\n",
    "        data_batch_path = fname\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = np.load(data_batch_path)\n",
    "        blob_size = len(data_blob['db_mels'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['db_mels'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['db_mels'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    X = [batch[i] for i in range(batch_size)]\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_keras_to_tflite(tflite_model_file, keras_model, keras_model_path, quant_mode='default', quantized_input=True,\\\n",
    "                            halved_convs=False, calibrate_data_dir=None, num_calibration_steps=1024):\n",
    "\n",
    "    def representative_dataset_gen():\n",
    "            print('Calibrating.........')\n",
    "            for _ in range(num_calibration_steps):\n",
    "                x = quant_data_generator(calibrate_data_dir, batch_size=1)\n",
    "                yield [np.array(x).astype(np.float32)]\n",
    "    \n",
    "    tf_version = tf.__version__.split('.')[0]\n",
    "    \n",
    "    if tf_version == '2':\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    else:\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\n",
    "    \n",
    "    if quant_mode == 'default':\n",
    "        if calibrate_data_dir is None:\n",
    "            raise ValueError('Quantized activation calibration needs data directory!')\n",
    "        \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        print('quantized_input={}'.format(quantized_input))\n",
    "        if quantized_input:\n",
    "            #converter.inference_input_type = tf.int8\n",
    "            converter.inference_output_type = tf.int8\n",
    "        #converter.default_ranges_stats = (-127, 128)\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "                \n",
    "    elif quant_mode == 'size':\n",
    "        converter.post_training_quantize = True\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_model_file, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print('Tflite model saved in:', tflite_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(model_path, calibrate_data_dir, quant_mode='default', quantized_input=True, \\\n",
    "                               halved_convs=False, flatten=False, calibration_steps=1024):\n",
    "    \n",
    "    #1. Convert l3model to keras model for quantization (with maxpooling layer but flatten removed)\n",
    "    dir_prefix = '/scratch/sk7898/quantization/' + os.path.basename(model_path).strip('.h5')\n",
    "    \n",
    "    if not os.path.isdir(dir_prefix):\n",
    "        os.makedirs(dir_prefix)\n",
    "    \n",
    "    tf_version = tf.__version__.split('.')[0]\n",
    "    \n",
    "    if tf_version=='2':\n",
    "        keras_model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        keras_model = keras.models.load_model(model_path)\n",
    "    #print(keras_model.summary())\n",
    "    \n",
    "    #2.1 Convert keras to tflite model\n",
    "    #2.2 Quantize model with mode 'default' for only weights quantization or 'size' for full quantization\n",
    "    #2.3 Save the quantized tflite model\n",
    "    \n",
    "    print('Quantizing keras model and saving as tflite')\n",
    "    input_type = '_float32'\n",
    "    tflite_model_file = os.path.join(dir_prefix, 'tf_' + str(tf_version) + '_full_quantized_'+ quant_mode + input_type + '.tflite')\n",
    "    \n",
    "    quantize_keras_to_tflite(tflite_model_file, keras_model, model_path, quant_mode=quant_mode,\\\n",
    "                             quantized_input=quantized_input, halved_convs=halved_convs, \\\n",
    "                             calibrate_data_dir=calibrate_data_dir, num_calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantize both the weights and the activations of the model**\\\n",
    "If the input is already in int8, set quantized_input = True\\\n",
    "If tflite should convert the float32 to int8 by adding a Quantize layer, quantized_input = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Quantizing keras model and saving as tflite\n",
      "quantized_input=True\n",
      "Calibrating.........\n",
      "04_002755.npz\n",
      "04_002755.npz\n",
      "01_001297.npz\n",
      "03_000965.npz\n",
      "01_001297.npz\n",
      "01_001297.npz\n",
      "03_000965.npz\n",
      "03_000965.npz\n",
      "03_000965.npz\n",
      "01_001297.npz\n",
      "04_002755.npz\n",
      "01_001297.npz\n",
      "03_000965.npz\n",
      "01_001297.npz\n",
      "03_000965.npz\n",
      "01_001297.npz\n",
      "04_002755.npz\n",
      "03_000965.npz\n",
      "04_002755.npz\n",
      "01_001297.npz\n",
      "03_000965.npz\n",
      "01_001297.npz\n",
      "01_001297.npz\n",
      "04_002755.npz\n",
      "01_001297.npz\n",
      "04_002755.npz\n",
      "04_002755.npz\n",
      "03_000965.npz\n",
      "01_001297.npz\n",
      "01_001297.npz\n",
      "03_000965.npz\n",
      "01_001297.npz\n",
      "Tflite model saved in: /scratch/sk7898/quantization/pipeline_cmsis_mels/tf_2_full_quantized_default_float32.tflite\n"
     ]
    }
   ],
   "source": [
    "#model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "#model_path = '/scratch/dr2915/l3pruning/embedding/fixed/reduced_input/l3_audio_20191108201753_8000_64_160_1024_half.h5'\n",
    "model_path = '/scratch/dr2915/Nathan/pipeline_cmsis_mels.h5'\n",
    "calibrate_data_dir = '/scratch/sk7898/cmsis_ml_data' #'/beegfs/dr2915/sonyc_ust/frames/8KHz'\n",
    "calibration_steps = 32\n",
    "\n",
    "quant_mode='default'\n",
    "flatten=True\n",
    "quantized_input=True\n",
    "halved_convs=True if 'half' in model_path else False\n",
    "\n",
    "post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode, quantized_input=quantized_input,\\\n",
    "                           halved_convs=halved_convs, flatten=flatten, calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input/Output of tflite model (Interpreter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "{'name': 'input_1', 'index': 45, 'shape': array([ 1, 64, 51,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'Identity', 'index': 46, 'shape': array([1, 8], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "output_path = '/scratch/sk7898/quantization'\n",
    "quant_model = 'pipeline_cmsis_mels/tf_2_full_quantized_default_float32.tflite'\n",
    "quant_output_path = os.path.join(output_path, quant_model)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(quant_output_path))\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "output_shape = output_details[0]['shape'][1:]\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Embedding from the tflite model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softmax_batch_from_tflite(data_gen, tflite_model_file, batch_size, classes=8):\n",
    "    \n",
    "    predictions = []\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "\n",
    "    interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "       \n",
    "    #predictions per batch   \n",
    "    for idx, batch_x in enumerate(data_gen):\n",
    "        x = np.array(batch_x).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        predictions.append(output)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_softmax(tflite_model_file, file_list, batch_size=64):\n",
    "    \n",
    "    output = None\n",
    "    classes = 8\n",
    "    print('Getting softmax output for downstream classes out of Quantized tflite model')\n",
    "    \n",
    "    data_gen = single_epoch_test_data_generator(file_list, batch_size=batch_size)\n",
    "\n",
    "    output = get_softmax_batch_from_tflite(data_gen, tflite_model_file, batch_size, classes=classes)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_files(data_dir, num_files=10):\n",
    "    shortlist_files = []\n",
    "    random.seed(23455)\n",
    "    \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        shortlist_files.append(data_batch_path)\n",
    "        if len(shortlist_files) >= num_files:\n",
    "            break\n",
    "    return shortlist_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'selected_audio_files_cmsis_mel.npz'\n",
    "tflite_model_file = '/scratch/sk7898/quantization/pipeline_cmsis_mels/tf_2_full_quantized_default_float32.tflite'\n",
    "\n",
    "test_data_dir = '/beegfs/dr2915/sonyc_ust/db_mels/test'\n",
    "shortlist_files = get_test_files(test_data_dir)\n",
    "np.savez(out_file, x=shortlist_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/beegfs/dr2915/sonyc_ust/db_mels/test/20_010340.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/00_010346.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/20_010593.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/06_010465.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/27_010536.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/10_010403.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/16_010802.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/34_010422.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/20_010651.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/db_mels/test/16_010377.npz']\n"
     ]
    }
   ],
   "source": [
    "files = np.load(out_file)\n",
    "shortlist_files = files['x']\n",
    "print(shortlist_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting softmax output for downstream classes out of Quantized tflite model\n",
      "== Input details ==\n",
      "{'name': 'input_1', 'index': 45, 'shape': array([91, 64, 51,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'Identity', 'index': 46, 'shape': array([91,  8], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "output = gen_softmax(tflite_model_file, shortlist_files, batch_size=91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 3 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 0 3 3 3 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 3 0 3 0 0 0 0 0 0 3 0 0 0 3 0 3 3 3 3 3\n",
      " 3 3 3 3 3 3 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 6 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 4 1 4 4 1 1 0 1 1 1 1 1 1 1 1 0 0 0 4 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 2 6 6 2 6 6 6 6 6 6 6 6 6 6 2 6 6 6 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 5 2\n",
      " 5 1 1 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 6 6 6 6 6 0\n",
      " 0 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 2 2 1 2 2 1 6 6 6 6 6 6 6 6 0 6 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 6 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 6 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 6 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 6 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 6 6 6 6 1 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "pred = np.array(output).reshape(-1, 8)\n",
    "pred_max = np.argmax(pred, axis=-1) \n",
    "print(pred_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
