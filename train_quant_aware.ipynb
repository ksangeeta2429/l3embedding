{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import getpass\n",
    "import git\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "import pescador\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import librosa\n",
    "from keras import backend as K\n",
    "from skimage import img_as_float\n",
    "from keras import activations\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from l3embedding.training_utils import conv_dict_to_val_list, multi_gpu_model, MultiGPUCheckpointCallback\n",
    "from l3embedding.model import *\n",
    "from l3embedding.audio import pcm2float\n",
    "from log import *\n",
    "from kapre.time_frequency import Spectrogram, Melspectrogram\n",
    "from resampy import resample\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "LOGGER = logging.getLogger('l3embedding')\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Keras callback to record loss history\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outfile):\n",
    "        super().__init__()\n",
    "        self.outfile = outfile\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    # def on_batch_end(self, batch, logs={}):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "\n",
    "        loss_dict = {'loss': self.loss, 'val_loss': self.val_loss}\n",
    "        with open(self.outfile, 'wb') as fp:\n",
    "            pickle.dump(loss_dict, fp)\n",
    "\n",
    "class GSheetLogger(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Keras callback to update Google Sheets Spreadsheet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, google_dev_app_name, spreadsheet_id, param_dict):\n",
    "        super(GSheetLogger).__init__()\n",
    "        self.google_dev_app_name = google_dev_app_name\n",
    "        self.spreadsheet_id = spreadsheet_id\n",
    "        self.credentials = get_credentials(google_dev_app_name)\n",
    "        self.service = discovery.build('sheets', 'v4', credentials=self.credentials)\n",
    "        self.param_dict = copy.deepcopy(param_dict)\n",
    "\n",
    "        row_num = get_row(self.service, self.spreadsheet_id, self.param_dict, 'embedding')\n",
    "        if row_num is None:\n",
    "            append_row(self.service, self.spreadsheet_id, self.param_dict, 'embedding')\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.best_train_loss = float('inf')\n",
    "        self.best_valid_loss = float('inf')\n",
    "        self.best_train_acc = float('-inf')\n",
    "        self.best_valid_acc = float('-inf')\n",
    "\n",
    "    # def on_batch_end(self, batch, logs={}):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        latest_epoch = epoch\n",
    "        latest_train_loss = logs.get('loss')\n",
    "        latest_valid_loss = logs.get('val_loss')\n",
    "        latest_train_acc = logs.get('acc')\n",
    "        latest_valid_acc = logs.get('val_acc')\n",
    "\n",
    "        if latest_train_loss < self.best_train_loss:\n",
    "            self.best_train_loss = latest_train_loss\n",
    "        if latest_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = latest_valid_loss\n",
    "        if latest_train_acc > self.best_train_acc:\n",
    "            self.best_train_acc = latest_train_acc\n",
    "        if latest_valid_acc > self.best_valid_acc:\n",
    "            self.best_valid_acc = latest_valid_acc\n",
    "\n",
    "        values = [\n",
    "            latest_epoch, latest_train_loss, latest_valid_loss,\n",
    "            latest_train_acc, latest_valid_acc, self.best_train_loss,\n",
    "            self.best_valid_loss, self.best_train_acc, self.best_valid_acc]\n",
    "\n",
    "        update_experiment(self.service, self.spreadsheet_id, self.param_dict, 'V', 'AD', values, 'embedding')\n",
    "                          #'R', 'Z', values, 'embedding')\n",
    "\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Keras callback to log epoch and batch running time\n",
    "    \"\"\"\n",
    "    # Copied from https://stackoverflow.com/a/43186440/1260544\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_times = []\n",
    "        self.batch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs=None):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        t = time.time() - self.epoch_time_start\n",
    "        LOGGER.info('Epoch took {} seconds'.format(t))\n",
    "        self.epoch_times.append(t)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.batch_time_start = time.time()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        t = time.time() - self.batch_time_start\n",
    "        LOGGER.info('Batch took {} seconds'.format(t))\n",
    "        self.batch_times.append(t)\n",
    "\n",
    "def get_restart_info(history_path):\n",
    "    last = None\n",
    "    with open(history_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            last = row\n",
    "\n",
    "    return int(last['epoch']), float(last['val_acc']), float(last['val_loss'])\n",
    "\n",
    "def initialize_uninitialized_variables(sess):\n",
    "    if hasattr(tf, 'global_variables'):\n",
    "        variables = tf.global_variables()\n",
    "    else:\n",
    "        variables = tf.all_variables()\n",
    "\n",
    "    #print(variables)\n",
    "    uninitialized_variables = []\n",
    "    for v in variables:\n",
    "        if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n",
    "            uninitialized_variables.append(v)\n",
    "            v._keras_initialized = True\n",
    "    \n",
    "    #print(uninitialized_variables)\n",
    "    if uninitialized_variables:\n",
    "        if hasattr(tf, 'variables_initializer'):\n",
    "            sess.run(tf.variables_initializer(uninitialized_variables))\n",
    "        else:\n",
    "            sess.run(tf.initialize_variables(uninitialized_variables)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cnn_L3_melspec2_spec_model(n_mels=64, n_hop=160, n_dft=1024, fmax=None,\n",
    "                                         asr = 8000, halved_convs=True, audio_window_dur=1):\n",
    "    \"\"\"\n",
    "    Constructs a model that replicates the audio subnetwork  used in Look,\n",
    "    Listen and Learn\n",
    "    Relja Arandjelovic and (2017). Look, Listen and Learn. CoRR, abs/1705.08168, .\n",
    "    Returns\n",
    "    -------\n",
    "    model:  L3 CNN model\n",
    "            (Type: keras.models.Model)\n",
    "    inputs: Model inputs\n",
    "            (Type: list[keras.layers.Input])\n",
    "    outputs: Model outputs\n",
    "            (Type: keras.layers.Layer)\n",
    "    \"\"\"\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    n_frames = 1 + int((asr * audio_window_dur) / float(n_hop))\n",
    "    x_a = Input(shape=(n_mels, n_frames, 1), dtype='float32')\n",
    "    y_a = BatchNormalization()(x_a)\n",
    "\n",
    "    # CONV BLOCK 1\n",
    "    n_filter_a_1 = 64\n",
    "    if halved_convs:\n",
    "        n_filter_a_1 //= 2\n",
    "\n",
    "    filt_size_a_1 = (3, 3)\n",
    "    pool_size_a_1 = (2, 2)\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_1, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 2\n",
    "    n_filter_a_2 = 128\n",
    "    if halved_convs:\n",
    "        n_filter_a_2 //= 2\n",
    "\n",
    "    filt_size_a_2 = (3, 3)\n",
    "    pool_size_a_2 = (2, 2)\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_2, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 3\n",
    "    n_filter_a_3 = 256\n",
    "    if halved_convs:\n",
    "        n_filter_a_3 //= 2\n",
    "\n",
    "    filt_size_a_3 = (3, 3)\n",
    "    pool_size_a_3 = (2, 2)\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_3, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 4\n",
    "    n_filter_a_4 = 512\n",
    "    if halved_convs:\n",
    "        n_filter_a_4 //= 2\n",
    "\n",
    "    filt_size_a_4 = (3, 3)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4,\n",
    "                 kernel_initializer='he_normal',\n",
    "                 name='audio_embedding_layer', padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    \n",
    "    m = Model(inputs=x_a, outputs=y_a)\n",
    "    m.name = 'audio_model'\n",
    "\n",
    "    return m, x_a, y_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_shuffle(iterable, shuffle=True):\n",
    "    lst = list(iterable)\n",
    "    while True:\n",
    "        yield from lst\n",
    "        if shuffle:\n",
    "            random.shuffle(lst)\n",
    "\n",
    "def data_generator(data_dir, batch_size=512, random_state=20180123, samp_rate=48000,\n",
    "                   start_batch_idx=None, keys=None):\n",
    "    random.seed(random_state)\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    # Limit keys to avoid producing batches with all of the metadata fields\n",
    "    if not keys:\n",
    "        keys = ['audio', 'video', 'label']\n",
    "\n",
    "    for fname in cycle_shuffle(os.listdir(data_dir)):\n",
    "        batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        blob = h5py.File(batch_path, 'r')\n",
    "        blob_size = len(blob['label'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = {k:blob[k][blob_start_idx:blob_end_idx]\n",
    "                             for k in keys}\n",
    "                else:\n",
    "                    for k in keys:\n",
    "                        batch[k] = np.concatenate([batch[k],\n",
    "                                                   blob[k][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Preprocess video so samples are in [-1,1]\n",
    "                    batch['video'] = 2 * img_as_float(batch['video']).astype('float32') - 1\n",
    "\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch['audio'] = pcm2float(batch['audio'], dtype='float32')\n",
    "                    else:\n",
    "                        batch['audio'] = resample(pcm2float(batch['audio'], dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "                    #print('Shape of audio batch:', np.shape(batch['audio']))\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None\n",
    "\n",
    "\n",
    "def single_epoch_data_generator(data_dir, epoch_size, **kwargs):\n",
    "    while True:\n",
    "        data_gen = data_generator(data_dir, **kwargs)\n",
    "        for idx, item in enumerate(data_gen):\n",
    "            yield item\n",
    "            # Once we generate all batches for an epoch, restart the generator\n",
    "            if (idx + 1) == epoch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_save_quantized_model(model_path, output_dir):\n",
    "    \n",
    "    K.clear_session()\n",
    "    output_path = os.path.join(output_dir, 'frozen_model_quant.pb')\n",
    "    eval_graph = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True    \n",
    "    eval_sess = tf.Session(config=config, graph=eval_graph)\n",
    "    \n",
    "    K.set_session(eval_sess)\n",
    "    \n",
    "    with eval_graph.as_default():\n",
    "        K.set_learning_phase(0)\n",
    "        eval_model = keras.models.load_model(model_path)\n",
    "        #print(eval_model.summary())\n",
    "        \n",
    "        tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\n",
    "        eval_sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        eval_graph_def = eval_graph.as_graph_def()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(eval_sess, os.path.join(output_dir, os.path.basename(output_dir)))\n",
    "\n",
    "        print(eval_model.input.op.name)\n",
    "        print(eval_model.output.op.name)\n",
    "        \n",
    "        frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "                                                                        eval_sess,\n",
    "                                                                        eval_graph_def,\n",
    "                                                                        [eval_model.output.op.name]\n",
    "                                                                        )\n",
    "\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_dir, validation_data_dir, output_dir, num_epochs=1, \n",
    "          train_epoch_size=512, validation_epoch_size=1024, train_batch_size=64,\n",
    "          validation_batch_size=64, model_type='cnn_L3_melspec2', random_state=20180123,\n",
    "          learning_rate=1e-4, verbose=False, checkpoint_interval=10, n_mels=64, n_hop=160, n_dft=1024,\n",
    "          samp_rate=8000, fmax=None, halved_convs=True, log_path=None, disable_logging=False, \n",
    "          gpus=1, continue_model_dir=None, gsheet_id=None, google_dev_app_name=None):\n",
    "\n",
    "    init_console_logger(LOGGER, verbose=verbose)\n",
    "    if not disable_logging:\n",
    "        init_file_logger(LOGGER, log_path=log_path)\n",
    "    LOGGER.debug('Initialized logging.')\n",
    "\n",
    "    print('Halved?', halved_convs)\n",
    "\n",
    "    # Form model ID\n",
    "    data_subset_name = os.path.basename(train_data_dir)\n",
    "    data_subset_name = data_subset_name[:data_subset_name.rindex('_')]\n",
    "    model_id = os.path.join(data_subset_name, model_type + '_quant')\n",
    "\n",
    "    param_dict = {\n",
    "          'username': getpass.getuser(),\n",
    "          'train_data_dir': train_data_dir,\n",
    "          'validation_data_dir': validation_data_dir,\n",
    "          'model_id': model_id,\n",
    "          'output_dir': output_dir,\n",
    "          'samp_rate': int(samp_rate/1000),\n",
    "          'num_mels': n_mels,\n",
    "          'num_hops': int(np.ceil(samp_rate/n_hop)),\n",
    "          'num_dft': n_dft,\n",
    "          'num_epochs': num_epochs,\n",
    "          'train_epoch_size': train_epoch_size,\n",
    "          'validation_epoch_size': validation_epoch_size,\n",
    "          'train_batch_size': train_batch_size,\n",
    "          'validation_batch_size': validation_batch_size,\n",
    "          'model_type': model_type,\n",
    "          'random_state': random_state,\n",
    "          'learning_rate': learning_rate,\n",
    "          'verbose': verbose,\n",
    "          'checkpoint_interval': checkpoint_interval,\n",
    "          'log_path': log_path,\n",
    "          'disable_logging': disable_logging,\n",
    "          'gpus': gpus,\n",
    "          'continue_model_dir': continue_model_dir,\n",
    "          'git_commit': '', #git.Repo(os.path.dirname(os.path.abspath(__file__)), search_parent_directories=True).head.object.hexsha,\n",
    "          'gsheet_id': gsheet_id,\n",
    "          'google_dev_app_name': google_dev_app_name\n",
    "    }\n",
    "    LOGGER.info('Training with the following arguments: {}'.format(param_dict))\n",
    "\n",
    "    # Make sure the directories we need exist\n",
    "    if continue_model_dir:\n",
    "        model_dir = continue_model_dir\n",
    "    else:\n",
    "        model_dir = os.path.join(output_dir, 'embedding', model_id, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    LOGGER.info('Model files can be found in \"{}\"'.format(model_dir))\n",
    "\n",
    "    param_dict['model_dir'] = model_dir\n",
    "    train_config_path = os.path.join(model_dir, 'config.json')\n",
    "    with open(train_config_path, 'w') as fd:\n",
    "        json.dump(param_dict, fd, indent=2)\n",
    "\n",
    "\n",
    "    param_dict.update({\n",
    "          'latest_epoch': '-',\n",
    "          'latest_train_loss': '-',\n",
    "          'latest_validation_loss': '-',\n",
    "          'latest_train_acc': '-',\n",
    "          'latest_validation_acc': '-',\n",
    "          'best_train_loss': '-',\n",
    "          'best_validation_loss': '-',\n",
    "          'best_train_acc': '-',\n",
    "          'best_validation_acc': '-',\n",
    "    })\n",
    "\n",
    "    latest_weight_path = os.path.join(model_dir, 'model_latest.h5')\n",
    "    best_valid_acc_weight_path = os.path.join(model_dir, 'model_best_valid_accuracy.h5')\n",
    "    best_valid_loss_weight_path = os.path.join(model_dir, 'model_best_valid_loss.h5')\n",
    "    checkpoint_weight_path = os.path.join(model_dir, 'model_checkpoint.{epoch:02d}.h5')\n",
    "\n",
    "    # Load information about last epoch for initializing callbacks and data generators\n",
    "    if continue_model_dir is not None:\n",
    "        prev_train_hist_path = os.path.join(continue_model_dir, 'history_csvlog.csv')\n",
    "        last_epoch_idx, last_val_acc, last_val_loss = get_restart_info(prev_train_hist_path)\n",
    "\n",
    "    LOGGER.info('Setting up train data generator...')\n",
    "    if continue_model_dir is not None:\n",
    "        train_start_batch_idx = train_epoch_size * (last_epoch_idx + 1)\n",
    "    else:\n",
    "        train_start_batch_idx = None\n",
    "\n",
    "    train_gen = data_generator(\n",
    "        train_data_dir,\n",
    "        batch_size=train_batch_size,\n",
    "        random_state=random_state,\n",
    "        start_batch_idx=train_start_batch_idx,\n",
    "        samp_rate=samp_rate)\n",
    "\n",
    "    train_gen = pescador.maps.keras_tuples(train_gen,\n",
    "                                           ['video', 'audio'],\n",
    "                                           'label')\n",
    "\n",
    "    LOGGER.info('Setting up validation data generator...')\n",
    "    val_gen = single_epoch_data_generator(\n",
    "        validation_data_dir,\n",
    "        validation_epoch_size,\n",
    "        batch_size=validation_batch_size,\n",
    "        random_state=random_state,\n",
    "        samp_rate=samp_rate)\n",
    "\n",
    "    val_gen = pescador.maps.keras_tuples(val_gen,\n",
    "                                         ['video', 'audio'],\n",
    "                                         'label')\n",
    "\n",
    "    # Fit the model\n",
    "    LOGGER.info('Fitting model...')\n",
    "    if verbose:\n",
    "        verbosity = 1\n",
    "    else:\n",
    "        verbosity = 2\n",
    "\n",
    "    if continue_model_dir is not None:\n",
    "        initial_epoch = last_epoch_idx + 1\n",
    "    else:\n",
    "        initial_epoch = 0\n",
    "\n",
    "    K.clear_session()\n",
    "    \n",
    "    #train graph\n",
    "    train_graph = tf.Graph()\n",
    "    train_sess = tf.Session(config=config, graph=train_graph)\n",
    "    K.set_session(train_sess)\n",
    "    \n",
    "    with train_graph.as_default():       \n",
    "        if continue_model_dir:\n",
    "            latest_model_path = os.path.join(continue_model_dir, 'model_latest.h5')\n",
    "            m = keras.models.load_model(latest_model_path)\n",
    "        else:\n",
    "            m, inputs, outputs = construct_cnn_L3_melspec2_spec_model(n_mels=n_mels,\n",
    "                                                                      n_hop=n_hop, \n",
    "                                                                      n_dft=n_dft, \n",
    "                                                                      asr=samp_rate,\n",
    "                                                                      fmax=fmax, \n",
    "                                                                      halved_convs=halved_convs,\n",
    "                                                                      audio_window_dur=1)\n",
    "       \n",
    "        # Save the model\n",
    "        model_spec_path = os.path.join(model_dir, 'model_spec.pkl')\n",
    "        model_spec = keras.utils.serialize_keras_object(m)\n",
    "        with open(model_spec_path, 'wb') as fd:\n",
    "            pickle.dump(model_spec, fd)\n",
    "            \n",
    "        model_json_path = os.path.join(model_dir, 'model.json')\n",
    "        model_json = m.to_json()\n",
    "        with open(model_json_path, 'w') as fd:\n",
    "            json.dump(model_json, fd, indent=2)\n",
    "            \n",
    "        # Set up callbacks\n",
    "        cb = []\n",
    "\n",
    "        cb.append(MultiGPUCheckpointCallback(latest_weight_path,\n",
    "                                             m,\n",
    "                                             save_weights_only=False,\n",
    "                                             verbose=1))\n",
    "\n",
    "        best_val_acc_cb = MultiGPUCheckpointCallback(best_valid_acc_weight_path,\n",
    "                                                     m,\n",
    "                                                     save_weights_only=False,\\\n",
    "                                                     save_best_only=True,\\\n",
    "                                                     verbose=1,\\\n",
    "                                                     monitor='val_acc')\n",
    "            \n",
    "        if continue_model_dir is not None:\n",
    "            best_val_acc_cb.best = last_val_acc\n",
    "        cb.append(best_val_acc_cb)\n",
    "\n",
    "        # Callback for multi-gpu model\n",
    "        best_val_loss_cb = MultiGPUCheckpointCallback(best_valid_loss_weight_path,\n",
    "                                                      m,\n",
    "                                                      save_weights_only=False,\n",
    "                                                      save_best_only=True,\n",
    "                                                      verbose=1,\n",
    "                                                      monitor='val_loss')\n",
    "            \n",
    "        if continue_model_dir is not None:\n",
    "            best_val_loss_cb.best = last_val_loss\n",
    "        cb.append(best_val_loss_cb)\n",
    "\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(checkpoint_weight_path,\n",
    "                                                        save_weights_only=True,\n",
    "                                                        period=checkpoint_interval)\n",
    "        \n",
    "        if continue_model_dir is not None:\n",
    "            checkpoint_cb.epochs_since_last_save = (last_epoch_idx + 1) % checkpoint_interval\n",
    "        cb.append(checkpoint_cb)\n",
    "\n",
    "        if gsheet_id:\n",
    "            cb.append(GSheetLogger(google_dev_app_name, gsheet_id, param_dict))\n",
    "        \n",
    "        timer_cb = TimeHistory()\n",
    "        cb.append(timer_cb)\n",
    "\n",
    "        history_checkpoint = os.path.join(model_dir, 'history_checkpoint.pkl')\n",
    "        cb.append(LossHistory(history_checkpoint))\n",
    "\n",
    "        history_csvlog = os.path.join(model_dir, 'history_csvlog.csv')\n",
    "        cb.append(keras.callbacks.CSVLogger(history_csvlog, append=True,\n",
    "                                            separator=','))\n",
    "\n",
    "        earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10)\n",
    "        reduceLR = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "        cb.append(earlyStopping)\n",
    "        cb.append(reduceLR)\n",
    "        \n",
    "        #Convert the base (single-GPU) model to Multi-GPU model\n",
    "        if gpus == 1:\n",
    "            model = m\n",
    "        else:\n",
    "            model = multi_gpu_model(m, gpus=gpus)\n",
    "            \n",
    "        loss = 'categorical_crossentropy'\n",
    "        metrics = ['accuracy']\n",
    "    \n",
    "        LOGGER.info('Compiling model...')\n",
    "        model.compile(Adam(lr=learning_rate), loss=loss, metrics=metrics)\n",
    "        \n",
    "        tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=50)\n",
    "        initialize_uninitialized_variables(train_sess)\n",
    "        \n",
    "        history = model.fit_generator(train_gen, train_epoch_size, num_epochs,\n",
    "                                      validation_data=val_gen,\n",
    "                                      validation_steps=validation_epoch_size,\n",
    "                                      callbacks=cb,\n",
    "                                      verbose=verbosity,\n",
    "                                      initial_epoch=initial_epoch)\n",
    "\n",
    "        #save graph and checkpoints\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(train_sess, save_path=os.path.join(model_dir, os.path.basename(model_dir)))\n",
    "\n",
    "        LOGGER.info('Done training. Saving results to disk...')\n",
    "        # Save history\n",
    "        with open(os.path.join(model_dir, 'history.pkl'), 'wb') as fd:\n",
    "            pickle.dump(history.history, fd)\n",
    "\n",
    "        restore_save_quantized_model(best_valid_loss_weight_path, model_dir)\n",
    "        \n",
    "    LOGGER.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:l3embedding:Initialized logging.\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,510 - l3embedding - INFO - Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "INFO:l3embedding:Training with the following arguments: {'username': 'sk7898', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'model_id': 'music/cnn_L3_melspec2_quant', 'output_dir': '/scratch/sk7898/models/reduced_input', 'samp_rate': 8, 'num_mels': 64, 'num_hops': 50, 'num_dft': 1024, 'num_epochs': 1, 'train_epoch_size': 512, 'validation_epoch_size': 1024, 'train_batch_size': 64, 'validation_batch_size': 64, 'model_type': 'cnn_L3_melspec2', 'random_state': 20180123, 'learning_rate': 0.0001, 'verbose': False, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None, 'git_commit': '', 'gsheet_id': None, 'google_dev_app_name': 'l3compression'}\n",
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "2020-02-20 17:43:51,517 - l3embedding - INFO - Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "INFO:l3embedding:Model files can be found in \"/scratch/sk7898/models/reduced_input/embedding/music/cnn_L3_melspec2_quant/20200220174351\"\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "2020-02-20 17:43:51,523 - l3embedding - INFO - Setting up train data generator...\n",
      "INFO:l3embedding:Setting up train data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "2020-02-20 17:43:51,527 - l3embedding - INFO - Setting up validation data generator...\n",
      "INFO:l3embedding:Setting up validation data generator...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "2020-02-20 17:43:51,532 - l3embedding - INFO - Fitting model...\n",
      "INFO:l3embedding:Fitting model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halved? True\n",
      "tracking <tf.Variable 'melspectrogram_1/real_kernels:0' shape=(1024, 1, 1, 513) dtype=float32> dft_real_kernels\n",
      "tracking <tf.Variable 'melspectrogram_1/imag_kernels:0' shape=(1024, 1, 1, 513) dtype=float32> dft_imag_kernels\n",
      "tracking <tf.Variable 'melspectrogram_1/Variable:0' shape=(513, 64) dtype=float32> freq2mel\n",
      "<bound method Network.summary of <keras.engine.training.Model object at 0x2b84c4933b00>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "2020-02-20 17:43:53,975 - l3embedding - INFO - Compiling model...\n",
      "INFO:l3embedding:Compiling model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_as_float' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8a0e56c69db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgsheet_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsheet_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgsheet_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoogle_dev_app_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgoogle_app\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-f038f35628b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data_dir, validation_data_dir, output_dir, num_epochs, train_epoch_size, validation_epoch_size, train_batch_size, validation_batch_size, model_type, random_state, learning_rate, verbose, checkpoint_interval, n_mels, n_hop, n_dft, samp_rate, fmax, halved_convs, log_path, disable_logging, gpus, continue_model_dir, gsheet_id, google_dev_app_name)\u001b[0m\n\u001b[1;32m    234\u001b[0m                                       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                                       initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m#save graph and checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    740\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \"\"\"\n\u001b[0;32m--> 650\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/pescador/maps.py\u001b[0m in \u001b[0;36mkeras_tuples\u001b[0;34m(stream, inputs, outputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m                             '`inputs` or `outputs`')\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-40f3a2fa0080>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(data_dir, batch_size, random_state, samp_rate, start_batch_idx, keys)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstart_batch_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mstart_batch_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;31m# Preprocess video so samples are in [-1,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg_as_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;31m# Convert audio to float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_as_float' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    train_data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "    validation_data_dir = '/beegfs/work/AudioSetSamples/music_valid'\n",
    "    output_dir = '/scratch/sk7898/models/reduced_input'\n",
    "    google_app = 'l3compression'\n",
    "    gsheet_id = None\n",
    "\n",
    "    train(train_data_dir, validation_data_dir, output_dir, gsheet_id=gsheet_id, google_dev_app_name=google_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
