{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://gist.github.com/crypt3lx2k/cec6ad66b948fe0e77a7b1e6d2205bf4\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "import pescador\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from log import *\n",
    "import oyaml as yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_shuffle(iterable, shuffle=True):\n",
    "    lst = list(iterable)\n",
    "    while True:\n",
    "        yield from lst\n",
    "        if shuffle:\n",
    "            random.shuffle(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_targets(annotation_path, taxonomy_path):\n",
    "    \n",
    "    annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "    with open(taxonomy_path, 'r') as f:\n",
    "        taxonomy = yaml.load(f, Loader=yaml.Loader)\n",
    "    \n",
    "    coarse_target_labels = [\"_\".join([str(k), v])\n",
    "                            for k, v in taxonomy['coarse'].items()]\n",
    "         \n",
    "    file_list = annotation_data.sort_values('audio_filename')['audio_filename'].unique().tolist()\n",
    "    target_list = {os.path.basename(filename.replace('wav', 'npz')): [] for filename in file_list}\n",
    "    \n",
    "    for filename in file_list:\n",
    "        key = os.path.basename(filename.replace('wav', 'npz'))\n",
    "        file_df = annotation_data[annotation_data['audio_filename'] == filename]\n",
    "        target = []\n",
    "\n",
    "        for label in coarse_target_labels:\n",
    "            count = 0\n",
    "\n",
    "            for _, row in file_df.iterrows():\n",
    "                if int(row['annotator_id']) == 0:\n",
    "                    # If we have a validated annotation, just use that\n",
    "                    count = row[label + '_presence']\n",
    "                    break\n",
    "                else:\n",
    "                    count += row[label + '_presence']\n",
    "\n",
    "            if count > 0:\n",
    "                target.append(1.0)\n",
    "            else:\n",
    "                target.append(0.0)\n",
    "\n",
    "        target_list[key] = target\n",
    "\n",
    "    return target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir, target_list, batch_size=64, random_state=20180123, start_batch_idx=None):\n",
    "\n",
    "    random.seed(random_state)\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "        \n",
    "    for fname in cycle_shuffle(os.listdir(data_dir)):\n",
    "        data_path = os.path.join(data_dir, fname)\n",
    "\n",
    "        blob_start_idx = 0\n",
    "        data_blob = np.load(data_path)\n",
    "        mel_blob = data_blob['db_mels']\n",
    "        target_blob = np.array([target_list[fname] for _ in range(mel_blob.shape[0])])\n",
    "\n",
    "        blob_size = mel_blob.shape[0]\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = {'mel': mel_blob[blob_start_idx:blob_end_idx],\\\n",
    "                             'target': target_blob[blob_start_idx:blob_end_idx]}\n",
    "                else:\n",
    "                    batch['mel'] = np.concatenate([batch['mel'], mel_blob[blob_start_idx:blob_end_idx]])\n",
    "                    batch['target'] = np.concatenate([batch['target'], target_blob[blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    batch['mel'] = np.array(batch['mel'])[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(np.array(batch['target']))) (64, 8)\n",
    "                    #print(np.shape(batch['mel'])) #(64, 64, 51, 1)\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_data_generator(data_dir, target_list, epoch_size, **kwargs):\n",
    "    while True:\n",
    "        data_gen = data_generator(data_dir, target_list, **kwargs)\n",
    "        for idx, item in enumerate(data_gen):\n",
    "            yield item\n",
    "            if (idx + 1) == epoch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_variables(sess):\n",
    "    if hasattr(tf, 'global_variables'):\n",
    "        variables = tf.compat.v1.global_variables() #tf.global_variables()\n",
    "    else:\n",
    "        variables = tf.all_variables()\n",
    "\n",
    "    #print(variables)\n",
    "    uninitialized_variables = []\n",
    "    for v in variables:\n",
    "        if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n",
    "            uninitialized_variables.append(v)\n",
    "            v._keras_initialized = True\n",
    "    \n",
    "    #print(uninitialized_variables)\n",
    "    if uninitialized_variables:\n",
    "        if hasattr(tf, 'variables_initializer'):\n",
    "            sess.run(tf.compat.v1.variables_initializer(uninitialized_variables))\n",
    "        else:\n",
    "            sess.run(tf.compat.v1.initialize_variables(uninitialized_variables)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_save_quantized_model(model_path, output_dir):\n",
    "    \n",
    "    K.clear_session()\n",
    "    output_path = os.path.join(output_dir, 'frozen_pipeline_cmsis_mels_quant.pb')\n",
    "    eval_graph = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True    \n",
    "    eval_sess = tf.Session(config=config, graph=eval_graph)\n",
    "    \n",
    "    K.set_session(eval_sess)\n",
    "    \n",
    "    with eval_graph.as_default():\n",
    "        K.set_learning_phase(0)\n",
    "        eval_model = keras.models.load_model(model_path)\n",
    "        #print(eval_model.summary())\n",
    "        \n",
    "        tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\n",
    "        eval_sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        eval_graph_def = eval_graph.as_graph_def()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(eval_sess, os.path.join(output_dir, os.path.basename(output_dir)))\n",
    "\n",
    "        print(eval_model.input.op.name)\n",
    "        print(eval_model.output.op.name)\n",
    "        exit(0)\n",
    "        \n",
    "        frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "                                                                        eval_sess,\n",
    "                                                                        eval_graph_def,\n",
    "                                                                        [eval_model.output.op.name]\n",
    "                                                                        )\n",
    "\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quantized_model(model_path, train_dir, valid_dir, output_dir, target_list, steps_per_epoch, \\\n",
    "                          valid_steps_per_epoch, loss=None, num_epochs=100, patience=500,\\\n",
    "                          learning_rate=1e-4, optimizer='adam'):\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_dir = os.path.join(output_dir, timestamp)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    metrics = []\n",
    "    train_gen = data_generator(train_dir, target_list)\n",
    "    valid_gen = single_epoch_data_generator(valid_dir, target_list, valid_steps_per_epoch)\n",
    "\n",
    "    train_gen = pescador.maps.keras_tuples(train_gen, 'mel', 'target')\n",
    "    valid_gen = pescador.maps.keras_tuples(valid_gen, 'mel', 'target')\n",
    "    \n",
    "    K.clear_session()\n",
    "    #train graph\n",
    "    train_graph = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    train_sess = tf.Session(config=config, graph=train_graph)\n",
    "    K.set_session(train_sess)\n",
    "    \n",
    "    with train_graph.as_default():\n",
    "        # Set up callbacks\n",
    "        #K.set_learning_phase(1)\n",
    "        cb = []\n",
    "        # checkpoint\n",
    "        model_weight_file = os.path.join(output_dir, 'cmsis_quantized_model_best.h5')\n",
    "\n",
    "        cb.append(keras.callbacks.ModelCheckpoint(model_weight_file,\n",
    "                                                  save_weights_only=False,\n",
    "                                                  save_best_only=True,\n",
    "                                                  monitor='val_loss'))\n",
    "        # early stopping\n",
    "        cb.append(keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=patience))\n",
    "\n",
    "        # monitor losses\n",
    "        history_csv_file = os.path.join(output_dir, 'history.csv')\n",
    "        cb.append(keras.callbacks.CSVLogger(history_csv_file, append=True,\n",
    "                                            separator=','))\n",
    "\n",
    "        loss = 'binary_crossentropy'\n",
    "        opt = tf.train.AdamOptimizer(learning_rate) #keras.optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "        model = keras.models.load_model(model_path)\n",
    "        #print(model.summary())\n",
    "            \n",
    "        model.compile(opt, loss=loss)\n",
    "        tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=2)\n",
    "        initialize_uninitialized_variables(train_sess)\n",
    "\n",
    "        history = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch, epochs=num_epochs,\\\n",
    "                                      validation_steps=valid_steps_per_epoch,\\\n",
    "                                      validation_data=valid_gen, callbacks=cb, verbose=2)\n",
    "    \n",
    "        #save graph and checkpoints\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(train_sess, save_path=os.path.join(output_dir, os.path.basename(output_dir)))\n",
    "    \n",
    "        # Save history\n",
    "        with open(os.path.join(output_dir, 'history.pkl'), 'wb') as fd:\n",
    "            pickle.dump(history.history, fd)\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inserting fake quant op activation_Mul_quant after loss/urban_sound_classifier_loss/binary_crossentropy/logistic_loss/mul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inserting fake quant op activation_Mul_quant after loss/urban_sound_classifier_loss/binary_crossentropy/logistic_loss/mul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 5s - loss: 0.4825 - val_loss: 1.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      " - 3s - loss: 0.4704 - val_loss: 0.8700\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.4654 - val_loss: 0.6362\n",
      "Epoch 4/5\n",
      " - 3s - loss: 0.4137 - val_loss: 0.5302\n",
      "Epoch 5/5\n",
      " - 3s - loss: 0.4488 - val_loss: 0.5100\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    DATA_DIR = '/beegfs/dr2915/sonyc_ust'\n",
    "    MODEL_DIR = '/scratch/sk7898/quantization/pipeline_cmsis/'\n",
    "    annotation_path = os.path.join(DATA_DIR, 'annotations.csv')\n",
    "    taxonomy_path = os.path.join(DATA_DIR, 'dcase-ust-taxonomy.yaml')\n",
    "    \n",
    "    model_path = os.path.join(MODEL_DIR, 'pipeline_cmsis_mels.h5')\n",
    "    train_data_dir = os.path.join(DATA_DIR, 'db_mels/train')\n",
    "    validation_data_dir = os.path.join(DATA_DIR, 'db_mels/validate')\n",
    "    output_dir = MODEL_DIR\n",
    "    \n",
    "    target_list = get_file_targets(annotation_path, taxonomy_path)\n",
    "    #print(target_list)\n",
    "    \n",
    "    batch_size = 64\n",
    "    steps_per_epoch = int(np.ceil(len(os.listdir(train_data_dir)) / batch_size))\n",
    "    valid_steps_per_epoch = int(np.ceil(len(os.listdir(validation_data_dir)) / batch_size))\n",
    "    #print(steps_per_epoch)\n",
    "    #print(valid_steps_per_epoch)\n",
    "    \n",
    "    train_quantized_model(model_path, train_data_dir, validation_data_dir, output_dir, target_list, \\\n",
    "                          steps_per_epoch, valid_steps_per_epoch, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /scratch/sk7898/quantization/pipeline_cmsis/20200219230027/20200219230027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /scratch/sk7898/quantization/pipeline_cmsis/20200219230027/20200219230027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "urban_sound_classifier/output/Sigmoid\n",
      "INFO:tensorflow:Froze 98 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 98 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 98 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 98 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/scratch/sk7898/quantization/pipeline_cmsis/20200219230027'\n",
    "quantize_trained_model = os.path.join(output_dir, 'cmsis_quantized_model_best.h5')\n",
    "restore_save_quantized_model(quantize_trained_model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False\n",
    "if test:\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    DATA_DIR = '/beegfs/dr2915/sonyc_ust'\n",
    "    train_data_dir = os.path.join(DATA_DIR, 'db_mels/train')\n",
    "    for fname in os.listdir(train_data_dir)[0:1]:\n",
    "        data_path = os.path.join(train_data_dir, fname)\n",
    "        data_blob = np.load(data_path)\n",
    "        mel_blob = data_blob['db_mels']\n",
    "        target_blob = np.array([target_list[fname] for _ in range(mel_blob.shape[0])])\n",
    "        print(target_blob.shape)\n",
    "\n",
    "    # import the graph from the file\n",
    "    imported_graph = tf.train.import_meta_graph(os.path.join(output_dir, os.path.basename(output_dir) +'.meta'))\n",
    "\n",
    "    # list all the tensors in the graph\n",
    "    for tensor in tf.get_default_graph().get_operations():\n",
    "        print(tensor.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
