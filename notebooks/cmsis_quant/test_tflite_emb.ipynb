{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import resampy\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "LOGGER = logging.getLogger('emb-gen-ust')\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr):\n",
    "    \"\"\"\n",
    "    Load audio file\n",
    "    \"\"\"\n",
    "    data, sr_orig = sf.read(path, dtype='float32', always_2d=True)\n",
    "    data = data.mean(axis=-1)\n",
    "\n",
    "    if sr_orig != sr:\n",
    "        data = resampy.resample(data, sr_orig, sr)\n",
    "\n",
    "    return data\n",
    "\n",
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l3_frames_uniform_tflite(audio, interpreter, input_index, output_index, output_shape,\n",
    "                                 n_fft=2048, n_mels=256, mel_hop_length=242,\n",
    "                                 hop_size=0.1, sr=48000, fmax=None, embedding_length=256):\n",
    "    \n",
    "    if type(audio) == str:\n",
    "        #audio_basename = os.path.basename(os.path.splitext(audio)[0])\n",
    "        audio = load_audio(audio, sr)\n",
    "\n",
    "    hop_size = hop_size\n",
    "    hop_length = int(hop_size * sr)\n",
    "    frame_length = sr * 1\n",
    "\n",
    "    audio_length = len(audio)\n",
    "    if audio_length < frame_length:\n",
    "        # Make sure we can have at least one frame of audio\n",
    "        pad_length = frame_length - audio_length\n",
    "    else:\n",
    "        # Zero pad so we compute embedding on all samples\n",
    "        pad_length = int(np.ceil(audio_length - frame_length)/hop_length) * hop_length \\\n",
    "                     - (audio_length - frame_length)\n",
    "\n",
    "    if pad_length > 0:\n",
    "        # Use (roughly) symmetric padding\n",
    "        left_pad = pad_length // 2\n",
    "        right_pad= pad_length - left_pad\n",
    "        audio = np.pad(audio, (left_pad, right_pad), mode='constant')\n",
    "   \n",
    "    frames = librosa.util.utils.frame(audio, frame_length=frame_length, hop_length=hop_length).T\n",
    "    X = []\n",
    "    for frame in frames:\n",
    "        S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length,\\\n",
    "                                     window='hann', center=True,\\\n",
    "                                     pad_mode='constant'))\n",
    "        S = librosa.feature.melspectrogram(sr=sr, S=S, n_mels=n_mels, fmax=fmax,\n",
    "                                           power=1.0, htk=True)\n",
    "        S = amplitude_to_db(np.array(S))\n",
    "        X.append(S)\n",
    "   \n",
    "    predictions = np.empty((len(X), embedding_length), dtype=np.float32)\n",
    "    for idx in range(len(X)):\n",
    "        x = np.array(X[idx])[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        predictions[idx] = np.reshape(output, (output.shape[0], output.shape[-1]))\n",
    "        \n",
    "        #sample_data_path = os.path.join('/scratch/sk7898/temp_data', audio_basename + '_data_' + str(idx) + '.npz')\n",
    "        #sample_emb_path = os.path.join('/scratch/sk7898/temp_data', audio_basename + '_emb_' + str(idx) + '.npz')\n",
    "        #np.savez(sample_data_path, data=x)\n",
    "        #np.savez(sample_emb_path, embedding=predictions[idx])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonyc_ust_data(annotation_path, dataset_dir, output_dir, tflite_model_path, hop_size=0.1,\n",
    "                            n_fft=1024, n_mels=64, mel_hop_length=160, sr=8000, fmax=None):\n",
    "    \n",
    "    print(\"* Loading annotations.\")\n",
    "    annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df = annotation_data[['split', 'audio_filename']].drop_duplicates()\n",
    "    row_iter = df.iterrows()\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "    emb_len = output_shape[-1]\n",
    "    \n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    LOGGER.info('* Extracting embeddings.')\n",
    "\n",
    "    c = 0\n",
    "    for _, row in row_iter:\n",
    "        c = c + 1\n",
    "        filename = row['audio_filename']\n",
    "        split_str = row['split']\n",
    "        audio_path = os.path.join(dataset_dir, split_str, filename)\n",
    "        output_path = os.path.join(output_dir, os.path.splitext(filename)[0] + '.npz')\n",
    "\n",
    "        if c == 2:\n",
    "            return\n",
    "            \n",
    "        if not os.path.exists(audio_path):\n",
    "            LOGGER.info('Audio file {} doesn''t exist'.format(audio_path))\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            LOGGER.info('Output file {} already exists'.format(output_path))\n",
    "            return\n",
    "            \n",
    "        X = get_l3_frames_uniform_tflite(audio_path, interpreter, input_index, output_index,\n",
    "                                         output_shape, hop_size=hop_size, n_fft=n_fft,\n",
    "                                         n_mels=n_mels, mel_hop_length=mel_hop_length,\n",
    "                                         sr=sr, fmax=fmax, embedding_length=emb_len)\n",
    "        if X is None:\n",
    "            LOGGER.error('Could not generate data for {}'.format(audio_path))\n",
    "            return\n",
    "        \n",
    "        np.savez(output_path, embedding=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Loading annotations.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_dir = '/beegfs/dr2915/sonyc_ust'\n",
    "    out_prefix = '/scratch/sk7898/embeddings/features/sonyc_ust/l3'\n",
    "    model_des = 'l3_audio_20200304152812_8000_64_160_1024_half'\n",
    "    quant_mode = 'default_int8'  \n",
    "    annotation_path = os.path.join(data_dir, 'annotations.csv')\n",
    "    dataset_output_dir = os.path.join(out_prefix, model_des, quant_mode)\n",
    "    model_dir = os.path.join('/scratch/sk7898/quantization', model_des)\n",
    "    model_path = os.path.join(model_dir, 'quantized_'+ quant_mode + '.tflite')\n",
    "    \n",
    "    splits = model_des.split('_')\n",
    "    hop_size = 0.1\n",
    "    samp_rate = int(splits[3])\n",
    "    n_mels = int(splits[4])\n",
    "    mel_hop_length = int(splits[5])\n",
    "    n_fft = int(splits[6])\n",
    "    \n",
    "    generate_sonyc_ust_data(annotation_path=annotation_path, dataset_dir=data_dir, output_dir=dataset_output_dir,\\\n",
    "                            tflite_model_path=model_path, hop_size=hop_size, n_fft=n_fft, n_mels=n_mels,\\\n",
    "                            mel_hop_length=mel_hop_length, sr=samp_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.7597065  0.8136277  0.49196088 ... 1.9489222  1.4380397  1.2488239 ]]\n",
      "\n",
      " [[1.7029417  1.021765   0.39735317 ... 1.8921574  1.3812749  1.2488239 ]]\n",
      "\n",
      " [[1.7975496  1.0028435  0.2459805  ... 1.7218633  1.4380397  1.2109808 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.4758828  1.0596082  0.62441194 ... 1.8353927  1.6272554  1.3245102 ]]\n",
      "\n",
      " [[1.5704907  0.9460788  0.5108825  ... 1.8164711  1.4001966  1.6272554 ]]\n",
      "\n",
      " [[1.6083338  0.5298041  0.58656883 ... 1.9678438  1.4758828  1.6083338 ]]]\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(dataset_output_dir):\n",
    "    l = np.load(os.path.join(dataset_output_dir, f))\n",
    "    print(l['embedding'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
