{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/l3embedding\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%cd /scratch/sk7898/l3embedding\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Activation, Lambda\n",
    "import keras.regularizers as regularizers\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from l3embedding.model import MODELS, load_model\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir, batch_size=512, random_state=20180123, samp_rate=48000,\n",
    "                   start_batch_idx=None, keys=None):\n",
    "    \n",
    "    random.seed(random_state)\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    # Limit keys to avoid producing batches with all of the metadata fields\n",
    "    if not keys:\n",
    "        keys = ['audio', 'video', 'label']\n",
    "\n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        blob = h5py.File(batch_path, 'r')\n",
    "        blob_size = len(blob['label'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = {k:blob[k][blob_start_idx:blob_end_idx]\n",
    "                             for k in keys}\n",
    "                else:\n",
    "                    for k in keys:\n",
    "                        batch[k] = np.concatenate([batch[k],\n",
    "                                                   blob[k][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Preprocess video so samples are in [-1,1]\n",
    "                    batch['video'] = 2 * img_as_float(batch['video']).astype('float32') - 1\n",
    "\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch['audio'] = pcm2float(batch['audio'], dtype='float32')\n",
    "                    else:\n",
    "                        batch['audio'] = resample(pcm2float(batch['audio'], dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "                    #print('Shape of audio batch:', np.shape(batch['audio']))\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None\n",
    "\n",
    "\n",
    "def single_epoch_data_generator(data_dir, epoch_size, **kwargs):\n",
    "    while True:\n",
    "        data_gen = data_generator(data_dir, **kwargs)\n",
    "        for idx, item in enumerate(data_gen):\n",
    "            yield item\n",
    "            # Once we generate all batches for an epoch, restart the generator\n",
    "            if (idx + 1) == epoch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_keras_to_tflite(tflite_model_file, keras_model, keras_model_path, quant_mode='default',\n",
    "                             quantized_input=True, n_mels=256, n_hop=242, n_dft=2048, asr=48000, \n",
    "                             halved_convs=False, calibrate_data_dir=None, num_calibration_steps=1024, tf_version=2):\n",
    "\n",
    "    def representative_dataset_gen():\n",
    "            print('Calibrating.........')\n",
    "            for _ in range(num_calibration_steps):\n",
    "                data_gen = quant_data_generator(calibrate_data_dir, batch_size=1)\n",
    "                data_gen = pescador.maps.keras_tuples(data_gen,\n",
    "                                                       ['video', 'audio'],\n",
    "                                                       'label')\n",
    "                yield data_gen\n",
    "    \n",
    "    if tf_version == 2:\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    else:\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\n",
    "    \n",
    "    if quant_mode == 'default':\n",
    "        if calibrate_data_dir is None:\n",
    "            raise ValueError('Quantized activation calibration needs data directory!')\n",
    "        \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        if quantized_input:\n",
    "            #converter.inference_input_type = tf.int8\n",
    "            converter.inference_output_type = tf.int8\n",
    "        #converter.default_ranges_stats = (0, 1)\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "                \n",
    "    elif quant_mode == 'size':\n",
    "        converter.post_training_quantize = True\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_model_file, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print('Tflite model saved in:', tflite_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(model_path, calibrate_data_dir, quant_mode='default', quantized_input=False,\\\n",
    "                               n_mels=256, n_hop=242, n_dft=2048, asr=48000, halved_convs=False,\\\n",
    "                               flatten=False, calibration_steps=1024):\n",
    "    \n",
    "    #1. Convert l3model to keras model for quantization (with maxpooling layer but flatten removed)\n",
    "    dir_prefix = '/scratch/sk7898/quantization/' + os.path.basename(model_path).strip('.h5')\n",
    "    \n",
    "    if not os.path.isdir(dir_prefix):\n",
    "        os.makedirs(dir_prefix)\n",
    "    \n",
    "    keras_model, inputs, outputs = load_model(model_path, 'cnn_L3_melspec2', return_io=True, src_num_gpus=1,\n",
    "                                              n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, asr=asr,\n",
    "                                              halved_convs=halved_convs)\n",
    "    #print(keras_model.summary())\n",
    "    \n",
    "    print('Quantizing keras model and saving as tflite')\n",
    "    input_type = '_int8Ip' if quantized_input else ''\n",
    "    tflite_model_file = os.path.join(dir_prefix, 'full_quantized_'+ quant_mode + input_type + '.tflite')\n",
    "    \n",
    "    quantize_keras_to_tflite(tflite_model_file, model_path, quant_mode=quant_mode,\\\n",
    "                             quantized_input=quantized_input, asr=asr,\\\n",
    "                             n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, halved_convs=halved_convs, \\\n",
    "                             calibrate_data_dir=calibrate_data_dir, num_calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'melspectrogram_3/real_kernels:0' shape=(2048, 1, 1, 1025) dtype=float32> dft_real_kernels\n",
      "tracking <tf.Variable 'melspectrogram_3/imag_kernels:0' shape=(2048, 1, 1, 1025) dtype=float32> dft_imag_kernels\n",
      "tracking <tf.Variable 'melspectrogram_3/Variable:0' shape=(1025, 256) dtype=float32> freq2mel\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e18c6e04e2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode, quantized_input=quantized_input,\\\n\u001b[1;32m     15\u001b[0m                            \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_hop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_dft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalved_convs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalved_convs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                            flatten=flatten, calibration_steps=calibration_steps)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-9ee3b893f6ac>\u001b[0m in \u001b[0;36mpost_training_quantization\u001b[0;34m(model_path, calibrate_data_dir, quant_mode, quantized_input, n_mels, n_hop, n_dft, asr, halved_convs, flatten, calibration_steps)\u001b[0m\n\u001b[1;32m     11\u001b[0m     keras_model, inputs, outputs = load_model(model_path, 'cnn_L3_melspec2', return_io=True, src_num_gpus=1,\n\u001b[1;32m     12\u001b[0m                                               \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_hop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_dft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                               halved_convs=halved_convs)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#print(keras_model.summary())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/l3embedding/l3embedding/model.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(weights_path, model_type, src_num_gpus, tgt_num_gpus, return_io, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_num_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtgt_num_gpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msrc_num_gpus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtgt_num_gpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1219\u001b[0m                                                        \u001b[0moriginal_keras_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                                                        \u001b[0moriginal_backend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m                                                        reshape=reshape)\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             raise ValueError('Layer #' + str(k) +\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mpreprocess_weights_for_loading\u001b[0;34m(layer, weights, original_keras_version, original_backend, reshape)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_nested_time_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sequential'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_nested_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mconvert_nested_model\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m    831\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                     \u001b[0moriginal_keras_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_keras_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     original_backend=original_backend))\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mpreprocess_weights_for_loading\u001b[0;34m(layer, weights, original_keras_version, original_backend, reshape)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_weights_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlayer_weights_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ConvLSTM2D'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \"\"\"\n\u001b[0;32m--> 650\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "model_path = '/scratch/sk7898/l3embedding/models/cnn_l3_melspec2_recent/model_best_valid_accuracy.h5'\n",
    "calibrate_data_dir = '/beegfs/work/AudioSetSamples_environmental/environmental_train'\n",
    "calibration_steps = 32\n",
    "\n",
    "quant_mode='default'\n",
    "flatten=True\n",
    "quantized_input=True\n",
    "n_mels=256\n",
    "n_hop=242\n",
    "n_dft=2048\n",
    "asr=48000\n",
    "halved_convs=False\n",
    "\n",
    "# post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode, quantized_input=quantized_input,\\\n",
    "#                            n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, asr=asr, halved_convs=halved_convs,\\\n",
    "#                            flatten=flatten, calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'melspectrogram_2/real_kernels:0' shape=(2048, 1, 1, 1025) dtype=float32> dft_real_kernels\n",
      "tracking <tf.Variable 'melspectrogram_2/imag_kernels:0' shape=(2048, 1, 1, 1025) dtype=float32> dft_imag_kernels\n",
      "tracking <tf.Variable 'melspectrogram_2/Variable:0' shape=(1025, 256) dtype=float32> freq2mel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-2-gpu/lib/python3.6/site-packages/librosa/filters.py:284: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_L3_kapredbinputbn\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 48000)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vision_model (Model)            (None, 512)          4693068     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "audio_model (Model)             (None, 512)          9152708     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           vision_model[1][0]               \n",
      "                                                                 audio_model[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          131200      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,977,234\n",
      "Trainable params: 13,969,546\n",
      "Non-trainable params: 7,688\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "blob = h5py.File(model_path, 'r')\n",
    "#print(blob.keys()) ['audio_model', 'concatenate_96', 'dense_127', 'dense_128', 'input_127', 'input_128', 'vision_model']\n",
    "m, inputs, outputs = MODELS['cnn_L3_melspec2'](n_mels=256, n_hop=242, n_dft=2048, asr=48000, num_gpus=1)\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "4\n",
      "-----\n",
      "2\n",
      "-----\n",
      "2\n",
      "-----\n",
      "2\n",
      "-----\n",
      "2\n",
      "-----\n",
      "2\n",
      "-----\n",
      "2\n",
      "-----\n",
      "2\n",
      "-----\n",
      "2\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for layer in blob['vision_model']:\n",
    "    weight_values = [np.asarray(blob['vision_model'][layer][grp]) for grp in blob['vision_model'][layer]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_1\n",
      "batch_normalization_2\n",
      "batch_normalization_3\n",
      "batch_normalization_4\n",
      "batch_normalization_5\n",
      "batch_normalization_6\n",
      "batch_normalization_7\n",
      "batch_normalization_8\n",
      "batch_normalization_9\n",
      "conv2d_1\n",
      "conv2d_2\n",
      "conv2d_3\n",
      "conv2d_4\n",
      "conv2d_5\n",
      "conv2d_6\n",
      "conv2d_7\n",
      "vision_embedding_layer\n"
     ]
    }
   ],
   "source": [
    "for layer in sorted(m.get_layer('vision_model').layers, key=lambda x: x.name):\n",
    "    symbolic_weights = layer.weights\n",
    "    weight_values = K.batch_get_value(symbolic_weights)\n",
    "    if len(weight_values) > 0:\n",
    "        print(layer.name)\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model\n",
      "audio_model\n"
     ]
    }
   ],
   "source": [
    "def load_attributes_from_hdf5_group(group, name):\n",
    "      \"\"\"Loads attributes of the specified name from the HDF5 group.\n",
    "      This method deals with an inherent problem\n",
    "      of HDF5 file which is not able to store\n",
    "      data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n",
    "      Arguments:\n",
    "          group: A pointer to a HDF5 group.\n",
    "          name: A name of the attributes to load.\n",
    "      Returns:\n",
    "          data: Attributes data.\n",
    "      \"\"\"\n",
    "        \n",
    "    if name in group.attrs:\n",
    "        data = [n.decode('utf8') for n in group.attrs[name]]\n",
    "    else:\n",
    "        data = []\n",
    "        chunk_id = 0\n",
    "        while '%s%d' % (name, chunk_id) in group.attrs:\n",
    "            data.extend([n.decode('utf8') for n in group.attrs['%s%d' % (name, chunk_id)]])\n",
    "            chunk_id += 1\n",
    "    return data\n",
    "\n",
    "def convert_nested_model(weights):\n",
    "    \"\"\"Converts layers nested in `Model` or `Sequential`.\n",
    "    This function uses `preprocess_weights_for_loading()` for converting nested\n",
    "    layers.\n",
    "    Arguments:\n",
    "        weights: List of weights values (Numpy arrays).\n",
    "    Returns:\n",
    "        A list of weights values (Numpy arrays).\n",
    "    \"\"\"\n",
    "    trainable_weights = weights[:len(layer.trainable_weights)]\n",
    "    non_trainable_weights = weights[len(layer.trainable_weights):]\n",
    "\n",
    "    new_trainable_weights = []\n",
    "    new_non_trainable_weights = []\n",
    "\n",
    "    for sublayer in layer.layers:\n",
    "        num_trainable_weights = len(sublayer.trainable_weights)\n",
    "        num_non_trainable_weights = len(sublayer.non_trainable_weights)\n",
    "        if sublayer.weights:\n",
    "            preprocessed = preprocess_weights_for_loading(layer=sublayer,\n",
    "                                                          weights=(trainable_weights[:num_trainable_weights] +\n",
    "                                                          non_trainable_weights[:num_non_trainable_weights]),\n",
    "                                                          original_keras_version=original_keras_version,\n",
    "                                                          original_backend=original_backend)\n",
    "            new_trainable_weights.extend(preprocessed[:num_trainable_weights])\n",
    "            new_non_trainable_weights.extend(preprocessed[num_trainable_weights:])\n",
    "\n",
    "            trainable_weights = trainable_weights[num_trainable_weights:]\n",
    "            non_trainable_weights = non_trainable_weights[num_non_trainable_weights:]\n",
    "\n",
    "    return new_trainable_weights + new_non_trainable_weights\n",
    "\n",
    "for layer in m.layers:\n",
    "    if layer.__class__.__name__ in ['Model', 'Sequential']: \n",
    "        print(layer.name)\n",
    "        weights = convert_nested_model(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = blob\n",
    "data = []\n",
    "names = 'layer_names'\n",
    "chunk_id = 0\n",
    "while '%s%d' % (name, chunk_id) in group.attrs:\n",
    "    data.extend([n.decode('utf8') for n in group.attrs['%s%d' % (name, chunk_id)]])\n",
    "    chunk_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
