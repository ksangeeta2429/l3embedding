{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/l3embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%cd /scratch/sk7898/l3embedding\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Activation, Lambda\n",
    "import keras.regularizers as regularizers\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files(iterable):\n",
    "    lst = list(iterable)\n",
    "    random.shuffle(lst)\n",
    "    return iter(lst)\n",
    "\n",
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec\n",
    "\n",
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_data_generator(data_dir, batch_size=512, samp_rate=48000, n_fft=2048, \\\n",
    "                         n_mels=256, mel_hop_length=242, hop_size=0.1, fmax=None,\\\n",
    "                         random_state=None, start_batch_idx=None):\n",
    "\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "        \n",
    "    frame_length = samp_rate * 1\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "       \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = h5py.File(data_batch_path, 'r')\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['audio'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch = pcm2float(batch, dtype='float32')\n",
    "                    else:\n",
    "                        batch = resample(pcm2float(batch, dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "\n",
    "                    X = [get_melspectrogram(batch[i].flatten(), n_fft=n_fft, mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels, fmax=fmax) for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    return batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None\n",
    "\n",
    "def single_epoch_test_data_generator(data_dir, epoch_size, **kwargs):\n",
    "    for _ in range(epoch_size):\n",
    "        x = quant_data_generator(data_dir, **kwargs)\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_stats(model):\n",
    "    for layer in model.layers:\n",
    "        if len(layer.get_weights()) > 0:\n",
    "            weights = np.array(layer.get_weights())\n",
    "            print('Min: {} Max: {}'.format(np.min(weights), np.max(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used only if there is no Batch Normalization after the Input layer\n",
    "def construct_cnn_L3_melspec2_spec_model(n_mels=256, n_hop = 242, n_dft = 2048,\n",
    "                                         fmax=None, asr = 48000, halved_convs=False, audio_window_dur = 1):\n",
    "    \"\"\"\n",
    "    Constructs a model that replicates the audio subnetwork  used in Look,\n",
    "    Listen and Learn\n",
    "    Relja Arandjelovic and (2017). Look, Listen and Learn. CoRR, abs/1705.08168, .\n",
    "    Returns\n",
    "    -------\n",
    "    model:  L3 CNN model\n",
    "            (Type: keras.models.Model)\n",
    "    inputs: Model inputs\n",
    "            (Type: list[keras.layers.Input])\n",
    "    outputs: Model outputs\n",
    "            (Type: keras.layers.Layer)\n",
    "    \"\"\"\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    n_frames = 1 + int((asr * audio_window_dur) / float(n_hop))\n",
    "    x_a = Input(shape=(n_mels, n_frames, 1), dtype='float32')\n",
    "    #y_a = BatchNormalization()(x_a)\n",
    "\n",
    "    # CONV BLOCK 1\n",
    "    n_filter_a_1 = 64\n",
    "    if halved_convs:\n",
    "        n_filter_a_1 //= 2\n",
    "\n",
    "    filt_size_a_1 = (3, 3)\n",
    "    pool_size_a_1 = (2, 2)\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(x_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_1, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 2\n",
    "    n_filter_a_2 = 128\n",
    "    if halved_convs:\n",
    "        n_filter_a_2 //= 2\n",
    "\n",
    "    filt_size_a_2 = (3, 3)\n",
    "    pool_size_a_2 = (2, 2)\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_2, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 3\n",
    "    n_filter_a_3 = 256\n",
    "    if halved_convs:\n",
    "        n_filter_a_3 //= 2\n",
    "\n",
    "    filt_size_a_3 = (3, 3)\n",
    "    pool_size_a_3 = (2, 2)\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_3, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 4\n",
    "    n_filter_a_4 = 512\n",
    "    if halved_convs:\n",
    "        n_filter_a_4 //= 2\n",
    "\n",
    "    filt_size_a_4 = (3, 3)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4,\n",
    "                 kernel_initializer='he_normal',\n",
    "                 name='audio_embedding_layer', padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    \n",
    "    m = Model(inputs=x_a, outputs=y_a)\n",
    "    m.name = 'audio_model'\n",
    "\n",
    "    return m, x_a, y_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_for_tflite(model_path, n_mels=256, n_hop=242, n_dft=2048,\n",
    "                     asr=48000, fmax=None, halved_convs=False):\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    keras.backend.set_learning_phase(0)\n",
    "    \n",
    "    l3model = tf.keras.models.load_model(model_path) #keras.models.load_model(model_path)\n",
    "\n",
    "    n_model, _ , _ = construct_cnn_L3_melspec2_spec_model(n_mels=n_mels, n_hop=n_hop, \\\n",
    "                                                          n_dft=n_dft, asr=asr, fmax=fmax,\\\n",
    "                                                          halved_convs=halved_convs, audio_window_dur=1)\n",
    "    for idx, layer in enumerate(l3model.layers):\n",
    "        if idx == 0:\n",
    "            n_model.layers[idx].set_weights(l3model.get_layer(layer.name).get_weights())\n",
    "        if idx!=0 and idx != 1:\n",
    "            n_model.layers[idx-1].set_weights(l3model.get_layer(layer.name).get_weights())\n",
    "    \n",
    "    return n_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_keras_to_tflite(tflite_model_file, keras_model, quant_mode='default',\\\n",
    "                             n_mels=256, n_hop=242, n_dft=2048, asr=48000, halved_convs=False,\\\n",
    "                             quant_type='int8', calibrate_data_dir=None, calibration_steps=1024):\n",
    "\n",
    "    def representative_dataset_gen():\n",
    "        print('Calibrating.........')\n",
    "        for _ in range(calibration_steps):\n",
    "            x = quant_data_generator(calibrate_data_dir, batch_size=1,\\\n",
    "                                     samp_rate=asr, n_fft=n_dft, n_mels=n_mels,\\\n",
    "                                     mel_hop_length=n_hop)\n",
    "            yield [np.array(x).astype(np.float32)]\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    converter.experimental_enable_mlir_converter = False\n",
    "    #converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\n",
    " \n",
    "    if quant_mode == 'default' or quant_mode == 'latency':\n",
    "        if calibrate_data_dir is None:\n",
    "            raise ValueError('Quantized activation calibration needs data directory!')\n",
    "\n",
    "        if quant_mode == 'default':\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        else:\n",
    "            converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "\n",
    "        if quant_type == 'int8':\n",
    "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  \n",
    "        else:\n",
    "            converter.target_spec.supported_types = [tf.float16]\n",
    "            \n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "                \n",
    "    elif quant_mode == 'size':\n",
    "        if quant_type == 'float16':\n",
    "            converter.target_spec.supported_types = [tf.float16]\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_model_file, \"wb\") as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(model_path, calibrate_data_dir, quant_mode='default',\\\n",
    "                               n_mels=256, n_hop=242, n_dft=2048, asr=48000, halved_convs=False,\\\n",
    "                               quant_type='int8', calibration_steps=1024):\n",
    "    \n",
    "    #1. Convert l3model to keras model for quantization (with maxpooling layer but flatten removed)\n",
    "    dir_prefix = '/scratch/sk7898/quantization/' + os.path.basename(model_path).strip('.h5')\n",
    "    \n",
    "    if not os.path.isdir(dir_prefix):\n",
    "        os.makedirs(dir_prefix)\n",
    "    \n",
    "#     keras_model = keras_for_tflite(model_path, n_mels=n_mels, n_hop=n_hop,\n",
    "#                                    n_dft=n_dft, asr=asr, halved_convs=halved_convs)\n",
    "#     keras_model.save(os.path.join(dir_prefix, 'for_quant_no_bn.h5')) \n",
    "\n",
    "    keras_model = tf.keras.models.load_model(model_path)\n",
    "    #keras_model.summary()\n",
    "\n",
    "    print('Quantizing keras model and saving as tflite')\n",
    "    #quant_op_type = '_uint8' if quantized_op else ''\n",
    "    tflite_model_file = os.path.join(dir_prefix, \n",
    "                                     'test_quantized_'+ quant_mode + '_'+ quant_type + '.tflite')\n",
    "    \n",
    "    quantize_keras_to_tflite(tflite_model_file, keras_model, quant_mode=quant_mode,\\\n",
    "                             quant_type=quant_type, asr=asr,\\\n",
    "                             n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, halved_convs=halved_convs, \\\n",
    "                             calibrate_data_dir=calibrate_data_dir, calibration_steps=calibration_steps)\n",
    "    \n",
    "    return tflite_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Quantizing keras model and saving as tflite\n"
     ]
    }
   ],
   "source": [
    "#model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "#model_path = '/scratch/dr2915/l3pruning/embedding/fixed/reduced_input/l3_audio_20191108201753_8000_64_160_1024_half.h5'\n",
    "model_path = '/scratch/sk7898/models/reduced_input/embedding/environmental/audio_models/l3_audio_20200304152812_8000_64_160_1024_half.h5'\n",
    "calibrate_data_dir = '/beegfs/work/AudioSetSamples_environmental/environmental_train'\n",
    "calibration_steps = 5\n",
    "\n",
    "quant_mode = 'size' #Options: {'size', 'default', 'latency'}\n",
    "n_mels = 64\n",
    "n_hop = 160\n",
    "n_dft = 1024\n",
    "asr = 8000\n",
    "halved_convs=True if 'half' in model_path else False\n",
    "quant_type = 'int8'\n",
    "\n",
    "quant_output_path = post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode,\n",
    "                                               n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, asr=asr,\n",
    "                                               halved_convs=halved_convs, quant_type=quant_type,\n",
    "                                               calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input/Output of tflite model (Interpreter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "{'name': 'input_3', 'index': 35, 'shape': array([ 1, 64, 51,  1], dtype=int32), 'shape_signature': array([ 1, 64, 51,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'Identity', 'index': 36, 'shape': array([  1, 256], dtype=int32), 'shape_signature': array([  1, 256], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(quant_output_path))\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "output_shape = output_details[0]['shape'][1:]\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Embedding from the tflite model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding(model_path, tflite_model_file, data_dir, quant_mode='default',\\\n",
    "                  emb_len=512, batch_size=64, epoch_size=1024):\n",
    "    \n",
    "    output = None\n",
    "    print('Getting embedding out of Quantized tflite model')\n",
    "    splits = os.path.basename(model_path).strip('.h5').split('_')\n",
    "    samp_rate = int(splits[3])\n",
    "    n_mels = int(splits[4])\n",
    "    mel_hop_length = int(splits[5])\n",
    "    n_fft = int(splits[-1])\n",
    "    \n",
    "    data_gen = single_epoch_test_data_generator(data_dir, epoch_size,\\\n",
    "                                                batch_size=batch_size, samp_rate=samp_rate,\\\n",
    "                                                n_fft=n_fft, n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "\n",
    "    output = get_embeddings_batch_from_tflite(data_gen, tflite_model_file,\\\n",
    "                                              epoch_size, batch_size, emb_len=emb_len)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch_from_tflite(data_gen, tflite_model_file, epoch_size, batch_size, emb_len=512):\n",
    "    \n",
    "    predictions = np.zeros(shape=(epoch_size, batch_size, emb_len))\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "\n",
    "    interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "                \n",
    "    #predictions per batch   \n",
    "    for idx, batch_x in enumerate(data_gen):\n",
    "        x = np.array(batch_x).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        flattened_output = np.reshape(output, (output.shape[0], output.shape[-1]))\n",
    "        predictions[idx, :, :] = flattened_output\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "tflite_model_file = '/scratch/sk7898/quantization/l3_audio_original_48000_256_242_2048/quantized_model_default.tflite'\n",
    "data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "\n",
    "embeddings = gen_embedding(model_path, tflite_model_file, data_dir, quant_mode='default',\\\n",
    "                           batch_size=64, epoch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
