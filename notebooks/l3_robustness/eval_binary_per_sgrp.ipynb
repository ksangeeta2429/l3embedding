{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/scratch/sk7898/l3embedding/classifier/sonyc_ust'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import resampy\n",
    "import librosa\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import cm\n",
    "from metrics import confusion_matrix_coarse\n",
    "from classify import load_embeddings, predict_mil, construct_mlp_mil\n",
    "# New modules: oyaml and pandas\n",
    "import oyaml as yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "version = 'v0.4'\n",
    "cls = 'engine'\n",
    "#cls = 'machinery-impact'\n",
    "SONYC_PATH = '/scratch/work/sonyc/sonyc/ust/annotations'\n",
    "META_FOLDER = '/scratch/sk7898/l3embedding/notebooks/data'\n",
    "DATA_FOLDER = os.path.join('/scratch/sk7898/l3embedding/notebooks/data', version)\n",
    "NEW_ANNOTATION_DIR = os.path.join(DATA_FOLDER, cls)\n",
    "\n",
    "THRESH_DICT = {\n",
    "    1: 0.8,\n",
    "    2: 0.2,\n",
    "    3: 0.2,\n",
    "    4: 0.2,\n",
    "    5: 0.8,\n",
    "    6: 0.1,\n",
    "    7: 0.3,\n",
    "    8: 0.1\n",
    "}\n",
    "\n",
    "annotation_path = os.path.join(NEW_ANNOTATION_DIR, 'annotations.csv')\n",
    "taxonomy_path = os.path.join(SONYC_PATH, '{}/dcase-ust-taxonomy.yaml'.format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "\n",
    "#coarse_dict from taxonomy\n",
    "with open(taxonomy_path, 'r') as f:\n",
    "    taxonomy = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "coarse_dict = {k: v for k, v in taxonomy['coarse'].items() if v == cls} \n",
    "\n",
    "# Path to the embeddings extracted from original L3 trained on env data\n",
    "embs_dir = '/scratch/sk7898/sonyc_output/embeddings/' + version\n",
    "emb_dir = os.path.join(embs_dir, 'features/sonyc_ust/l3/melSpec_20200304183233_48000_256_242_2048')\n",
    "\n",
    "arch_str = '0_0' #'1_128'\n",
    "# Path to MIL classifier trained with v0.4 training data\n",
    "cls_prefix = 'classifier/sonyc_ust/mil/binary/melSpec_20200304183233_48000_256_242_2048'\n",
    "cls_path = os.path.join(embs_dir, cls_prefix, '{}/{}/results'.format(cls, arch_str))\n",
    "output_path = os.path.join(cls_path, 'output.csv')\n",
    "scaler_path = os.path.join(cls_path, 'stdizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1_classwise_thresh(df_dict, thresh_dict):\n",
    "    mu = 0.5\n",
    "    TPs, FPs, FNs = 0, 0, 0\n",
    "    \n",
    "    for coarse_id in df_dict.keys():\n",
    "        threshold = thresh_dict[coarse_id]\n",
    "        coarse_df = df_dict[coarse_id]\n",
    "        coarse_thresholds = coarse_df[\"threshold\"]\n",
    "        row = coarse_df[coarse_thresholds>=threshold].iloc[-1]\n",
    "        \n",
    "        TPs += row[\"TP\"]\n",
    "        FPs += row[\"FP\"]\n",
    "        FNs += row[\"FN\"]\n",
    "    \n",
    "    precision = TPs / np.maximum(TPs + FPs, mu)\n",
    "    recall = TPs / np.maximum(TPs + FNs, mu)\n",
    "    f1 = 2 / (1/precision + 1/recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_df(gt_df, pred_df, yaml_dict, mode='coarse'):\n",
    "    # NOTE: Make sure mode matches pred_df!\n",
    "\n",
    "    # Set minimum threshold.\n",
    "    min_threshold = 0.01\n",
    "\n",
    "    # Make sure the size of the tables match\n",
    "    if not (len(gt_df) == len(pred_df)):\n",
    "        err_msg =\\\n",
    "            \"Size mismatch between ground truth ({} files) \" \\\n",
    "            \"and prediction table ({} files).\"\n",
    "        raise ValueError(err_msg.format(len(gt_df), len(pred_df)))\n",
    "\n",
    "    # Initialize dictionary of DataFrames.\n",
    "    df_dict = {}\n",
    "\n",
    "    # Loop over coarse categories.\n",
    "    for coarse_id in yaml_dict:\n",
    "        # List columns corresponding to that category\n",
    "        if mode == \"coarse\":\n",
    "            columns = [str(coarse_id)]\n",
    "        else:\n",
    "            columns = [column for column in pred_df.columns\n",
    "                if (str(column).startswith(str(coarse_id))) and\n",
    "                   (\"-\" in str(column)) and\n",
    "                   (not str(column).endswith(\"X\"))]\n",
    "\n",
    "        # Sort columns in alphanumeric order.\n",
    "        columns.sort()\n",
    "\n",
    "        # Restrict prediction to columns of interest.\n",
    "        restricted_pred_df = np.clip(pred_df[columns], 0.0, 1.0)\n",
    "\n",
    "        # Restrict ground truth to columns of interest.\n",
    "        restricted_gt_df = np.clip(gt_df[columns], 0.0, 1.0)\n",
    "\n",
    "        # Aggregate all prediction values into a \"raveled\" vector.\n",
    "        # We make an explicit numpy, so that the original DataFrame\n",
    "        # is left unchanged.\n",
    "        thresholds = np.ravel(np.copy(restricted_pred_df.values))\n",
    "\n",
    "        # Sort in place.\n",
    "        thresholds.sort()\n",
    "\n",
    "        # Skip very low values.\n",
    "        # This is to speed up the computation of the precision-recall curve\n",
    "        # in the low-precision regime.\n",
    "        thresholds = thresholds[np.searchsorted(thresholds, min_threshold):]\n",
    "        thresholds = np.append(thresholds, 1.0)\n",
    "        thresholds = np.unique(thresholds)[::-1]\n",
    "\n",
    "        # Count number of thresholds.\n",
    "        n_thresholds = len(thresholds)\n",
    "        TPs = np.zeros((n_thresholds,)).astype('int')\n",
    "        FPs = np.zeros((n_thresholds,)).astype('int')\n",
    "        FNs = np.zeros((n_thresholds,)).astype('int')\n",
    "\n",
    "        # FINE MODE.\n",
    "        if mode == \"fine\":\n",
    "            incomplete_tag = str(coarse_id) + \"-X\"\n",
    "\n",
    "            # Load ground truth as numpy array.\n",
    "            Y_true = restricted_gt_df.values\n",
    "            is_true_incomplete = gt_df[incomplete_tag].values\n",
    "\n",
    "            # Loop over thresholds in a decreasing order.\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                # Threshold prediction for complete tag.\n",
    "                Y_pred = restricted_pred_df.values >= threshold\n",
    "\n",
    "                # Threshold prediction for incomplete tag.\n",
    "                is_pred_incomplete =\\\n",
    "                    pred_df[incomplete_tag].values >= threshold\n",
    "\n",
    "                # Evaluate.\n",
    "                TPs[i], FPs[i], FNs[i] = confusion_matrix_fine(\n",
    "                    Y_true, Y_pred, is_true_incomplete, is_pred_incomplete)\n",
    "\n",
    "        # COARSE MODE.\n",
    "        elif mode == \"coarse\":\n",
    "            # Load ground truth as numpy array.\n",
    "            Y_true = restricted_gt_df.values\n",
    "\n",
    "            # Loop over thresholds in a decreasing order.\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                # Threshold prediction.\n",
    "                Y_pred = restricted_pred_df.values >= threshold\n",
    "\n",
    "                # Evaluate.\n",
    "                TPs[i], FPs[i], FNs[i] = confusion_matrix_coarse(Y_true, Y_pred)\n",
    "\n",
    "        # Build DataFrame from columns.\n",
    "        eval_df = pd.DataFrame({\n",
    "            \"threshold\": thresholds, \"TP\": TPs, \"FP\": FPs, \"FN\": FNs})\n",
    "\n",
    "        mu = 0.5\n",
    "        eval_df[\"P\"] = TPs / np.maximum(TPs + FPs, mu)\n",
    "        eval_df[\"R\"] = TPs / np.maximum(TPs + FNs, mu)\n",
    "        eval_df[\"F\"] = 2 / (1/eval_df[\"P\"] + 1/eval_df[\"R\"])\n",
    "\n",
    "        # Store DataFrame in the dictionary.\n",
    "        df_dict[coarse_id] = eval_df\n",
    "\n",
    "    # Return dictionary.\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_coarse_prediction(pred_df, coarse_dict):\n",
    "\n",
    "    # Collect tag names as strings and map them to coarse ID pairs.\n",
    "    rev_coarse_dict = {\"_\".join([str(k), coarse_dict[k]]): k\n",
    "        for k in coarse_dict}\n",
    "\n",
    "    # Assign a predicted column to each coarse key, by using the tag as an\n",
    "    # intermediate hashing step.\n",
    "    pred_coarse_dict = {}\n",
    "    for c in rev_coarse_dict:\n",
    "        if c in pred_df:\n",
    "            pred_coarse_dict[str(rev_coarse_dict[c])] = pred_df[c]\n",
    "        else:\n",
    "            pred_coarse_dict[str(rev_coarse_dict[c])] = np.zeros((len(pred_df),))\n",
    "            warnings.warn(\"Column not found: \" + c)\n",
    "\n",
    "    # Copy over the audio filename, split and sensor_id strings corresponding to each sample.\n",
    "    pred_coarse_dict[\"audio_filename\"] = pred_df[\"audio_filename\"]\n",
    "    pred_coarse_dict[\"new_split\"] = pred_df[\"new_split\"]\n",
    "    pred_coarse_dict[\"sensor_id\"] = pred_df[\"sensor_id\"]\n",
    "    pred_coarse_dict[\"grp_id\"] = pred_df[\"grp_id\"]\n",
    "\n",
    "    # Build a new Pandas DataFrame with coarse keys as column names.\n",
    "    pred_coarse_df = pd.DataFrame.from_dict(pred_coarse_dict)\n",
    "\n",
    "    pred_coarse_df.sort_values('audio_filename')\n",
    "    pred_coarse_df.reset_index(inplace=True)\n",
    "    if 'index' in pred_coarse_df.keys():\n",
    "        pred_coarse_df = pred_coarse_df.drop(columns=['index'])\n",
    "\n",
    "    return pred_coarse_df\n",
    "\n",
    "def filter_ground_truth(ann_df, aggregate=True):\n",
    "    \n",
    "    gt_df = ann_df.groupby(\"audio_filename\", group_keys=False).max()\n",
    "    \n",
    "    gt_df.reset_index(inplace=True)\n",
    "    if 'index' in gt_df.keys():\n",
    "        gt_df = gt_df.drop(columns=['index'])\n",
    "    return gt_df\n",
    "\n",
    "def parse_ground_truth(ann_df, coarse_dict):\n",
    "\n",
    "    gt_df = filter_ground_truth(ann_df)\n",
    "\n",
    "    # Rename coarse columns.\n",
    "    coarse_renaming = {\n",
    "        \"_\".join([str(c), coarse_dict[c], \"presence\"]): str(c)\n",
    "        for c in coarse_dict}\n",
    "    gt_df = gt_df.rename(columns=coarse_renaming)\n",
    "\n",
    "    gt_df = gt_df.sort_values('audio_filename')\n",
    "\n",
    "    gt_df.reset_index(inplace=True)\n",
    "    if 'index' in gt_df.keys():\n",
    "        gt_df = gt_df.drop(columns=['index'])\n",
    "\n",
    "    return gt_df\n",
    "\n",
    "def evaluate_sgrp_df(gt_df, pred_df, coarse_dict, grp_by='grp_id'):\n",
    "    \n",
    "    dicts = {}\n",
    "    # Do not consider train data from each sensor group\n",
    "    sgrps = pred_df[(pred_df['new_split'] == 'validate') | (pred_df['new_split'] == 'test')][grp_by].unique().tolist()\n",
    "        \n",
    "    for gid in sgrps:\n",
    "        gt = gt_df[gt_df[grp_by] == gid]\n",
    "        pred = pred_df[pred_df['grp_id'] == gid]\n",
    "        gt = gt.drop(columns=['new_split', grp_by])\n",
    "        pred = pred.drop(columns=['new_split', grp_by])\n",
    "        dicts[gid] = evaluate_df(gt, pred, coarse_dict)\n",
    "    return dicts\n",
    "\n",
    "def evaluate(prediction_data, annotation_data, coarse_dict, grp_by='grp_id'):\n",
    "\n",
    "    # Parse ground truth.\n",
    "    gt_df = parse_ground_truth(annotation_data, coarse_dict)\n",
    "    pred_df = parse_coarse_prediction(prediction_data, coarse_dict)\n",
    "\n",
    "    flist = [value for value in gt_df['audio_filename'].unique().tolist() \n",
    "             if value in pred_df['audio_filename'].unique().tolist()] \n",
    "    pred_df = pred_df[pred_df['audio_filename'].isin(flist)]\n",
    "    gt_df = gt_df[gt_df['audio_filename'].isin(flist)]\n",
    "\n",
    "    if grp_by:\n",
    "        return evaluate_sgrp_df(gt_df, pred_df, coarse_dict, grp_by=grp_by)\n",
    "    else:\n",
    "        return evaluate_df(gt_df, pred_df, coarse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv(output_path)\n",
    "\n",
    "df_dicts = evaluate(\n",
    "                    out,\n",
    "                    annotation_data,\n",
    "                    coarse_dict, \n",
    "                    grp_by='grp_id'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['grp_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_metrics = {}\n",
    "\n",
    "# df_dict has per coarse class metrics for each sensor group id\n",
    "for sgrp, df_dict in df_dicts.items():\n",
    "    grp_metrics[sgrp] = {}\n",
    "    micro_auprc, eval_df = micro_averaged_auprc(df_dict, return_df=True)\n",
    "    macro_auprc, class_auprc = macro_averaged_auprc(df_dict, return_classwise=True)\n",
    "\n",
    "    grp_metrics[sgrp][\"micro_auprc\"] = micro_auprc\n",
    "    grp_metrics[sgrp][\"macro_auprc\"] = macro_auprc\n",
    "    grp_metrics[sgrp][\"micro_f1\"] = micro_f1_classwise_thresh(df_dict, THRESH_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10}) \n",
    "metrics = ['micro_auprc', 'macro_auprc', 'micro_f1']\n",
    "colors = ['steelblue', 'darkorange', 'forestgreen']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "grp_metrics_df = pd.DataFrame.from_dict(grp_metrics, orient='index', columns=metrics)\n",
    "grp_metrics_df = grp_metrics_df.sort_values(by=['micro_f1'])\n",
    "grp_metrics_df.plot.bar(ax=ax, color=colors)\n",
    "x_min, x_max = ax.get_xlim()\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     plt.hlines(mlist[i]/n_sensors, xmin=x_min, xmax=x_max, color=colors[i], ls='--')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = False\n",
    "DATA_FOLDER = os.path.join('/scratch/sk7898/l3embedding/notebooks/data', version)\n",
    "# Group by the sensor metadata and plot distribution per group                            \n",
    "grouped_df = meta_df.groupby(cols)['sensor_name']\n",
    "\n",
    "# Generate the pdf\n",
    "if combine:\n",
    "    grp_pdf_path = os.path.join(DATA_FOLDER, 'meta_grp_metric_combined.pdf')\n",
    "    v1_pred_path = os.path.join(META_FOLDER, 'v0.4/per_sensor_test_v0.4.csv')\n",
    "    v2_pred_path = os.path.join(META_FOLDER, 'v2.2/per_sensor_test_v2.2.csv')\n",
    "    v1_df = pd.read_csv(v1_pred_path) \n",
    "    v2_df = pd.read_csv(v2_pred_path) \n",
    "    overall_df = pd.concat([v1_df, v2_df])  \n",
    "else:\n",
    "    grp_pdf_path = os.path.join(DATA_FOLDER, 'meta_grp_metric_{}.pdf'.format(version))\n",
    "    overall_pred_path = os.path.join(DATA_FOLDER, 'per_sensor_{}_{}.csv'.format(split, version))\n",
    "    overall_df = pd.read_csv(overall_pred_path)  \n",
    "    \n",
    "plt.rcParams.update({'font.size': 10}) \n",
    "metrics = ['micro_f1', 'micro_auprc', 'macro_auprc']\n",
    "colors = ['steelblue', 'darkorange', 'forestgreen']\n",
    "    \n",
    "grp_metrics = {}\n",
    "mlist = np.zeros(len(metrics), dtype=np.float32)\n",
    "n_sensors = 0\n",
    "\n",
    "for i, (grp, df_group) in enumerate(grouped_df):\n",
    "    sensors = df_group.values\n",
    "    grp_lst = [c for c, g in zip(cols, grp) if g > 0 and c!='nyu_location'] \n",
    "    grp_name = '| '.join(grp_lst)\n",
    "    test = overall_df[overall_df['sensor_name'].isin(sensors)]\n",
    "    if len(test) > 0:\n",
    "        #print(grp_name, test['sensor_name'].tolist())\n",
    "        grp_metrics[grp_name] = {}\n",
    "        n_sensors += len(test) \n",
    "        for i, metric in enumerate(metrics):\n",
    "            mlist[i] += np.sum(test[metric])\n",
    "            grp_metrics[grp_name][metric] = np.mean(test[metric])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "grp_metrics_df = pd.DataFrame.from_dict(grp_metrics, orient='index', columns=metrics)\n",
    "grp_metrics_df = grp_metrics_df.sort_values(by=['micro_f1'])\n",
    "grp_metrics_df.plot.bar(ax=ax, color=colors)\n",
    "x_min, x_max = ax.get_xlim()\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.hlines(mlist[i]/n_sensors, xmin=x_min, xmax=x_max, color=colors[i], ls='--')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(grp_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:l3embedding-tf-2-gpu]",
   "language": "python",
   "name": "conda-env-l3embedding-tf-2-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
