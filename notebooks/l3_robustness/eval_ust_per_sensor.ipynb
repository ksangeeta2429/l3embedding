{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/scratch/sk7898/l3embedding/classifier/sonyc_ust'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import resampy\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import keras\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import load_model\n",
    "from matplotlib import cm\n",
    "from metrics import parse_coarse_prediction, micro_averaged_auprc, macro_averaged_auprc, evaluate_df\n",
    "from classify import load_embeddings, predict_mil, construct_mlp_mil\n",
    "# New modules: oyaml and pandas\n",
    "import oyaml as yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxonomy = {\n",
    "#     1: 'engine',\n",
    "#     2: 'machinery-impact',\n",
    "#     3: 'non-machinery-impact',\n",
    "#     4: 'powered-saw',\n",
    "#     5: 'alert-signal',\n",
    "#     6: 'music',\n",
    "#     7: 'human-voice',\n",
    "#     8: 'dog'\n",
    "# }\n",
    "\n",
    "version = 'v0.4'\n",
    "SONYC_PATH = '/scratch/work/sonyc/sonyc/ust/annotations'\n",
    "META_FOLDER = '/scratch/sk7898/l3embedding/notebooks/data'\n",
    "DATA_FOLDER = os.path.join('/scratch/sk7898/l3embedding/notebooks/data', version)\n",
    "THRESH_DICT = {\n",
    "    1: 0.8,\n",
    "    2: 0.2,\n",
    "    3: 0.2,\n",
    "    4: 0.2,\n",
    "    5: 0.8,\n",
    "    6: 0.1,\n",
    "    7: 0.3,\n",
    "    8: 0.1\n",
    "}\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_output(y_pred, output_path, test_file_list, annotation_data, label_mode, taxonomy):\n",
    "    \n",
    "    coarse_fine_labels = [[\"{}-{}_{}\".format(coarse_id, fine_id, fine_label)\n",
    "                           for fine_id, fine_label in fine_dict.items()]\n",
    "                          for coarse_id, fine_dict in taxonomy['fine'].items()]\n",
    "        \n",
    "    full_fine_target_labels = [fine_label for fine_list in coarse_fine_labels\n",
    "                               for fine_label in fine_list]\n",
    "        \n",
    "    coarse_target_labels = [\"_\".join([str(k), v])\n",
    "                            for k, v in taxonomy['coarse'].items()]\n",
    "    \n",
    "    annotation = annotation_data.sort_values('audio_filename')[['split', 'sensor_id', 'audio_filename']].drop_duplicates()\n",
    "    annotation = annotation[annotation['audio_filename'].isin(test_file_list)]\n",
    "    split_list = annotation['split'].tolist()\n",
    "    sensor_list = annotation['sensor_id'].tolist()\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "\n",
    "        # Write fields\n",
    "        fields = [\"split\", \"sensor_id\", \"audio_filename\"] + full_fine_target_labels + coarse_target_labels\n",
    "        csvwriter.writerow(fields)\n",
    "\n",
    "        # Write results for each file to CSV\n",
    "        for split, sid, filename, y in zip(split_list, sensor_list, test_file_list, y_pred):\n",
    "            row = [split, sid, filename]\n",
    "\n",
    "            if label_mode == \"fine\":\n",
    "                fine_values = []\n",
    "                coarse_values = [0 for _ in range(len(coarse_target_labels))]\n",
    "                coarse_idx = 0\n",
    "                fine_idx = 0\n",
    "                for coarse_label, fine_label_list in zip(coarse_target_labels,\n",
    "                                                         coarse_fine_labels):\n",
    "                    for fine_label in fine_label_list:\n",
    "                        if 'X' in fine_label.split('_')[0].split('-')[1]:\n",
    "                            # Put a 0 for other, since the baseline doesn't\n",
    "                            # account for it\n",
    "                            fine_values.append(0.0)\n",
    "                            continue\n",
    "\n",
    "                        # Append the next fine prediction\n",
    "                        fine_values.append(y[fine_idx])\n",
    "\n",
    "                        # Add coarse level labels corresponding to fine level\n",
    "                        # predictions. Obtain by taking the maximum from the\n",
    "                        # fine level labels\n",
    "                        coarse_values[coarse_idx] = max(coarse_values[coarse_idx],\n",
    "                                                        y[fine_idx])\n",
    "                        fine_idx += 1\n",
    "                    coarse_idx += 1\n",
    "\n",
    "                row += fine_values + coarse_values\n",
    "                \n",
    "            else:\n",
    "                # Add placeholder values for fine level\n",
    "                row += [0.0 for _ in range(len(full_fine_target_labels))]\n",
    "                # Add coarse level labels\n",
    "                row += list(y)\n",
    "\n",
    "            csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ground_truth_split(ann_df, aggregate=True, valid_sensor_ids=None, split_path=None, split=None):\n",
    "    \n",
    "    if split and not valid_sensor_ids:\n",
    "        gt_df = ann_df[(ann_df[\"annotator_id\"] > 0) & (ann_df['split'] == split)]\n",
    "        #gt_df = ann_df[(ann_df[\"annotator_id\"] == 0) & (ann_df['split'] == split)]\n",
    "    elif split and valid_sensor_ids:\n",
    "        gt_df = ann_df[(ann_df[\"annotator_id\"] == 0) & (ann_df['split'] == split) & (ann_df['sensor_id'].isin(valid_sensor_ids))]\n",
    "    else:\n",
    "        gt_df = ann_df[ann_df[\"annotator_id\"] == 0]\n",
    "    \n",
    "    if aggregate:\n",
    "        gt_df = gt_df.groupby(\"audio_filename\", group_keys=False).max()\n",
    "    \n",
    "    gt_df.reset_index(inplace=True)\n",
    "    if 'index' in gt_df.keys():\n",
    "        gt_df = gt_df.drop(columns=['index'])\n",
    "    return gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ground_truth(annotation_path, yaml_path, valid_sensor_ids=None, split_path=None, split=None):\n",
    "\n",
    "    # Create dictionary to parse tags\n",
    "    with open(yaml_path, 'r') as stream:\n",
    "        yaml_dict = yaml.load(stream, Loader=yaml.Loader)\n",
    "\n",
    "    # Load CSV file into a Pandas DataFrame.\n",
    "    ann_df = pd.read_csv(annotation_path)\n",
    "    gt_df = filter_ground_truth_split(ann_df,\n",
    "                                      valid_sensor_ids=valid_sensor_ids,\n",
    "                                      split_path=split_path,\n",
    "                                      split=split)\n",
    "\n",
    "    # Rename coarse columns.\n",
    "    coarse_dict = yaml_dict[\"coarse\"]\n",
    "    coarse_renaming = {\n",
    "        \"_\".join([str(c), coarse_dict[c], \"presence\"]): str(c)\n",
    "        for c in coarse_dict}\n",
    "    gt_df = gt_df.rename(columns=coarse_renaming)\n",
    "\n",
    "    # Collect tag names as strings and map them to mixed (coarse-fine) ID pairs.\n",
    "    # The \"mixed key\" is a hyphenation of the coarse ID and fine ID.\n",
    "    fine_dict = {}\n",
    "    for coarse_id in yaml_dict[\"fine\"]:\n",
    "        for fine_id in yaml_dict[\"fine\"][coarse_id]:\n",
    "            mixed_key = \"-\".join([str(coarse_id), str(fine_id)])\n",
    "            fine_dict[mixed_key] = yaml_dict[\"fine\"][coarse_id][fine_id]\n",
    "\n",
    "    # Rename fine columns.\n",
    "    fine_renaming = {\"_\".join([k, fine_dict[k], \"presence\"]): k\n",
    "        for k in fine_dict}\n",
    "    gt_df = gt_df.rename(columns=fine_renaming)\n",
    "\n",
    "    # Loop over coarse tags.\n",
    "    n_samples = len(gt_df)\n",
    "    coarse_dict = yaml_dict[\"coarse\"]\n",
    "    for coarse_id in yaml_dict[\"coarse\"]:\n",
    "        # Construct incomplete fine tag by appending -X to the coarse tag.\n",
    "        incomplete_tag = str(coarse_id) + \"-X\"\n",
    "\n",
    "        # If the incomplete tag is not in the prediction, append a column of zeros.\n",
    "        # This is the case e.g. for coarse ID 7 (\"dogs\") which has a single\n",
    "        # fine-level tag (\"7-1_dog-barking-whining\") and thus no incomplete\n",
    "        # tag 7-X.\n",
    "        if incomplete_tag not in gt_df.columns:\n",
    "            gt_df[incomplete_tag] = np.zeros((n_samples,)).astype('int')\n",
    "\n",
    "    gt_df = gt_df.sort_values('audio_filename')\n",
    "\n",
    "    gt_df.reset_index(inplace=True)\n",
    "    if 'index' in gt_df.keys():\n",
    "        gt_df = gt_df.drop(columns=['index'])\n",
    "\n",
    "    # Return output in DataFrame format.\n",
    "    return gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sensor_df(gt_df, pred_df, mode, yaml_dict, split=None):\n",
    "    \n",
    "    dicts = {}\n",
    "    split = split if split else 'validate'\n",
    "    sids = pred_df[pred_df['split'] == split]['sensor_id'].unique().tolist()\n",
    "        \n",
    "    for sid in sids:\n",
    "        gt = gt_df[(gt_df['split'] == split) & (gt_df['sensor_id'] == sid)]\n",
    "        pred = pred_df[(pred_df['split'] == split) & (pred_df['sensor_id'] == sid)]\n",
    "        gt = gt.drop(columns=['split', 'sensor_id'])\n",
    "        pred = pred.drop(columns=['split', 'sensor_id'])\n",
    "        dicts[sid] = evaluate_df(gt, pred, mode, yaml_dict)\n",
    "    return dicts\n",
    "\n",
    "def evaluate(prediction_path, annotation_path, yaml_path, mode, \n",
    "             valid_sensor_ids=None,\n",
    "             split_path=None, \n",
    "             per_sensor=False, \n",
    "             split=None):\n",
    "\n",
    "    with open(yaml_path, 'r') as stream:\n",
    "        yaml_dict = yaml.load(stream, Loader=yaml.Loader)\n",
    "\n",
    "    # Parse ground truth.\n",
    "    gt_df = parse_ground_truth(annotation_path, yaml_path,\n",
    "                               valid_sensor_ids=valid_sensor_ids,\n",
    "                               split_path=split_path,\n",
    "                               split=split)\n",
    "\n",
    "    # Parse predictions.\n",
    "    if mode == \"fine\":\n",
    "        pred_df = parse_fine_prediction(prediction_path, yaml_path)\n",
    "    elif mode == \"coarse\":\n",
    "        pred_df = parse_coarse_prediction(prediction_path, yaml_path)\n",
    "    \n",
    "    flist = [value for value in gt_df['audio_filename'].unique().tolist() \n",
    "             if value in pred_df['audio_filename'].unique().tolist()] \n",
    "    pred_df = pred_df[pred_df['audio_filename'].isin(flist)]\n",
    "    gt_df = gt_df[gt_df['audio_filename'].isin(flist)]\n",
    "\n",
    "    if per_sensor:\n",
    "        return evaluate_sensor_df(gt_df, pred_df, mode, yaml_dict, split=split)\n",
    "    else:\n",
    "        return evaluate_df(gt_df, pred_df, mode, yaml_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1_classwise_thresh(df_dict, thresh_dict):\n",
    "    mu = 0.5\n",
    "    TPs, FPs, FNs = 0, 0, 0\n",
    "    \n",
    "    for coarse_id in df_dict.keys():\n",
    "        threshold = thresh_dict[coarse_id]\n",
    "        coarse_df = df_dict[coarse_id]\n",
    "        coarse_thresholds = coarse_df[\"threshold\"]\n",
    "        row = coarse_df[coarse_thresholds>=threshold].iloc[-1]\n",
    "        \n",
    "        TPs += row[\"TP\"]\n",
    "        FPs += row[\"FP\"]\n",
    "        FNs += row[\"FN\"]\n",
    "    \n",
    "    precision = TPs / np.maximum(TPs + FPs, mu)\n",
    "    recall = TPs / np.maximum(TPs + FNs, mu)\n",
    "    f1 = 2 / (1/precision + 1/recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "\n",
    "taxonomy_path = os.path.join(SONYC_PATH, '{}/dcase-ust-taxonomy.yaml'.format(version))\n",
    "if version == 'v2.2':\n",
    "    annotation_path = os.path.join(SONYC_PATH, 'latest/annotations_w_test_anns.csv')\n",
    "else:\n",
    "    annotation_path = os.path.join(SONYC_PATH, '{}/annotations.csv'.format(version))\n",
    "\n",
    "# Path to the embeddings extracted from original L3 trained on env data\n",
    "embs_dir = '/scratch/sk7898/sonyc_output/embeddings'\n",
    "emb_dir = os.path.join(embs_dir, version, 'features/sonyc_ust/l3/melSpec_20200304183233_48000_256_242_2048')\n",
    "\n",
    "# Path to MIL classifier trained with v0.4 training data\n",
    "cls_path = os.path.join(embs_dir, 'classifier/sonyc_ust/mil/melSpec_20200304183233_48000_256_242_2048/0_0/results')\n",
    "output_path = os.path.join(cls_path, 'output_{}.csv'.format(version))\n",
    "scaler_path = os.path.join(cls_path, 'stdizer.pkl')\n",
    "\n",
    "annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "with open(taxonomy_path, 'r') as f:\n",
    "    taxonomy = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "if split:\n",
    "    file_list = annotation_data[annotation_data['split'] == split].sort_values('audio_filename')['audio_filename'].unique().tolist()\n",
    "else:\n",
    "    file_list = annotation_data.sort_values('audio_filename')['audio_filename'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict (with model for coarse labels) on test data and save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mode = 'coarse'\n",
    "if not os.path.exists(output_path):\n",
    "    embeddings = load_embeddings(file_list, emb_dir)\n",
    "        \n",
    "    if os.path.exists(scaler_path):\n",
    "        print('Found Standardizer!')\n",
    "        scaler = pk.load(open(scaler_path,'rb'), encoding='utf-8')\n",
    "        embeddings = [scaler.transform(emb_grp) for emb_grp in embeddings]\n",
    "        \n",
    "    X = np.array(embeddings)\n",
    "    _, num_frames, emb_size = X.shape\n",
    "\n",
    "    num_classes = 8\n",
    "    model_weight_file = os.path.join(cls_path, 'model_best.h5')\n",
    "    model = construct_mlp_mil(num_frames,\n",
    "                              emb_size,\n",
    "                              num_classes,\n",
    "                              num_hidden_layers=0,\n",
    "                              hidden_layer_size=0)\n",
    "\n",
    "    model.load_weights(model_weight_file)\n",
    "            \n",
    "    pred = model.predict(X)\n",
    "\n",
    "    # Discard auxilliary predictions\n",
    "    if type(pred) == list:\n",
    "        pred = pred[0]\n",
    "\n",
    "    pred.tolist()\n",
    "    write_to_output(pred, output_path, file_list, annotation_data, label_mode, taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the output file saved above to evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv(output_path)\n",
    "\n",
    "df_dicts = evaluate(\n",
    "                    output_path,\n",
    "                    annotation_path,\n",
    "                    taxonomy_path, \n",
    "                    mode=label_mode,\n",
    "                    valid_sensor_ids=None,\n",
    "                    per_sensor=True,\n",
    "                    split=split\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "class_metrics = {}\n",
    "sensor_df = pd.read_csv(os.path.join(DATA_FOLDER, 'sensor_split_ids_{}.csv'.format(version)))\n",
    "\n",
    "# df_dict has per coarse class metrics for each sensor id\n",
    "for sensor, df_dict in df_dicts.items():\n",
    "    metrics[sensor] = {}\n",
    "    class_metrics[sensor] = {}\n",
    "    micro_auprc, eval_df = micro_averaged_auprc(df_dict, return_df=True)\n",
    "    macro_auprc, class_auprc = macro_averaged_auprc(df_dict, return_classwise=True)\n",
    "\n",
    "    metrics[sensor][\"sensor_name\"] = sensor_df[sensor_df['sensor_id'] == sensor]['sensor_name'].tolist()[0]\n",
    "    metrics[sensor][\"micro_auprc\"] = micro_auprc\n",
    "    metrics[sensor][\"macro_auprc\"] = macro_auprc\n",
    "    metrics[sensor][\"micro_f1\"] = micro_f1_classwise_thresh(df_dict, THRESH_DICT)\n",
    "    \n",
    "#     # Get index of first threshold that is at least the threshold set for that class\n",
    "#     thresh_0pt5_idx = (eval_df['threshold'] >= 0.5).to_numpy().nonzero()[0][0]\n",
    "#     metrics[sensor][\"micro_f1\"] = eval_df[\"F\"][thresh_0pt5_idx]\n",
    "    \n",
    "    for coarse_id, auprc in class_auprc.items():\n",
    "        coarse_name = taxonomy['coarse'][int(coarse_id)]\n",
    "        class_metrics[sensor][coarse_name] = auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_pred_path = os.path.join(DATA_FOLDER, 'per_sensor_{}_{}.csv'.format(split, version))\n",
    "classwise_pred_path = os.path.join(DATA_FOLDER, 'classwise_per_sensor_{}_{}.csv'.format(split, version))\n",
    "\n",
    "if not os.path.exists(overall_pred_path):\n",
    "    result = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "    result['sensor_id'] = result.index.tolist()\n",
    "    result.to_csv(overall_pred_path, index=False)\n",
    "    \n",
    "if not os.path.exists(classwise_pred_path):\n",
    "    classwise_result = pd.DataFrame.from_dict(class_metrics, orient='index')\n",
    "    classwise_result['sensor_id'] = classwise_result.index.tolist()\n",
    "    classwise_result.to_csv(classwise_pred_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 4 pdfs with the following information:\n",
    "- `gen_metrics_pdf`: Performance distribution of Test sensors which are not part of training set\n",
    "- `gen_metrics_pdf`: Performance distribution of Test sensors which are also part of training set\n",
    "- `gen_clsdist_pdf`: Class distribution of a test sensor vs. Class distribution of train dataset\n",
    "- `gen_grp_metrics_pdf`: Distributions of different sensors in the same metadata group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_clsdist_pdf(\n",
    "    pdf_path, \n",
    "    sensors,\n",
    "    sensor_df,\n",
    "    meta_df\n",
    "):\n",
    "    coarse_columns = [\n",
    "        '1_engine_presence', '2_machinery-impact_presence', \n",
    "        '3_non-machinery-impact_presence', '4_powered-saw_presence', \n",
    "        '5_alert-signal_presence', '6_music_presence',\n",
    "        '7_human-voice_presence', '8_dog_presence'\n",
    "    ]\n",
    "    # Get the train and test class distribution\n",
    "    cdist = {}\n",
    "    for split in ['train', 'test']:\n",
    "        if split == 'train':\n",
    "            data = annotation_data[annotation_data['split'] == 'train']\n",
    "            data = data.groupby('audio_filename', group_keys=False).max()\n",
    "        elif split == 'test':\n",
    "            data = annotation_data[\n",
    "                                (annotation_data['split'] == 'test') & \n",
    "                                (annotation_data['annotator_id'] == 0)\n",
    "                                ]\n",
    "            # For version 2.2, we do not have annotator_id 0\n",
    "            if len(data) == 0:\n",
    "                data = annotation_data[\n",
    "                                   (annotation_data['split'] == 'test') & \n",
    "                                   (annotation_data['annotator_id'] > 0)\n",
    "                                  ]\n",
    "                data = data.groupby('audio_filename', group_keys=False).max()\n",
    "\n",
    "        c_arr = [np.count_nonzero(np.array(data[cls])) for cls in coarse_columns]\n",
    "        cdist[split] = [cnt/sum(c_arr) for cnt in c_arr]\n",
    "\n",
    "    cols = list(taxonomy['coarse'].values())\n",
    "    cls_df = pd.DataFrame.from_dict(cdist, orient='index', columns=cols)\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for sensor_name in sensors:\n",
    "            fig, ax = plt.subplots()\n",
    "            sensor_meta = meta_df[meta_df['sensor_name'] == sensor_name]\n",
    "            sid = sensor_df[sensor_df['sensor_name'] == sensor_name]['sensor_id'].unique()[0]            \n",
    "            txt = \"Sensor: {}, {}\".format(sid, sensor_name)\n",
    "            plt.text(0.05, 0.95, txt, transform=fig.transFigure, size=8)\n",
    "\n",
    "            # Plot the data distribution\n",
    "            data = annotation_data[(annotation_data['split'] == 'test') & \n",
    "                                   (annotation_data['annotator_id'] == 0) & \n",
    "                                   (annotation_data['sensor_id'] == sid)\n",
    "                                  ]\n",
    "            if len(data) == 0:\n",
    "                data = annotation_data[(annotation_data['split'] == 'test') & \n",
    "                       (annotation_data['annotator_id'] > 0) & \n",
    "                       (annotation_data['sensor_id'] == sid)\n",
    "                      ]\n",
    "                data = data.groupby('audio_filename', group_keys=False).max()\n",
    "\n",
    "            c_arr = [np.count_nonzero(np.array(data[cls])) for cls in coarse_columns]\n",
    "            cls_df.loc[sid] = [cnt/sum(c_arr) for cnt in c_arr]\n",
    "\n",
    "            cdist = cls_df.loc[['train', 'test', sid]]\n",
    "            cdist.T.plot.bar(ax=ax)\n",
    "            \n",
    "            plt.title('Class Distribution')  \n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.cla()\n",
    "            plt.clf()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_metrics_pdf(\n",
    "    pdf_path, \n",
    "    sensors, \n",
    "    classwise_pred_path, \n",
    "    overall_pred_path,\n",
    "    sensor_df,\n",
    "    meta_df,\n",
    "    meta_cols\n",
    "):\n",
    "    modes = ['classwise', 'overall']   \n",
    "    cls_df = pd.read_csv(classwise_pred_path)\n",
    "    overall_df = pd.read_csv(overall_pred_path)\n",
    "    \n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        # Plot the 3 graphs for each sensor in pdf\n",
    "        for sensor_name in sensors:\n",
    "            fig = plt.figure()\n",
    "            sensor_meta = meta_df[meta_df['sensor_name'] == sensor_name]\n",
    "            sid = sensor_df[sensor_df['sensor_name'] == sensor_name]['sensor_id'].unique()[0]\n",
    "            txt = \"Sensor: {}, {}\\n\".format(sid, sensor_name)\n",
    "            if len(sensor_meta) > 0:\n",
    "                #txt += \"Metadata: {}\".format(dict(sensor_meta.iloc[0][~pd.isna(sensor_meta.iloc[0])]))\n",
    "                grp_lst = [c for c in cols if sensor_meta.iloc[0][c] > 0] \n",
    "                txt += '; '.join(grp_lst)\n",
    "\n",
    "            for mode in modes:\n",
    "                # classwise performance \n",
    "                if mode == 'classwise':\n",
    "                    ax = fig.add_subplot(211)\n",
    "                    plt.text(0.05, 0.95, txt, transform=fig.transFigure, size=6)\n",
    "                    plot_title = 'Classwise AUPRC on Test Data'\n",
    "                    plt.ylim(0, 5)\n",
    "                    test1 = cls_df \n",
    "                    metrics = test1.columns[test1.columns != 'sensor_id']\n",
    "                    \n",
    "                # overall performance \n",
    "                else:\n",
    "                    ax = fig.add_subplot(212)\n",
    "                    plt.ylim(0, 4)\n",
    "                    plot_title = 'Metrics on Test Data'\n",
    "                    metrics = ['micro_f1', 'micro_auprc', 'macro_auprc']\n",
    "                    test1 = overall_df\n",
    "\n",
    "                test = test1[test1['sensor_id'] != sid]\n",
    "                evenly_spaced_interval = np.linspace(0, 1, len(metrics))\n",
    "                colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n",
    "                for i, metric in enumerate(metrics):\n",
    "                    s = test1[test1['sensor_id'] == sid].iloc[0][metric]\n",
    "                    sns.kdeplot(test[metric], color=colors[i], label=metric, ax=ax)\n",
    "\n",
    "                    xf, yf = ax.lines[i].get_data()\n",
    "                    y = np.interp(s, xf, yf)\n",
    "                    plt.vlines(s, ymin=0, ymax=y, color=colors[i], ls='--')\n",
    "\n",
    "                ax.set_title(plot_title)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.cla()\n",
    "            plt.clf()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_grp_metrics_kde_pdf(\n",
    "        pdf_path, \n",
    "        overall_pred_path, \n",
    "        grouped_df,\n",
    "        sensor_df,\n",
    "        meta_df\n",
    "    ):\n",
    "    \n",
    "    overall_df = pd.read_csv(overall_pred_path)\n",
    "    metrics = ['micro_f1', 'micro_auprc', 'macro_auprc']\n",
    "    evenly_spaced_interval = np.linspace(0, 1, len(metrics))\n",
    "    colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n",
    "            \n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for grp, df_group in grouped_df:\n",
    "            sensors = df_group.values\n",
    "            if len(sensors) > 1:\n",
    "                fig, ax = plt.subplots()\n",
    "                grp_lst = [c for c, g in zip(cols, grp) if g > 0] \n",
    "                txt = '; '.join(grp_lst)\n",
    "                txt += '\\n'\n",
    "                txt += '| '.join(sensors)  \n",
    "            \n",
    "                plt.text(0.05, 0.95, txt, transform=fig.transFigure, size=6)\n",
    "                test = overall_df[overall_df['sensor_name'].isin(sensors)]\n",
    "                for i, metric in enumerate(metrics):\n",
    "                    sns.kdeplot(test[metric], color=colors[i], label=metric, ax=ax)\n",
    "            \n",
    "                plt.tight_layout()\n",
    "                pdf.savefig()\n",
    "                plt.cla()\n",
    "                plt.clf()\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "# Meta data attributes\n",
    "cols = ['near_construction', 'on_thoroughfare', 'near_park',\n",
    "       'near_dogpark', 'near_highway', 'near_commercial', 'nyu_location',\n",
    "       'nyu_surroundings', 'near_transporthub', 'near_touristspot',\n",
    "       'bus_route']\n",
    "\n",
    "meta_df = pd.read_csv(os.path.join(META_FOLDER, 'node_meta.csv'))\n",
    "sensor_df = pd.read_csv(os.path.join(DATA_FOLDER, 'sensor_split_ids_{}.csv'.format(version)))\n",
    "\n",
    "# Replace nan with 0\n",
    "meta_df = meta_df.fillna(0)\n",
    "meta_df['sensor_name'] = meta_df['node_id'].apply(lambda x: x[10:-6])\n",
    "s_in_test = sensor_df[sensor_df['split'] == 'test']['sensor_name'].unique().tolist()\n",
    "s_in_train = sensor_df[sensor_df['split'] == 'train']['sensor_name'].unique().tolist()\n",
    "                            \n",
    "# Sensors in test but not in train with or without metadata information                            \n",
    "test_sensors = sensor_df[(sensor_df['sensor_name'].isin(s_in_test)) &\n",
    "                         (~sensor_df['sensor_name'].isin(s_in_train))\n",
    "                        ]['sensor_name']\n",
    "                            \n",
    "# Sensors in both test and train with metadata present                           \n",
    "train_test_sensors = meta_df[(meta_df['sensor_name'].isin(s_in_test)) & \n",
    "                             (meta_df['sensor_name'].isin(s_in_train))\n",
    "                            ]['sensor_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_dist_path = os.path.join(DATA_FOLDER, 'cls_dist_test_{}.pdf'.format(version))\n",
    "gen_clsdist_pdf(\n",
    "    cls_dist_path, \n",
    "    test_sensors,\n",
    "    sensor_df,\n",
    "    meta_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_pdf_path = os.path.join(DATA_FOLDER, 'metric_test_{}.pdf'.format(version))\n",
    "gen_metrics_pdf(\n",
    "    metric_pdf_path, \n",
    "    test_sensors,\n",
    "    classwise_pred_path, \n",
    "    overall_pred_path,\n",
    "    sensor_df,\n",
    "    meta_df,\n",
    "    cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_test_sensors) > 0:\n",
    "    cls_dist_path = os.path.join(DATA_FOLDER, 'cls_dist_test_train_{}.pdf'.format(version))\n",
    "    gen_clsdist_pdf(\n",
    "        cls_dist_path, \n",
    "        train_test_sensors,\n",
    "        sensor_df,\n",
    "        meta_df\n",
    "    )\n",
    "    \n",
    "    metric_tt_pdf_path = os.path.join(DATA_FOLDER, 'metric_train_test_{}.pdf'.format(version))\n",
    "    gen_metrics_pdf(\n",
    "        metric_tt_pdf_path, \n",
    "        train_test_sensors,\n",
    "        classwise_pred_path, \n",
    "        overall_pred_path,\n",
    "        sensor_df,\n",
    "        meta_df,\n",
    "        cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = False\n",
    "DATA_FOLDER = os.path.join('/scratch/sk7898/l3embedding/notebooks/data', version)\n",
    "# Group by the sensor metadata and plot distribution per group                            \n",
    "grouped_df = meta_df.groupby(cols)['sensor_name']\n",
    "\n",
    "# Generate the pdf\n",
    "if combine:\n",
    "    grp_pdf_path = os.path.join(DATA_FOLDER, 'meta_grp_metric_combined.pdf')\n",
    "    v1_pred_path = os.path.join(META_FOLDER, 'v0.4/per_sensor_test_v0.4.csv')\n",
    "    v2_pred_path = os.path.join(META_FOLDER, 'v2.2/per_sensor_test_v2.2.csv')\n",
    "    v1_df = pd.read_csv(v1_pred_path) \n",
    "    v2_df = pd.read_csv(v2_pred_path) \n",
    "    overall_df = pd.concat([v1_df, v2_df])  \n",
    "else:\n",
    "    grp_pdf_path = os.path.join(DATA_FOLDER, 'meta_grp_metric_{}.pdf'.format(version))\n",
    "    overall_pred_path = os.path.join(DATA_FOLDER, 'per_sensor_{}_{}.csv'.format(split, version))\n",
    "    overall_df = pd.read_csv(overall_pred_path)  \n",
    "    \n",
    "plt.rcParams.update({'font.size': 10}) \n",
    "metrics = ['micro_f1', 'micro_auprc', 'macro_auprc']\n",
    "colors = ['steelblue', 'darkorange', 'forestgreen']\n",
    "    \n",
    "grp_metrics = {}\n",
    "mlist = np.zeros(len(metrics), dtype=np.float32)\n",
    "n_sensors = 0\n",
    "\n",
    "for i, (grp, df_group) in enumerate(grouped_df):\n",
    "    sensors = df_group.values\n",
    "    grp_lst = [c for c, g in zip(cols, grp) if g > 0 and c!='nyu_location'] \n",
    "    grp_name = '| '.join(grp_lst)\n",
    "    test = overall_df[overall_df['sensor_name'].isin(sensors)]\n",
    "    if len(test) > 0:\n",
    "        #print(grp_name, test['sensor_name'].tolist())\n",
    "        grp_metrics[grp_name] = {}\n",
    "        n_sensors += len(test) \n",
    "        for i, metric in enumerate(metrics):\n",
    "            mlist[i] += np.sum(test[metric])\n",
    "            grp_metrics[grp_name][metric] = np.mean(test[metric])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "grp_metrics_df = pd.DataFrame.from_dict(grp_metrics, orient='index', columns=metrics)\n",
    "grp_metrics_df = grp_metrics_df.sort_values(by=['micro_f1'])\n",
    "grp_metrics_df.plot.bar(ax=ax, color=colors)\n",
    "x_min, x_max = ax.get_xlim()\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.hlines(mlist[i]/n_sensors, xmin=x_min, xmax=x_max, color=colors[i], ls='--')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(grp_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:l3embedding-tf-2-gpu]",
   "language": "python",
   "name": "conda-env-l3embedding-tf-2-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
