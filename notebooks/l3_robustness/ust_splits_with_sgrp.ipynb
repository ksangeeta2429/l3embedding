{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/l3embedding/classifier/sonyc_ust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%cd '/scratch/sk7898/l3embedding/classifier/sonyc_ust'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import resampy\n",
    "import librosa\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import cm\n",
    "from metrics import parse_coarse_prediction, micro_averaged_auprc, macro_averaged_auprc, evaluate_df\n",
    "from classify import load_embeddings, predict_mil, construct_mlp_mil\n",
    "# New modules: oyaml and pandas\n",
    "import oyaml as yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_lists(l1, l2):\n",
    "    l1.sort()\n",
    "    l2.sort()\n",
    "    return l1 == l2\n",
    "\n",
    "def get_split(row, cols, test_grps):\n",
    "    col_list = [c for c in cols if row[c] != 0]\n",
    "    for test_col in test_grps:\n",
    "        split = 'test' if equal_lists(col_list, test_col) else 'train_val'\n",
    "        if split == 'test':\n",
    "            break\n",
    "    row['new_split'] = split\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_df(meta_df, sensor_df, cols, test_grps):\n",
    "    meta_df['grp_id'] = None\n",
    "    # Replace nan with 0\n",
    "    meta_df = meta_df.fillna(0)\n",
    "    meta_df['sensor_name'] = meta_df['node_id'].apply(lambda x: x[10:-6])\n",
    "    meta_df['sensor_id'] = meta_df['sensor_name'].apply(\n",
    "                                        lambda x: sensor_df[sensor_df['sensor_name']==x].iloc[0]['sensor_id'] \n",
    "                                                    if len(sensor_df['sensor_name']==x) > 0 else None\n",
    "                                        )\n",
    "    gp = meta_df.groupby(cols)\n",
    "    keys = {k: i for i, k in enumerate(gp.groups.keys())}\n",
    "\n",
    "    for i, (grp, grp_df) in enumerate(gp):\n",
    "        idxs = grp_df.index.tolist()\n",
    "        meta_df.loc[idxs, 'grp_id'] = keys[grp]\n",
    "\n",
    "    meta_df = meta_df.apply(get_split, args=(cols, test_grps, ), axis=1)\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coarse_targets(taxonomy_path, cls):\n",
    "    with open(taxonomy_path, 'r') as f:\n",
    "        taxonomy = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "    # cls_dict = {k: v for k, v in taxonomy['coarse'].items() if v in cls_list}\n",
    "    cls_dict = {k: v for k, v in taxonomy['coarse'].items() if v == cls} \n",
    "    coarse_target_labels = [\"_\".join([str(k), v, 'presence'])\n",
    "                                for k, v in cls_dict.items()]\n",
    "    return coarse_target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_files(annotation_data, file_list, positive=True, valid_ratio=0.15, n_neg_files=None):\n",
    "\n",
    "    valid_files = []\n",
    "    cls_data = annotation_data[annotation_data['audio_filename'].isin(file_list)][['new_split', 'audio_filename', 'grp_id']].drop_duplicates()\n",
    "    \n",
    "    if not positive and n_neg_files and n_neg_files < len(file_list):\n",
    "        file_list = np.random.choice(file_list, n_neg_files, replace=False) \n",
    "        cls_data = cls_data[cls_data['audio_filename'].isin(file_list)]\n",
    "\n",
    "    grouped_df = cls_data[cls_data['new_split'] == 'train_val'].groupby('grp_id')\n",
    "\n",
    "    for i, (grp, df_group) in enumerate(grouped_df):\n",
    "        files = df_group['audio_filename']\n",
    "        n_valid = int(valid_ratio * len(files))\n",
    "        s = np.random.choice(files, n_valid, replace=False)\n",
    "        for i in range(n_valid):\n",
    "            valid_files.append(s[i])\n",
    "            \n",
    "    train_files = [f for f in file_list if f not in valid_files]\n",
    " \n",
    "    return train_files, valid_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_targets(annotation_data, file_list, labels):\n",
    "    target_list = []\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_df = annotation_data[annotation_data['audio_filename'] == filename]\n",
    "        target = []\n",
    "\n",
    "        for label in labels:\n",
    "            count = 0\n",
    "\n",
    "            for _, row in file_df.iterrows():\n",
    "                if int(row['annotator_id']) == 0:\n",
    "                    # If we have a validated annotation, just use that\n",
    "                    count = row[label]\n",
    "                    break\n",
    "                else:\n",
    "                    count += row[label]\n",
    "\n",
    "            if count > 0:\n",
    "                target.append(1.0)\n",
    "            else:\n",
    "                target.append(0.0)\n",
    "\n",
    "        target_list.append(target)\n",
    "\n",
    "    return np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "version = 'v0.4'\n",
    "cls_list = ['engine', 'machinery-impact', 'non-machinery-impact', 'powered-saw',\n",
    "            'alert-signal', 'music', 'human-voice', 'dog'\n",
    "           ]\n",
    "SONYC_PATH = '/scratch/work/sonyc/sonyc/ust/annotations'\n",
    "META_FOLDER = '/scratch/sk7898/l3embedding/notebooks/data'\n",
    "DATA_FOLDER = os.path.join('/scratch/sk7898/l3embedding/notebooks/data', version)\n",
    "\n",
    "\n",
    "# Meta data attributes\n",
    "meta_cols = ['near_construction', 'on_thoroughfare', 'near_park',\n",
    "       'near_dogpark', 'near_highway', 'near_commercial', 'nyu_location',\n",
    "       'nyu_surroundings', 'near_transporthub', 'near_touristspot',\n",
    "       'bus_route']\n",
    "\n",
    "test_grp_dict = {\n",
    "    'engine': [\n",
    "        ['nyu_location', 'nyu_surroundings', 'near_touristspot'], \n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_park', 'near_touristspot', 'bus_route']\n",
    "    ],\n",
    "    'machinery-impact': [\n",
    "        ['nyu_location', 'nyu_surroundings', 'near_touristspot'], \n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub'],\n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub', 'near_touristspot'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_park', 'near_touristspot', 'bus_route']\n",
    "    ],\n",
    "    'non-machinery-impact': [\n",
    "        ['nyu_location', 'nyu_surroundings', 'near_touristspot'], \n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub'],\n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub', 'near_touristspot'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_park', 'near_touristspot', 'bus_route']\n",
    "    ],\n",
    "    'powered-saw': [\n",
    "        ['nyu_location', 'nyu_surroundings', 'near_touristspot'], \n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub', 'near_touristspot'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_commercial', 'bus_route'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_park', 'near_touristspot', 'bus_route']\n",
    "    ],\n",
    "    'alert-signal': [\n",
    "        ['nyu_location', 'nyu_surroundings', 'near_touristspot'], \n",
    "        ['nyu_location', 'near_commercial'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_park', 'near_touristspot', 'bus_route']\n",
    "    ],\n",
    "    'music': [\n",
    "        ['nyu_location', 'nyu_surroundings', 'near_touristspot'],  \n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub', 'near_touristspot'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_commercial', 'near_transporthub', 'bus_route']\n",
    "        \n",
    "    ],\n",
    "    'human-voice':[\n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_commercial', 'bus_route'],\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_park', 'near_touristspot', 'bus_route']\n",
    "    ],\n",
    "    'dog':[\n",
    "        ['nyu_location', 'on_thoroughfare', 'near_commercial', 'near_transporthub', 'bus_route'],\n",
    "        ['nyu_location', 'near_park', 'near_commercial', 'near_transporthub', 'near_touristspot'],\n",
    "        ['nyu_location', 'near_construction', 'near_commercial']\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sensor groups in metadata:  [ 8 11  5]\n",
      "Train Valid sensor groups in metadata:  [ 3  0  6  7  4  2  1 10  9 12]\n"
     ]
    }
   ],
   "source": [
    "cls = cls_list[7]\n",
    "NEW_ANNOTATION_DIR = os.path.join(DATA_FOLDER, cls)\n",
    "taxonomy_path = os.path.join(SONYC_PATH, '{}/dcase-ust-taxonomy.yaml'.format(version))\n",
    "coarse_target_labels = get_coarse_targets(taxonomy_path, cls)\n",
    "\n",
    "if version == 'v2.2':\n",
    "    annotation_path = os.path.join(SONYC_PATH, 'latest/annotations_w_test_anns.csv')\n",
    "else:\n",
    "    annotation_path = os.path.join(SONYC_PATH, '{}/annotations.csv'.format(version))\n",
    "\n",
    "annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "annotation_data = annotation_data.merge(meta_df, on='sensor_id')\n",
    "\n",
    "test_grps = test_grp_dict[cls]\n",
    "sensor_df = pd.read_csv(os.path.join(DATA_FOLDER, 'sensor_split_ids_{}.csv'.format(version)))\n",
    "meta_df = pd.read_csv(os.path.join(META_FOLDER, 'node_meta.csv'))\n",
    "\n",
    "meta_df = get_meta_df(meta_df, sensor_df, meta_cols, test_grps)\n",
    "print('Test sensor groups in metadata: ',meta_df[meta_df['new_split'] == 'test']['grp_id'].unique())\n",
    "print('Train Valid sensor groups in metadata: ', meta_df[meta_df['new_split'] == 'train_val']['grp_id'].unique())\n",
    "\n",
    "# print('Test sensor groups in annotation: ', annotation_data[annotation_data['new_split'] == 'test']['grp_id'].unique())\n",
    "# print('Train Valid sensor groups in annotation: ', annotation_data[annotation_data['new_split'] == 'train_val']['grp_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train:250 \t#positive: 124 \t| #negative: 126\n",
      "Total Valid: 36 \t#positive: 19 \t| #negative: 17\n",
      "Total Test : 30 \t#positive: 15 \t| #negative: 15\n"
     ]
    }
   ],
   "source": [
    "file_list = annotation_data[annotation_data['new_split'] == 'train_val'].sort_values('audio_filename')['audio_filename'].unique().tolist()\n",
    "targets = get_file_targets(annotation_data, file_list, coarse_target_labels)\n",
    "\n",
    "pos_idxs = np.where(targets == 1)[0]\n",
    "neg_idxs = np.where(targets == 0)[0]\n",
    "pos_files, neg_files = np.array(file_list)[pos_idxs].tolist(), np.array(file_list)[neg_idxs].tolist()\n",
    "train_val = annotation_data[annotation_data['new_split'] == 'train_val']\n",
    "\n",
    "pos_train_files, pos_valid_files = get_train_valid_files(\n",
    "                                        train_val,\n",
    "                                        pos_files,\n",
    "                                        positive=True, \n",
    "                                        valid_ratio=0.15, \n",
    "                                        n_neg_files=None\n",
    "                                    )\n",
    "\n",
    "neg_train_files, neg_valid_files = get_train_valid_files(\n",
    "                                        train_val,\n",
    "                                        neg_files,\n",
    "                                        positive=False, \n",
    "                                        valid_ratio=0.15, \n",
    "                                        n_neg_files=len(pos_train_files)+len(pos_valid_files)\n",
    "                                    )\n",
    "valid_files = pos_valid_files + neg_valid_files\n",
    "train_files = pos_train_files + neg_train_files\n",
    "\n",
    "test_list = annotation_data[annotation_data['new_split'] == 'test'].sort_values('audio_filename')['audio_filename'].unique().tolist()\n",
    "targets = get_file_targets(annotation_data, test_list, coarse_target_labels)\n",
    "test_pos_idxs = np.where(targets == 1)[0]\n",
    "test_neg_idxs = np.where(targets == 0)[0]\n",
    "pos_test_files = np.array(test_list)[test_pos_idxs].tolist()\n",
    "neg_test_files = np.array(test_list)[test_neg_idxs].tolist()\n",
    "\n",
    "n_neg_files = min(len(pos_test_files), len(neg_test_files))\n",
    "if n_neg_files < len(neg_test_files):\n",
    "    reduced_neg_list = np.random.choice(neg_test_files, n_neg_files, replace=False) \n",
    "    reduced_test_files = pos_test_files + reduced_neg_list.tolist()\n",
    "    annotation_data.drop(annotation_data[\n",
    "                        (annotation_data['new_split'] == 'test') & \n",
    "                        (~annotation_data['audio_filename'].isin(reduced_test_files))\n",
    "                   ].index, inplace=True) \n",
    "\n",
    "print('Total Train:{} \\t#positive: {} \\t| #negative: {}'.format(len(train_files), len(pos_train_files), len(neg_train_files)))\n",
    "print('Total Valid: {} \\t#positive: {} \\t| #negative: {}'.format(len(valid_files), len(pos_valid_files), len(neg_valid_files)))\n",
    "print('Total Test : {} \\t#positive: {} \\t| #negative: {}'.format(len(pos_test_files)+n_neg_files, len(pos_test_files), n_neg_files))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sensor groups in annotation:  [ 8  5 11]\n",
      "Train sensor groups in annotation:  [ 6  7  1  0 10  3  2]\n",
      "Valid sensor groups in annotation:  [ 6  7  1  0 10  3  2]\n"
     ]
    }
   ],
   "source": [
    "for f in train_files:\n",
    "    annotation_data.loc[((annotation_data['audio_filename'] == f) & (annotation_data['new_split'] == 'train_val')), 'new_split'] = 'train'\n",
    "    \n",
    "for f in valid_files:\n",
    "    annotation_data.loc[((annotation_data['audio_filename'] == f) & (annotation_data['new_split'] == 'train_val')), 'new_split'] = 'validate'    \n",
    "    \n",
    "l1 = len(annotation_data[annotation_data['new_split'] == 'train']['audio_filename'].unique().tolist())\n",
    "l2 = len(annotation_data[annotation_data['new_split'] == 'validate']['audio_filename'].unique().tolist())\n",
    "assert (l1 + l2) == (len(train_files) + len(valid_files)) \n",
    "\n",
    "annotation_data.drop(annotation_data[annotation_data['new_split'] == 'train_val'].index, inplace=True) \n",
    "print('Test sensor groups in annotation: ', annotation_data[annotation_data['new_split'] == 'test']['grp_id'].unique())\n",
    "print('Train sensor groups in annotation: ', annotation_data[annotation_data['new_split'] == 'train']['grp_id'].unique())\n",
    "print('Valid sensor groups in annotation: ', annotation_data[annotation_data['new_split'] == 'train']['grp_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(NEW_ANNOTATION_DIR):\n",
    "    os.makedirs(NEW_ANNOTATION_DIR)\n",
    "    \n",
    "annotation_path = os.path.join(NEW_ANNOTATION_DIR, 'annotations.csv')\n",
    "annotation_data.to_csv(annotation_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:l3embedding-tf-2-gpu]",
   "language": "python",
   "name": "conda-env-l3embedding-tf-2-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
