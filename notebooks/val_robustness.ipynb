{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "#import umap\n",
    "import io\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from decrypt import read_encrypted_tar_audio_file\n",
    "from kapre.time_frequency import Melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_windows_from_encrypted_audio(audio_path, tar_data, sample_rate=8000, clip_duration=10,\n",
    "                                         decrypt_url='https://decrypt-sonyc.engineering.nyu.edu/decrypt',\n",
    "                                         cacert_path='/home/jtc440/sonyc/decrypt/CA.pem',\n",
    "                                         cert_path='/home/jtc440/sonyc/decrypt/jason_data.pem',\n",
    "                                         key_path='/home/jtc440/sonyc/decrypt/sonyc_key.pem'):\n",
    "    \n",
    "    audio = read_encrypted_tar_audio_file(audio_path,\n",
    "                                          enc_tar_filebuf=tar_data,\n",
    "                                          sample_rate=sample_rate,\n",
    "                                          url=decrypt_url,\n",
    "                                          cacert=cacert_path,\n",
    "                                          cert=cert_path,\n",
    "                                          key=key_path)[0]\n",
    "    if audio is None:\n",
    "        return None\n",
    "\n",
    "    audio_len = int(sample_rate * clip_duration)\n",
    "\n",
    "    # Make sure audio is all consistent length (10 seconds)\n",
    "    if len(audio) > audio_len:\n",
    "        audio = audio[:audio_len]\n",
    "    elif len(audio) < audio_len:\n",
    "        pad_len = audio_len - len(audio)\n",
    "        audio = np.pad(audio, (0, pad_len), mode='constant')\n",
    "\n",
    "    # Return raw windows\n",
    "    return get_audio_windows(audio, sr=sample_rate)\n",
    "\n",
    "\n",
    "def get_audio_windows(audio, sr=8000, center=True, hop_size=0.5):\n",
    "    \"\"\"\n",
    "    Similar to openl3.get_embedding(...)\n",
    "    \"\"\"\n",
    "\n",
    "    def _center_audio(audio, frame_len):\n",
    "        \"\"\"Center audio so that first sample will occur in the middle of the first frame\"\"\"\n",
    "        return np.pad(audio, (int(frame_len / 2.0), 0), mode='constant', constant_values=0)\n",
    "\n",
    "    def _pad_audio(audio, frame_len, hop_len):\n",
    "        \"\"\"Pad audio if necessary so that all samples are processed\"\"\"\n",
    "        audio_len = audio.size\n",
    "        if audio_len < frame_len:\n",
    "            pad_length = frame_len - audio_len\n",
    "        else:\n",
    "            pad_length = int(np.ceil((audio_len - frame_len) / float(hop_len))) * hop_len \\\n",
    "                         - (audio_len - frame_len)\n",
    "\n",
    "        if pad_length > 0:\n",
    "            audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)\n",
    "\n",
    "        return audio\n",
    "\n",
    "    # Check audio array dimension\n",
    "    if audio.ndim > 2:\n",
    "        raise AssertionError('Audio array can only be be 1D or 2D')\n",
    "    elif audio.ndim == 2:\n",
    "        # Downmix if multichannel\n",
    "        audio = np.mean(audio, axis=1)\n",
    "\n",
    "    audio_len = audio.size\n",
    "    frame_len = sr\n",
    "    hop_len = int(hop_size * sr)\n",
    "\n",
    "    if audio_len < frame_len:\n",
    "        warnings.warn('Duration of provided audio is shorter than window size (1 second). Audio will be padded.')\n",
    "\n",
    "    if center:\n",
    "        # Center audio\n",
    "        audio = _center_audio(audio, frame_len)\n",
    "\n",
    "    # Pad if necessary to ensure that we process all samples\n",
    "    audio = _pad_audio(audio, frame_len, hop_len)\n",
    "\n",
    "    # Split audio into frames, copied from librosa.util.frame\n",
    "    n_frames = 1 + int((len(audio) - frame_len) / float(hop_len))\n",
    "    x = np.lib.stride_tricks.as_strided(audio, shape=(frame_len, n_frames),\n",
    "                                        strides=(audio.itemsize, hop_len * audio.itemsize)).T\n",
    "\n",
    "    # Add a channel dimension\n",
    "    # x = x.reshape((x.shape[0], 1, x.shape[-1]))\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_audio_feats(audio_dir, feats_dir, indices_dir, path, n_feats=100):\n",
    "    feats_list = []\n",
    "    audio_list = [] \n",
    "    h5_path = os.path.join(feats_dir, path)\n",
    "    f = h5py.File(h5_path, 'r')\n",
    "    num_datasets = f[list(f.keys())[0]].shape[0]\n",
    "    \n",
    "    for i in range(n_feats):\n",
    "        dataset_index = np.random.randint(0, num_datasets)\n",
    "        num_features = f[list(f.keys())[0]][dataset_index]['openl3'].shape[0]\n",
    "        index = h5py.File(\n",
    "            os.path.join(\n",
    "                indices_dir, \n",
    "                os.path.basename(h5_path).split('.')[0]+'.sonyc_recording_index.h5'), 'r'\n",
    "                )\n",
    "        audio_file_name = os.path.join(audio_dir,\n",
    "                                       index[list(index.keys())[0]][dataset_index]['day_hdf5_path'].decode()\n",
    "                                       )\n",
    "        row = index[list(index.keys())[0]][dataset_index]['day_h5_index']\n",
    "        audio_file = h5py.File(audio_file_name, 'r')\n",
    "        tar_data = io.BytesIO(audio_file['recordings'][row]['data'])\n",
    "        raw_audio = get_raw_windows_from_encrypted_audio(audio_file_name, tar_data, sample_rate=8000)\n",
    "\n",
    "        if raw_audio is None:  \n",
    "            continue\n",
    "        feature_index = random.sample(range(num_features), 10)\n",
    "        feats_list.append(f[list(f.keys())[0]][dataset_index]['openl3'][feature_index])\n",
    "        audio_list.append(raw_audio[feature_index])\n",
    "\n",
    "    return np.array(audio_list).reshape(-1, 8000), np.array(feats_list).reshape(-1, 512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "#'/scratch/sk7898/embedding_approx_mse/models/sonyc/pca/dpp/day/500000/pca_batch_500000_len_128_kernel_linear/8000_64_160_1024_half_fmax_None/20201004094550'\n",
    "model_dir = '/scratch/sk7898/embedding_approx_mse/models/sonyc/mse_original/8000_64_160_1024_fmax_None/20200909145902'\n",
    "weight_path = os.path.join(model_dir, 'model_best_valid_loss.h5')\n",
    "model = models.load_model(weight_path, custom_objects={'Melspectrogram': Melspectrogram})\n",
    "\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfiles_dict = {}\n",
    "data_dir = '/scratch/sk7898/sonyc_30mil/train'\n",
    "parts = random.sample(range(15), 7)\n",
    "\n",
    "for part in parts:\n",
    "    splits = random.sample(range(2000), 100)\n",
    "    trfiles_dict[part] = []\n",
    "    for split in splits:\n",
    "        fname = 'sonyc_ndata=2500000_part={}_split={}.h5'.format(part, split)\n",
    "        if os.path.exists(os.path.join(data_dir, fname)):\n",
    "            trfiles_dict[part].append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part: 1 Files: 93\n",
      "Part: 9 Files: 92\n",
      "Part: 0 Files: 92\n",
      "Part: 2 Files: 92\n",
      "Part: 8 Files: 84\n",
      "Part: 10 Files: 93\n",
      "Part: 5 Files: 87\n"
     ]
    }
   ],
   "source": [
    "for k, v in trfiles_dict.items():\n",
    "    print('Part: {} Files: {}'.format(k, len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dict = {}\n",
    "embs_dict = {}\n",
    "n_feats = 100\n",
    "\n",
    "for sensor in trfiles_dict.keys():\n",
    "    mse_error = 0\n",
    "    embs_dict[sensor] = []\n",
    "    train_files = trfiles_dict[sensor]\n",
    "    for fname in train_files:\n",
    "        idxs = sorted(random.sample(range(1024), n_feats))\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        data_blob = h5py.File(data_batch_path, 'r')\n",
    "        audio_batch = np.array(data_blob['audio'][idxs])[:, np.newaxis, :]\n",
    "        ref_embs = data_blob['l3_embedding'][idxs]\n",
    "        pred_embs = model.predict(audio_batch)\n",
    "        embs_dict[sensor].append(pred_embs)\n",
    "        mse_error += np.mean((ref_embs - pred_embs)**2)\n",
    "    mse_dict[sensor] = mse_error/len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor: 1 Mean MSE: 0.08692146004528127\n",
      "Sensor: 9 Mean MSE: 0.0880258701255788\n",
      "Sensor: 0 Mean MSE: 0.09333788692627264\n",
      "Sensor: 2 Mean MSE: 0.07931454180051452\n",
      "Sensor: 8 Mean MSE: 0.09055087520253091\n",
      "Sensor: 10 Mean MSE: 0.08365342541727969\n",
      "Sensor: 5 Mean MSE: 0.08558810005585353\n"
     ]
    }
   ],
   "source": [
    "for k, v in mse_dict.items():\n",
    "    print('Sensor: {} Mean MSE: {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = {}\n",
    "n_feats = 20\n",
    "audio_dir = '/scratch/work/sonyc'\n",
    "indices_dir = '/scratch/work/sonyc/indices/2017'\n",
    "feats_dir = '/scratch/work/sonyc/features/openl3/2017'\n",
    "test_sensors = [\n",
    "    'sonycnode-b827ebc6dcc6.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827ebba613d.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827ebad073b.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827eb0fedda.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827eb44506f.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827eb122f0f.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827eb0d8af7.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827eb29eb77.sonyc_features_openl3.h5'\n",
    "]\n",
    "for i, path in enumerate(test_sensors):\n",
    "    embs_dict[30+i] = []\n",
    "    audio_list, feats_list = get_audio_feats(audio_dir, feats_dir, indices_dir, path, n_feats=n_feats)\n",
    "    test_error = 0\n",
    "    for audio, ref_embs in zip(audio_list, feats_list):\n",
    "        ref_embs = ref_embs.reshape(-1, 512)\n",
    "        audio_batch = audio.reshape((1, 1, audio.shape[-1]))\n",
    "        pred_embs = model.predict(audio_batch)\n",
    "        embs_dict[30+i].append(pred_embs)\n",
    "        test_error += np.mean((ref_embs - pred_embs)**2)\n",
    "    mse_test[path] = test_error/len(audio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor: sonycnode-b827ebc6dcc6.sonyc_features_openl3.h5 Mean MSE: 0.0812323743570596\n",
      "Sensor: sonycnode-b827ebba613d.sonyc_features_openl3.h5 Mean MSE: 0.08905987495556474\n",
      "Sensor: sonycnode-b827ebad073b.sonyc_features_openl3.h5 Mean MSE: 0.09444785553961993\n",
      "Sensor: sonycnode-b827eb0fedda.sonyc_features_openl3.h5 Mean MSE: 0.09134651693515479\n",
      "Sensor: sonycnode-b827eb44506f.sonyc_features_openl3.h5 Mean MSE: 0.1044423685502261\n",
      "Sensor: sonycnode-b827eb122f0f.sonyc_features_openl3.h5 Mean MSE: 0.10117710350081324\n",
      "Sensor: sonycnode-b827eb0d8af7.sonyc_features_openl3.h5 Mean MSE: 0.0779745231475681\n",
      "Sensor: sonycnode-b827eb29eb77.sonyc_features_openl3.h5 Mean MSE: 0.08979474962688982\n"
     ]
    }
   ],
   "source": [
    "for k, v in mse_test.items():\n",
    "    print('Sensor: {} Mean MSE: {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "clsses = []\n",
    "for k, v in embs_dict.items():\n",
    "    e = np.array(embs_dict[k]).reshape(-1, 512)\n",
    "    clsses += [k for i in range(e.shape[0])]\n",
    "    if embeddings is None:\n",
    "        embeddings = e\n",
    "    else:\n",
    "        embeddings = np.concatenate((embeddings, e), axis=0)\n",
    "\n",
    "with open('test_embs.npy', 'wb') as f:\n",
    "    np.save(f, embeddings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-be42fb5183ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mumap_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m plt.scatter(umap_projection[:, 0], umap_projection[:, 1], \n\u001b[1;32m      5\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclsses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "# umap_projection = umap.UMAP(random_state=42, n_neighbors=40, n_components=2, metric='euclidean').fit_transform(embeddings) \n",
    "# plt.scatter(umap_projection[:, 0], umap_projection[:, 1], \n",
    "#             c=clsses, \n",
    "#             edgecolor='none', \n",
    "#             alpha=0.80, \n",
    "#             s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('sonyc-research-data': conda)",
   "language": "python",
   "name": "python36964bitsonycresearchdataconda566197082bbe4a9e9b0f46f74d889d94"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
