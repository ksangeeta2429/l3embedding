{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitsonycresearchdataconda566197082bbe4a9e9b0f46f74d889d94",
   "display_name": "Python 3.6.9 64-bit ('sonyc-research-data': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import io\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from decrypt import read_encrypted_tar_audio_file\n",
    "from kapre.time_frequency import Melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_windows_from_encrypted_audio(audio_path, tar_data, sample_rate=8000, clip_duration=10,\n",
    "                                         decrypt_url='https://decrypt-sonyc.engineering.nyu.edu/decrypt',\n",
    "                                         cacert_path='/home/jtc440/sonyc/decrypt/CA.pem',\n",
    "                                         cert_path='/home/jtc440/sonyc/decrypt/jason_data.pem',\n",
    "                                         key_path='/home/jtc440/sonyc/decrypt/sonyc_key.pem'):\n",
    "    \n",
    "    audio = read_encrypted_tar_audio_file(audio_path,\n",
    "                                          enc_tar_filebuf=tar_data,\n",
    "                                          sample_rate=sample_rate,\n",
    "                                          url=decrypt_url,\n",
    "                                          cacert=cacert_path,\n",
    "                                          cert=cert_path,\n",
    "                                          key=key_path)[0]\n",
    "    if audio is None:\n",
    "        return None\n",
    "\n",
    "    audio_len = int(sample_rate * clip_duration)\n",
    "\n",
    "    # Make sure audio is all consistent length (10 seconds)\n",
    "    if len(audio) > audio_len:\n",
    "        audio = audio[:audio_len]\n",
    "    elif len(audio) < audio_len:\n",
    "        pad_len = audio_len - len(audio)\n",
    "        audio = np.pad(audio, (0, pad_len), mode='constant')\n",
    "\n",
    "    # Return raw windows\n",
    "    return get_audio_windows(audio, sr=sample_rate)\n",
    "\n",
    "\n",
    "def get_audio_windows(audio, sr=8000, center=True, hop_size=0.5):\n",
    "    \"\"\"\n",
    "    Similar to openl3.get_embedding(...)\n",
    "    \"\"\"\n",
    "\n",
    "    def _center_audio(audio, frame_len):\n",
    "        \"\"\"Center audio so that first sample will occur in the middle of the first frame\"\"\"\n",
    "        return np.pad(audio, (int(frame_len / 2.0), 0), mode='constant', constant_values=0)\n",
    "\n",
    "    def _pad_audio(audio, frame_len, hop_len):\n",
    "        \"\"\"Pad audio if necessary so that all samples are processed\"\"\"\n",
    "        audio_len = audio.size\n",
    "        if audio_len < frame_len:\n",
    "            pad_length = frame_len - audio_len\n",
    "        else:\n",
    "            pad_length = int(np.ceil((audio_len - frame_len) / float(hop_len))) * hop_len \\\n",
    "                         - (audio_len - frame_len)\n",
    "\n",
    "        if pad_length > 0:\n",
    "            audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)\n",
    "\n",
    "        return audio\n",
    "\n",
    "    # Check audio array dimension\n",
    "    if audio.ndim > 2:\n",
    "        raise AssertionError('Audio array can only be be 1D or 2D')\n",
    "    elif audio.ndim == 2:\n",
    "        # Downmix if multichannel\n",
    "        audio = np.mean(audio, axis=1)\n",
    "\n",
    "    audio_len = audio.size\n",
    "    frame_len = sr\n",
    "    hop_len = int(hop_size * sr)\n",
    "\n",
    "    if audio_len < frame_len:\n",
    "        warnings.warn('Duration of provided audio is shorter than window size (1 second). Audio will be padded.')\n",
    "\n",
    "    if center:\n",
    "        # Center audio\n",
    "        audio = _center_audio(audio, frame_len)\n",
    "\n",
    "    # Pad if necessary to ensure that we process all samples\n",
    "    audio = _pad_audio(audio, frame_len, hop_len)\n",
    "\n",
    "    # Split audio into frames, copied from librosa.util.frame\n",
    "    n_frames = 1 + int((len(audio) - frame_len) / float(hop_len))\n",
    "    x = np.lib.stride_tricks.as_strided(audio, shape=(frame_len, n_frames),\n",
    "                                        strides=(audio.itemsize, hop_len * audio.itemsize)).T\n",
    "\n",
    "    # Add a channel dimension\n",
    "    # x = x.reshape((x.shape[0], 1, x.shape[-1]))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/sonyc-research-data/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#'/scratch/sk7898/embedding_approx_mse/models/sonyc/pca/dpp/day/500000/pca_batch_500000_len_128_kernel_linear/8000_64_160_1024_half_fmax_None/20201004094550'\n",
    "model_dir = '/scratch/sk7898/embedding_approx_mse/models/sonyc/mse_original/8000_64_160_1024_fmax_None/20200909145902'\n",
    "weight_path = os.path.join(model_dir, 'model_best_valid_loss.h5')\n",
    "model = models.load_model(weight_path, custom_objects={'Melspectrogram': Melspectrogram})\n",
    "\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47 from training set: ['sonyc_ndata=2500000_part=7_split=1264.h5', 'sonyc_ndata=2500000_part=7_split=25.h5', 'sonyc_ndata=2500000_part=5_split=557.h5', 'sonyc_ndata=2500000_part=12_split=1498.h5', 'sonyc_ndata=2500000_part=10_split=965.h5', 'sonyc_ndata=2500000_part=7_split=1969.h5', 'sonyc_ndata=2500000_part=9_split=516.h5', 'sonyc_ndata=2500000_part=8_split=464.h5', 'sonyc_ndata=2500000_part=4_split=1341.h5', 'sonyc_ndata=2500000_part=8_split=364.h5', 'sonyc_ndata=2500000_part=9_split=352.h5', 'sonyc_ndata=2500000_part=1_split=1715.h5', 'sonyc_ndata=2500000_part=11_split=226.h5', 'sonyc_ndata=2500000_part=2_split=919.h5', 'sonyc_ndata=2500000_part=10_split=508.h5', 'sonyc_ndata=2500000_part=0_split=819.h5', 'sonyc_ndata=2500000_part=0_split=1617.h5', 'sonyc_ndata=2500000_part=10_split=1671.h5', 'sonyc_ndata=2500000_part=2_split=91.h5', 'sonyc_ndata=2500000_part=14_split=1583.h5', 'sonyc_ndata=2500000_part=11_split=1558.h5', 'sonyc_ndata=2500000_part=5_split=1681.h5', 'sonyc_ndata=2500000_part=12_split=1194.h5', 'sonyc_ndata=2500000_part=5_split=1927.h5', 'sonyc_ndata=2500000_part=13_split=456.h5', 'sonyc_ndata=2500000_part=2_split=1274.h5', 'sonyc_ndata=2500000_part=5_split=1571.h5', 'sonyc_ndata=2500000_part=5_split=1917.h5', 'sonyc_ndata=2500000_part=9_split=861.h5', 'sonyc_ndata=2500000_part=1_split=1623.h5', 'sonyc_ndata=2500000_part=7_split=1876.h5', 'sonyc_ndata=2500000_part=0_split=1601.h5', 'sonyc_ndata=2500000_part=7_split=153.h5', 'sonyc_ndata=2500000_part=13_split=141.h5', 'sonyc_ndata=2500000_part=11_split=1847.h5', 'sonyc_ndata=2500000_part=2_split=425.h5', 'sonyc_ndata=2500000_part=3_split=677.h5', 'sonyc_ndata=2500000_part=4_split=24.h5', 'sonyc_ndata=2500000_part=13_split=777.h5', 'sonyc_ndata=2500000_part=4_split=1337.h5', 'sonyc_ndata=2500000_part=8_split=1644.h5', 'sonyc_ndata=2500000_part=7_split=1999.h5', 'sonyc_ndata=2500000_part=9_split=1572.h5', 'sonyc_ndata=2500000_part=2_split=1831.h5', 'sonyc_ndata=2500000_part=5_split=787.h5', 'sonyc_ndata=2500000_part=6_split=650.h5', 'sonyc_ndata=2500000_part=2_split=1155.h5']\n"
     ]
    }
   ],
   "source": [
    "train_files = []\n",
    "data_dir = '/scratch/sk7898/sonyc_30mil/train'\n",
    "\n",
    "for i in range(60):\n",
    "    part = random.randint(0, 15)\n",
    "    split = random.randint(0, 2000)\n",
    "    fname = 'sonyc_ndata=2500000_part={}_split={}.h5'.format(part, split)\n",
    "    if os.path.exists(os.path.join(data_dir, fname)):\n",
    "        train_files.append(fname)\n",
    "\n",
    "print('{} from training set: {}'.format(len(train_files), train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error between openl3 reference embeddings and reduced input student model's predicted embeddings\n",
      "MSE on Training subset:  0.0871038207031311\n"
     ]
    }
   ],
   "source": [
    "mse_error = 0\n",
    "print('Error between openl3 reference embeddings and reduced input student model\\'s predicted embeddings')\n",
    "\n",
    "for fname in train_files:\n",
    "    idxs = sorted(random.sample(range(1024), 10))\n",
    "    data_batch_path = os.path.join(data_dir, fname)\n",
    "    data_blob = h5py.File(data_batch_path, 'r')\n",
    "    audio_batch = np.array(data_blob['audio'][idxs])[:, np.newaxis, :]\n",
    "    ref_embs = data_blob['l3_embedding'][idxs]\n",
    "    pred_embs = model.predict(audio_batch)\n",
    "    mse_error += np.mean((ref_embs - pred_embs)**2)\n",
    "print('MSE on Training subset: ', mse_error/len(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50 50\n"
     ]
    }
   ],
   "source": [
    "feats_list = []\n",
    "audio_list = []\n",
    "audio_dir = '/scratch/work/sonyc'\n",
    "indices_dir = '/scratch/work/sonyc/indices/2017'\n",
    "feats_dir = '/scratch/work/sonyc/features/openl3/2017'\n",
    "test_sensors = [\n",
    "    'sonycnode-b827ebc6dcc6.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827ebba613d.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827ebad073b.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827eb0fedda.sonyc_features_openl3.h5',\n",
    "    'sonycnode-b827eb44506f.sonyc_features_openl3.h5'\n",
    "]\n",
    "for path in test_sensors:\n",
    "    h5_path = os.path.join(feats_dir, path)\n",
    "    f = h5py.File(h5_path, 'r')\n",
    "    num_datasets = f[list(f.keys())[0]].shape[0]\n",
    "    for i in range(10):\n",
    "        dataset_index = np.random.randint(0, num_datasets)\n",
    "        num_features = f[list(f.keys())[0]][dataset_index]['openl3'].shape[0]\n",
    "        index = h5py.File(\n",
    "            os.path.join(\n",
    "                indices_dir, \n",
    "                os.path.basename(h5_path).split('.')[0]+'.sonyc_recording_index.h5'), 'r'\n",
    "                )\n",
    "        audio_file_name = os.path.join(audio_dir,\n",
    "                                       index[list(index.keys())[0]][dataset_index]['day_hdf5_path'].decode()\n",
    "                                       )\n",
    "        row = index[list(index.keys())[0]][dataset_index]['day_h5_index']\n",
    "        audio_file = h5py.File(audio_file_name, 'r')\n",
    "        tar_data = io.BytesIO(audio_file['recordings'][row]['data'])\n",
    "        raw_audio = get_raw_windows_from_encrypted_audio(audio_file_name, tar_data, sample_rate=8000)\n",
    "\n",
    "        if raw_audio is None:\n",
    "            continue\n",
    "        feature_index = np.random.randint(0, num_features)\n",
    "        feats_list.append(f[list(f.keys())[0]][dataset_index]['openl3'][feature_index])\n",
    "        audio_list.append(raw_audio[feature_index])\n",
    "\n",
    "print(len(audio_list), len(feats_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE on Test subset:  0.35140286549925803\n"
     ]
    }
   ],
   "source": [
    "test_error = 0\n",
    "for audio, ref_embs in zip(audio_list, feats_list):\n",
    "    audio_batch = audio.reshape((1, 1, audio.shape[-1]))\n",
    "    pred_embs = model.predict(audio_batch)\n",
    "    test_error += np.mean((ref_embs - pred_embs)**2)\n",
    "print('MSE on Test subset: ', mse_error/len(audio_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}