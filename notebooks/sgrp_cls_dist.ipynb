{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/l3embedding/classifier/sonyc_ust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%cd '/scratch/sk7898/l3embedding/classifier/sonyc_ust'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import librosa\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import load_model\n",
    "from matplotlib import cm\n",
    "from metrics import parse_coarse_prediction, micro_averaged_auprc, macro_averaged_auprc, evaluate_df\n",
    "from classify import load_embeddings, predict_mil, construct_mlp_mil\n",
    "# New modules: oyaml and pandas\n",
    "import oyaml as yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_targets(annotation_data, file_list, labels):\n",
    "    target_list = []\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_df = annotation_data[annotation_data['audio_filename'] == filename]\n",
    "        target = []\n",
    "\n",
    "        for label in labels:\n",
    "            count = 0\n",
    "\n",
    "            for _, row in file_df.iterrows():\n",
    "                if int(row['annotator_id']) == 0:\n",
    "                    # If we have a validated annotation, just use that\n",
    "                    count = row[label]\n",
    "                    break\n",
    "                else:\n",
    "                    count += row[label]\n",
    "\n",
    "            if count > 0:\n",
    "                target.append(1.0)\n",
    "            else:\n",
    "                target.append(0.0)\n",
    "\n",
    "        target_list.append(target)\n",
    "\n",
    "    return np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v0.4'\n",
    "SONYC_PATH = '/scratch/work/sonyc/sonyc/ust/annotations'\n",
    "META_FOLDER = '/scratch/sk7898/l3embedding/notebooks/data'\n",
    "DATA_FOLDER = os.path.join('/scratch/sk7898/l3embedding/notebooks/data', version)\n",
    "\n",
    "taxonomy_path = os.path.join(SONYC_PATH, '{}/dcase-ust-taxonomy.yaml'.format(version))\n",
    "if version == 'v2.2':\n",
    "    annotation_path = os.path.join(SONYC_PATH, 'latest/annotations_w_test_anns.csv')\n",
    "else:\n",
    "    annotation_path = os.path.join(SONYC_PATH, '{}/annotations.csv'.format(version))\n",
    "\n",
    "annotation_data = pd.read_csv(annotation_path).sort_values('audio_filename')\n",
    "with open(taxonomy_path, 'r') as f:\n",
    "    taxonomy = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "coarse_target_labels = [\"_\".join([str(k), v, 'presence']) for k, v in taxonomy['coarse'].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_engine_presence',\n",
       " '2_machinery-impact_presence',\n",
       " '3_non-machinery-impact_presence',\n",
       " '4_powered-saw_presence',\n",
       " '5_alert-signal_presence',\n",
       " '6_music_presence',\n",
       " '7_human-voice_presence',\n",
       " '8_dog_presence']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(os.path.join(META_FOLDER, 'node_meta.csv'))\n",
    "sensor_df = pd.read_csv(os.path.join(DATA_FOLDER, 'sensor_split_ids_{}.csv'.format(version)))\n",
    "\n",
    "# Replace nan with 0\n",
    "meta_df = meta_df.fillna(0)\n",
    "meta_df['sensor_name'] = meta_df['node_id'].apply(lambda x: x[10:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_cls_path = os.path.join(META_FOLDER, 'grp_cls_dist.csv')\n",
    "\n",
    "meta_cols = ['near_construction', 'on_thoroughfare', 'near_park',\n",
    "       'near_dogpark', 'near_highway', 'near_commercial', 'nyu_location',\n",
    "       'nyu_surroundings', 'near_transporthub', 'near_touristspot',\n",
    "       'bus_route']\n",
    "coarse_columns = [\n",
    "    '1_engine_presence', '2_machinery-impact_presence', \n",
    "    '3_non-machinery-impact_presence', '4_powered-saw_presence', \n",
    "    '5_alert-signal_presence', '6_music_presence',\n",
    "    '7_human-voice_presence', '8_dog_presence'\n",
    "]\n",
    "\n",
    "df2 = sensor_df[['sensor_id', 'sensor_name']].drop_duplicates()\n",
    "df = meta_df.merge(df2, how='left', on='sensor_name')\n",
    "final = annotation_data.merge(df, how='right', on='sensor_id')\n",
    "\n",
    "grouped_df = df.groupby(meta_cols)['sensor_id']\n",
    "\n",
    "grp_cls = {}\n",
    "for i, (grp, df_group) in enumerate(grouped_df):\n",
    "    sensors = df_group.values\n",
    "    grp_lst = [c for c, g in zip(meta_cols, grp) if g > 0 and c!='nyu_location'] \n",
    "    grp_name = '| '.join(grp_lst)\n",
    "    file_list = final[final['sensor_id'].isin(sensors)]['audio_filename'].unique().tolist()\n",
    "    \n",
    "    targets = get_file_targets(annotation_data, file_list, coarse_target_labels)\n",
    "    \n",
    "#     test = test.groupby('audio_filename', group_keys=False).max()\n",
    "#     if len(pos_test_files) > 0:\n",
    "    grp_cls[grp_name] = {}\n",
    "\n",
    "    for i, cls in enumerate(coarse_columns):\n",
    "        idxs = np.where(targets[:, i] == 1)\n",
    "        if idxs:\n",
    "            pos_idxs = idxs[0]\n",
    "            pos_test_files = np.array(file_list)[pos_idxs].tolist()\n",
    "            grp_cls[grp_name][cls] = len(pos_test_files) #len(test[test[cls] == 1])\n",
    "        else:\n",
    "            grp_cls[grp_name][cls] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the class counts are right before saving into csv\n",
    "# ann = annotation_data[annotation_data['sensor_id'].isin(final['sensor_id'].unique().tolist())]\n",
    "# ann_test = ann.groupby('audio_filename', group_keys=False).max()\n",
    "\n",
    "# for i, cls in enumerate(coarse_columns):\n",
    "#     assert cls_df[cls].sum() == len(ann_test[ann_test[cls] == 1])\n",
    "\n",
    "cls_df = pd.DataFrame.from_dict(grp_cls, orient='index', columns=coarse_columns)\n",
    "cls_df['sensor_group'] = cls_df.index.tolist()\n",
    "cls_df.to_csv(grp_cls_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:l3embedding-tf-2-gpu]",
   "language": "python",
   "name": "conda-env-l3embedding-tf-2-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
