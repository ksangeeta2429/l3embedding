{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import resampy\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from log import *\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from kapre.time_frequency import Melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger('quantized_inference')\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr):\n",
    "    \"\"\"\n",
    "    Load audio file\n",
    "    \"\"\"\n",
    "    data, sr_orig = sf.read(path, dtype='float32', always_2d=True)\n",
    "    data = data.mean(axis=-1)\n",
    "\n",
    "    if sr_orig != sr:\n",
    "        data = resampy.resample(data, sr_orig, sr)\n",
    "\n",
    "    return data\n",
    "\n",
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec\n",
    "\n",
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_variables(sess):\n",
    "    if hasattr(tf, 'global_variables'):\n",
    "        variables = tf.global_variables()\n",
    "    else:\n",
    "        variables = tf.all_variables()\n",
    "\n",
    "    #print(variables)\n",
    "    uninitialized_variables = []\n",
    "    for v in variables:\n",
    "        if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n",
    "            uninitialized_variables.append(v)\n",
    "            v._keras_initialized = True\n",
    "    \n",
    "    #print(uninitialized_variables)\n",
    "    if uninitialized_variables:\n",
    "        if hasattr(tf, 'variables_initializer'):\n",
    "            sess.run(tf.variables_initializer(uninitialized_variables))\n",
    "        else:\n",
    "            sess.run(tf.initialize_variables(uninitialized_variables)) \n",
    "            \n",
    "def get_l3model(model_path, saved_model_type='keras'):\n",
    "    \n",
    "    if saved_model_type == 'keras': \n",
    "        model = keras.models.load_model(model_path, custom_objects={'Melspectrogram': Melspectrogram})\n",
    "        if 'flatten' in model.layers[-1].name:\n",
    "            print(\"Flatten Layer is part of model\")\n",
    "            l3embedding_model = model\n",
    "        else:\n",
    "            embed_layer = model.get_layer('audio_embedding_layer')\n",
    "            pool_size = tuple(embed_layer.get_output_shape_at(0)[1:3])\n",
    "            y_a = keras.layers.MaxPooling2D(pool_size=pool_size, padding='same')(model.output)\n",
    "            y_a = keras.layers.Flatten()(y_a)\n",
    "            l3embedding_model = keras.models.Model(inputs=model.input, outputs=y_a)\n",
    "        \n",
    "    elif saved_model_type == 'tflite':\n",
    "        tflite_model_file = model_path\n",
    "        l3embedding_model = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "    else:\n",
    "        l3embedding_model = model_path\n",
    "        \n",
    "    return l3embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_us8k_metadata(path):\n",
    "    \"\"\"\n",
    "    Load UrbanSound8K metadata\n",
    "    Args:\n",
    "        path: Path to metadata csv file\n",
    "              (Type: str)\n",
    "    Returns:\n",
    "        metadata: List of metadata dictionaries\n",
    "                  (Type: list[dict[str, *]])\n",
    "    \"\"\"\n",
    "    metadata = [{} for _ in range(10)]\n",
    "    with open(path) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            fname = row['slice_file_name']\n",
    "            row['start'] = float(row['start'])\n",
    "            row['end'] = float(row['end'])\n",
    "            row['salience'] = float(row['salience'])\n",
    "            fold_num = row['fold'] = int(row['fold'])\n",
    "            row['classID'] = int(row['classID'])\n",
    "            metadata[fold_num-1][fname] = row\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def get_l3_frames_uniform_keras_quantized(audio, model_path, n_fft=2048, n_mels=256,\\\n",
    "                                          mel_hop_length=242, hop_size=0.1, sr=48000,\\\n",
    "                                          with_melSpec=None, fmax=None, **kwargs):\n",
    "    if type(audio) == str:\n",
    "        audio = load_audio(audio, sr)\n",
    "\n",
    "    hop_size = hop_size\n",
    "    hop_length = int(hop_size * sr)\n",
    "    frame_length = sr * 1\n",
    "\n",
    "    audio_length = len(audio)\n",
    "    if audio_length < frame_length:\n",
    "        # Make sure we can have at least one frame of audio\n",
    "        pad_length = frame_length - audio_length\n",
    "    else:\n",
    "        # Zero pad so we compute embedding on all samples\n",
    "        pad_length = int(np.ceil(audio_length - frame_length)/hop_length) * hop_length \\\n",
    "                     - (audio_length - frame_length)\n",
    "\n",
    "    if pad_length > 0:\n",
    "        # Use (roughly) symmetric padding\n",
    "        left_pad = pad_length // 2\n",
    "        right_pad= pad_length - left_pad\n",
    "        audio = np.pad(audio, (left_pad, right_pad), mode='constant')\n",
    "    \n",
    "    if with_melSpec:\n",
    "        #print(\"Melspectrogram is part of the weight file\")\n",
    "        # Divide into overlapping 1 second frames\n",
    "        x = librosa.util.utils.frame(audio, frame_length=frame_length, hop_length=hop_length).T    \n",
    "        # Add a channel dimension\n",
    "        X = x.reshape((x.shape[0], 1, x.shape[-1]))\n",
    "    \n",
    "    else:\n",
    "        #print(\"Melspectrogram has been removed from the weight file\")\n",
    "        frames = librosa.util.utils.frame(audio, frame_length=frame_length, hop_length=hop_length).T\n",
    "\n",
    "        X = []\n",
    "        for frame in frames:\n",
    "            S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length,\n",
    "                                         window='hann', center=True,\n",
    "                                         pad_mode='constant'))\n",
    "            S = librosa.feature.melspectrogram(sr=sr, S=S, n_mels=n_mels, fmax=fmax,\n",
    "                                           power=1.0, htk=True)\n",
    "            S = amplitude_to_db(np.array(S))\n",
    "            X.append(S)\n",
    "\n",
    "        X = np.array(X)[:, :, :, np.newaxis]\n",
    "\n",
    "    # Get the L3 embedding for each frame\n",
    "    l3embedding = predict_quantized_model(model_path, X)\n",
    "\n",
    "    return l3embedding\n",
    "\n",
    "def compute_file_features(path, feature_type, l3embedding_model=None, model_type='keras', **feature_args):\n",
    "    \n",
    "    if model_type == 'quantized_keras' and type(l3embedding_model) != str:\n",
    "        raise ValueError('For quantized keras model, pass the model path instead of the model')\n",
    "        \n",
    "    if feature_type == 'l3':\n",
    "        if not l3embedding_model:\n",
    "            err_msg = 'Must provide L3 embedding model to use {} features'\n",
    "            raise ValueError(err_msg.format(feature_type))\n",
    "        #hop_size = feature_args.get('hop_size', 0.1)\n",
    "        #samp_rate = feature_args.get('samp_rate', 48000)\n",
    "        \n",
    "        if model_type == 'keras':\n",
    "            file_features = get_l3_frames_uniform(path, l3embedding_model, **feature_args)\n",
    "        elif model_type == 'quantized_keras':\n",
    "            file_features = get_l3_frames_uniform_keras_quantized(path, model_path=l3embedding_model, **feature_args)\n",
    "        elif model_type == 'tflite':\n",
    "            file_features = get_l3_frames_uniform_tflite(path, interpreter=l3embedding_model, **feature_args)\n",
    "        else:\n",
    "            raise ValueError('Model type not supported!')\n",
    "            \n",
    "    else:\n",
    "        raise ValueError('Invalid feature type: {}'.format(feature_type))\n",
    "\n",
    "    return file_features\n",
    "\n",
    "def generate_us8k_fold_data(metadata, data_dir, fold_idx, output_dir, l3embedding_model=None, model_type='keras',\n",
    "                            features='l3', random_state=12345678, **feature_args):\n",
    "    \"\"\"\n",
    "    Generate all of the data for a specific fold\n",
    "\n",
    "    Args:\n",
    "        metadata: List of metadata dictionaries, or a path to a metadata file to be loaded\n",
    "                  (Type: list[dict[str,*]] or str)\n",
    "\n",
    "        data_dir: Path to data directory\n",
    "                  (Type: str)\n",
    "\n",
    "        fold_idx: Index of fold to load\n",
    "                  (Type: int)\n",
    "\n",
    "        output_dir: Path to output directory where fold data will be stored\n",
    "                    (Type: str)\n",
    "\n",
    "    Keyword Args:\n",
    "        l3embedding_model: L3 embedding model, used if L3 features are used\n",
    "                           (Type: keras.engine.training.Model or None)\n",
    "\n",
    "        features: Type of features to be computed\n",
    "                  (Type: str)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if type(metadata) == str:\n",
    "        metadata = load_us8k_metadata(metadata)\n",
    "\n",
    "    # Set random seed\n",
    "    random_state = random_state + fold_idx\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    audio_fold_dir = os.path.join(data_dir, \"fold{}\".format(fold_idx+1))\n",
    "\n",
    "    # Create fold directory if it does not exist\n",
    "    output_fold_dir = os.path.join(output_dir, \"fold{}\".format(fold_idx+1))\n",
    "    if not os.path.isdir(output_fold_dir):\n",
    "        os.makedirs(output_fold_dir)\n",
    "\n",
    "    LOGGER.info('Generating fold {} in {}'.format(fold_idx+1, output_fold_dir))\n",
    "\n",
    "    num_files = len(metadata[fold_idx])\n",
    "\n",
    "    for idx, (fname, example_metadata) in enumerate(metadata[fold_idx].items()):\n",
    "        desc = '({}/{}) Processed {} -'.format(idx+1, num_files, fname)\n",
    "        with LogTimer(LOGGER, desc, log_level=logging.DEBUG):\n",
    "            # TODO: Make sure glob doesn't catch things with numbers afterwards\n",
    "            variants = [x for x in glob.glob(os.path.join(audio_fold_dir,\n",
    "                '**', os.path.splitext(fname)[0] + '[!0-9]*[wm][ap][v3]'), recursive=True)\n",
    "                if os.path.isfile(x) and not x.endswith('.jams')]\n",
    "            num_variants = len(variants)\n",
    "            for var_idx, var_path in enumerate(variants):\n",
    "                audio_dir = os.path.dirname(var_path)\n",
    "                var_fname = os.path.basename(var_path)\n",
    "                desc = '\\t({}/{}) Variants {} -'.format(var_idx+1, num_variants, var_fname)\n",
    "                with LogTimer(LOGGER, desc, log_level=logging.DEBUG):\n",
    "                    generate_us8k_file_data(var_fname, example_metadata, audio_dir,\n",
    "                                            output_fold_dir, features,\n",
    "                                            l3embedding_model, model_type, **feature_args)\n",
    "\n",
    "\n",
    "def generate_us8k_file_data(fname, example_metadata, audio_fold_dir,\n",
    "                            output_fold_dir, features,\n",
    "                            l3embedding_model, model_type, **feature_args):\n",
    "    audio_path = os.path.join(audio_fold_dir, fname)\n",
    "\n",
    "    basename, _ = os.path.splitext(fname)\n",
    "    output_path = os.path.join(output_fold_dir, basename + '.npz')\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        LOGGER.info('File {} already exists'.format(output_path))\n",
    "        return\n",
    "\n",
    "    X = compute_file_features(audio_path, features, l3embedding_model=l3embedding_model,\\\n",
    "                                           model_type=model_type, **feature_args)\n",
    "\n",
    "    # If we were not able to compute the features, skip this file\n",
    "    if X is None:\n",
    "        LOGGER.error('Could not generate data for {}'.format(audio_path))\n",
    "        return\n",
    "\n",
    "    class_label = example_metadata['classID']\n",
    "    y = class_label\n",
    "\n",
    "    np.savez_compressed(output_path, X=X, y=y)\n",
    "\n",
    "    return output_path, 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cd76f611dedb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                             \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                             \u001b[0mmel_hop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_hop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                             n_fft=n_dft, fmax=fmax, sr=samp_rate, with_melSpec=with_melSpec)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e0fb9f6ce5e8>\u001b[0m in \u001b[0;36mgenerate_us8k_fold_data\u001b[0;34m(metadata, data_dir, fold_idx, output_dir, l3embedding_model, model_type, features, random_state, **feature_args)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     generate_us8k_file_data(var_fname, example_metadata, audio_dir,\n\u001b[1;32m    164\u001b[0m                                             \u001b[0moutput_fold_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                                             l3embedding_model, model_type, **feature_args)\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e0fb9f6ce5e8>\u001b[0m in \u001b[0;36mgenerate_us8k_file_data\u001b[0;34m(fname, example_metadata, audio_fold_dir, output_fold_dir, features, l3embedding_model, model_type, **feature_args)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     X = compute_file_features(audio_path, features, l3embedding_model=l3embedding_model,\\\n\u001b[0;32m--> 181\u001b[0;31m                                            model_type=model_type, **feature_args)\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# If we were not able to compute the features, skip this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e0fb9f6ce5e8>\u001b[0m in \u001b[0;36mcompute_file_features\u001b[0;34m(path, feature_type, l3embedding_model, model_type, **feature_args)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mfile_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_l3_frames_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3embedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfeature_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quantized_keras'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mfile_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_l3_frames_uniform_keras_quantized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml3embedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfeature_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tflite'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfile_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_l3_frames_uniform_tflite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml3embedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfeature_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e0fb9f6ce5e8>\u001b[0m in \u001b[0;36mget_l3_frames_uniform_keras_quantized\u001b[0;34m(audio, model_path, n_fft, n_mels, mel_hop_length, hop_size, sr, with_melSpec, fmax, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Get the L3 embedding for each frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0ml3embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_quantized_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml3embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d1669eeb7d87>\u001b[0m in \u001b[0;36mpredict_quantized_model\u001b[0;34m(model_path, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0minitialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_sess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    model_path = '/scratch/sk7898/l3pruning/embedding_approx_mse/embedding_approx/music/48000_256_242_2048_fmax_None/quantized_mse_original/20190930153319/model_best_valid_loss.h5'\n",
    "    fold_num = 1\n",
    "    metadata_path = '/beegfs/jtc440/UrbanSound8K/metadata/UrbanSound8K.csv'\n",
    "    data_dir = '/beegfs/jtc440/UrbanSound8K/audio'\n",
    "    dataset_output_dir = '/scratch/sk7898/test_quant_keras'\n",
    "    random_state = 20180302\n",
    "    samp_rate = 48000\n",
    "    n_mels = 256\n",
    "    n_hop = 242\n",
    "    n_dft = 2048 \n",
    "    fmax=None\n",
    "    with_melSpec = False\n",
    "    \n",
    "    \n",
    "    _, model_ext = os.path.splitext(os.path.basename(model_path))\n",
    "    saved_model_type = 'tflite' if model_ext == '.tflite' else ('quantized_keras' if 'quantized' in model_path else 'keras')\n",
    "\n",
    "    #l3embedding_model = get_l3model(model_path, saved_model_type=saved_model_type)\n",
    "\n",
    "    # Generate a single fold if a fold was specified\n",
    "    generate_us8k_fold_data(metadata_path, data_dir, fold_num-1, dataset_output_dir,\n",
    "                            l3embedding_model=model_path, model_type=saved_model_type, \n",
    "                            features='l3', random_state=random_state,\n",
    "                            mel_hop_length=n_hop, n_mels=n_mels,\\\n",
    "                            n_fft=n_dft, fmax=fmax, sr=samp_rate, with_melSpec=with_melSpec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 80 variables.\n",
      "INFO:tensorflow:Converted 80 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "def restore_save_quantized_model(model_path):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    import keras\n",
    "\n",
    "    features = []\n",
    "    eval_graph = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True    \n",
    "    eval_sess = tf.Session(config=config, graph=eval_graph)\n",
    "        \n",
    "    K.set_session(eval_sess)\n",
    "\n",
    "    with eval_graph.as_default():\n",
    "        optimizer = Adam(lr=0.00001)\n",
    "        K.set_learning_phase(0)\n",
    "        eval_model = keras.models.load_model(model_path)\n",
    "        tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\n",
    "        initialize_uninitialized_variables(eval_sess)\n",
    "        \n",
    "        eval_graph_def = eval_graph.as_graph_def()\n",
    "        frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "                                                                        eval_sess,\n",
    "                                                                        eval_graph_def,\n",
    "                                                                        [eval_model.output.op.name]\n",
    "                                                                        )\n",
    "        \n",
    "        with open('/scratch/sk7898/test_quant_keras/eval_graph.pb', 'w') as f:\n",
    "            f.write(str(eval_graph_def))\n",
    "        \n",
    "        with open('/scratch/sk7898/test_quant_keras/eval_frozen_graph.pb', 'w') as f:\n",
    "            f.write(str(frozen_graph_def))\n",
    "        \n",
    "restore_save_quantized_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Check out the input placeholders:\n",
      "Model loading complete!\n"
     ]
    }
   ],
   "source": [
    "def load_graph(model_filepath):\n",
    "    '''\n",
    "    Lode trained model.\n",
    "    '''\n",
    "    print('Loading model...')\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    with tf.gfile.GFile(model_filepath, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    print('Check out the input placeholders:')\n",
    "    nodes = [n.name + ' => ' +  n.op for n in graph_def.node if n.op in ('Placeholder')]\n",
    "    for node in nodes:\n",
    "        print(node)\n",
    "\n",
    "    # Define input tensor\n",
    "    #input = tf.placeholder(np.float32, shape = [None, 32, 32, 3], name='input')\n",
    "    #self.dropout_rate = tf.placeholder(tf.float32, shape = [], name = 'dropout_rate')\n",
    "\n",
    "    #tf.import_graph_def(graph_def, {'input': self.input, 'dropout_rate': self.dropout_rate})\n",
    "\n",
    "    print('Model loading complete!')\n",
    "\n",
    "    '''\n",
    "    # Get layer names\n",
    "    layers = [op.name for op in self.graph.get_operations()]\n",
    "    for layer in layers:\n",
    "        print(layer)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # Check out the weights of the nodes\n",
    "    weight_nodes = [n for n in graph_def.node if n.op == 'Const']\n",
    "    for n in weight_nodes:\n",
    "        print(\"Name of the node - %s\" % n.name)\n",
    "        print(\"Value - \" )\n",
    "        print(tensor_util.MakeNdarray(n.attr['value'].tensor))\n",
    "    '''\n",
    "\n",
    "frozen_file_path = '/scratch/sk7898/test_quant_keras/eval_frozen_graph.pb'\n",
    "load_graph(frozen_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
