{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import argparse\n",
    "import h5py\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import librosa\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import numpy as np\n",
    "from l3embedding.model import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling1D, MaxPooling2D, Flatten, Activation, Lambda\n",
    "import tensorflow as tf\n",
    "import keras.regularizers as regularizers\n",
    "from kapre.time_frequency import Melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cnn_L3_melspec2_spec_model(n_mels=256, n_hop = 242, n_dft = 2048,\n",
    "                                         fmax=None, asr = 48000, halved_convs=False, audio_window_dur = 1):\n",
    "    \n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    n_frames = 1 + int((asr * audio_window_dur) / float(n_hop))\n",
    "    x_a = Input(shape=(n_mels, n_frames, 1), dtype=np.float32)\n",
    "    y_a = BatchNormalization()(x_a)\n",
    "\n",
    "    # CONV BLOCK 1\n",
    "    n_filter_a_1 = np.uint8(64)\n",
    "    if halved_convs:\n",
    "        n_filter_a_1 //= 2\n",
    "\n",
    "    filt_size_a_1 = (np.uint8(3), np.uint8(3))\n",
    "    pool_size_a_1 = (np.uint8(2), np.uint8(2))\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_1, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 2\n",
    "    n_filter_a_2 = np.uint8(128)\n",
    "    if halved_convs:\n",
    "        n_filter_a_2 //= 2\n",
    "\n",
    "    filt_size_a_2 = (np.uint8(3), np.uint8(3))\n",
    "    pool_size_a_2 = (np.uint8(2), np.uint8(2))\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_2, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 3\n",
    "    n_filter_a_3 = np.uint16(256)\n",
    "    if halved_convs:\n",
    "        n_filter_a_3 //= 2\n",
    "\n",
    "    filt_size_a_3 = (np.uint8(3), np.uint8(3))\n",
    "    pool_size_a_3 = (np.uint8(2), np.uint8(2))\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    \n",
    "    if y_a.shape[-2] >= 2:\n",
    "        y_a = MaxPooling2D(pool_size=pool_size_a_3, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 4\n",
    "    n_filter_a_4 = np.uint16(512)\n",
    "    if halved_convs:\n",
    "        n_filter_a_4 //= 2\n",
    "\n",
    "    filt_size_a_4 = (3, 3)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4,\n",
    "                 kernel_initializer='he_normal',\n",
    "                 name='audio_embedding_layer', padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    \n",
    "    m = Model(inputs=x_a, outputs=y_a)\n",
    "    m.name = 'audio_model'\n",
    "\n",
    "    return m, x_a, y_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files(iterable):\n",
    "    lst = list(iterable)\n",
    "    random.shuffle(lst)\n",
    "    return iter(lst)\n",
    "\n",
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec\n",
    "\n",
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_data_generator(data_dir, batch_size=512, samp_rate=48000, n_fft=2048, \\\n",
    "                         n_mels=256, mel_hop_length=242, hop_size=0.1, fmax=None,\\\n",
    "                         random_state=None, start_batch_idx=None):\n",
    "\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "        \n",
    "    frame_length = samp_rate * 1\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "       \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = h5py.File(data_batch_path, 'r')\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['audio'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch = pcm2float(batch, dtype='float32')\n",
    "                    else:\n",
    "                        batch = resample(pcm2float(batch, dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "\n",
    "                    X = [get_melspectrogram(batch[i].flatten(), n_fft=n_fft, mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels, fmax=fmax) for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    return batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(keras_model_path, out_path=None, quant_mode='default',\\\n",
    "                               input_shape=None, calibration_steps=5, samp_rate=8000, n_fft=1024,\\\n",
    "                               n_mels=64, mel_hop_length=160):\n",
    "      \n",
    "    calibrate_data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "    def representative_dataset_gen():\n",
    "        print('Calibrating.........')\n",
    "        for _ in range(calibration_steps):\n",
    "            x = quant_data_generator(calibrate_data_dir, batch_size=1,\\\n",
    "                                     samp_rate=samp_rate, n_fft=n_fft,\\\n",
    "                                     n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "            yield [np.array(x).astype(np.float32)]\n",
    "                \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path) \n",
    "    \n",
    "    if quant_mode == 'default':       \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        \n",
    "    elif quant_mode == 'size':\n",
    "        converter.post_training_quantize = True\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Representation:  8000_64_160_1024_half\n",
      "Input Shape:  (?, 64, 51, 1)\n",
      "Model: \"audio_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 51, 1)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 51, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 51, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 51, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 51, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 51, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 51, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "audio_embedding_layer (Conv2 (None, 8, 6, 256)         590080    \n",
      "=================================================================\n",
      "Total params: 1,174,500\n",
      "Trainable params: 1,173,090\n",
      "Non-trainable params: 1,410\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_models_dir = '/scratch/sk7898/test_models'\n",
    "if not os.path.isdir(test_models_dir):\n",
    "        os.makedirs(test_models_dir)\n",
    "            \n",
    "model_names = ['16000_64_320_1024_half',\\\n",
    "               '16000_64_320_1024_same',\\\n",
    "               '8000_64_160_1024_same',\\\n",
    "               '8000_64_160_1024_half',\\\n",
    "               '16000_64_160_1024_same']\n",
    "\n",
    "model_names = ['8000_64_160_1024_half']\n",
    "\n",
    "for name in model_names:\n",
    "    if name == '16000_64_320_1024_half': \n",
    "        samp_rate = 16000\n",
    "        n_mels = 64\n",
    "        n_hop = 320\n",
    "        n_dft = 1024\n",
    "        halved_convs = True\n",
    "    elif name == '16000_64_320_1024_same':\n",
    "        samp_rate = 16000\n",
    "        n_mels = 64\n",
    "        n_hop = 320\n",
    "        n_dft = 1024\n",
    "        halved_convs = False    \n",
    "    elif name == '8000_64_160_1024_same':\n",
    "        samp_rate = 8000\n",
    "        n_mels = 64\n",
    "        n_hop = 160\n",
    "        n_dft = 1024\n",
    "        halved_convs = False    \n",
    "    elif name == '8000_64_160_1024_half':\n",
    "        samp_rate = 8000\n",
    "        n_mels = 64\n",
    "        n_hop = 160\n",
    "        n_dft = 1024\n",
    "        halved_convs = True     \n",
    "    else:\n",
    "        samp_rate = 16000\n",
    "        n_mels = 64\n",
    "        n_hop = 160\n",
    "        n_dft = 1024\n",
    "        halved_convs = False\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    keras.backend.set_learning_phase(0)\n",
    "    \n",
    "    if halved_convs:\n",
    "        input_repr = str(samp_rate)+'_'+str(n_mels)+'_'+str(n_hop)+'_'+str(n_dft)+'_half'\n",
    "    else:\n",
    "        input_repr = str(samp_rate)+'_'+str(n_mels)+'_'+str(n_hop)+'_'+str(n_dft)\n",
    "\n",
    "    model_output_path = os.path.join(test_models_dir, 'test_l3_audio_{}.h5'.format(input_repr))\n",
    "    audio_spec_embed_model, x_a, _ = construct_cnn_L3_melspec2_spec_model(n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, \\\n",
    "                                                                        halved_convs=halved_convs, asr=samp_rate)\n",
    "    print('Model Representation: ', name)\n",
    "    print('Input Shape: ', x_a.shape)\n",
    "    audio_spec_embed_model.summary()\n",
    "    audio_spec_embed_model.save(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "INFO:tensorflow:Froze 48 variables.\n",
      "INFO:tensorflow:Converted 48 variables to const ops.\n",
      "Calibrating.........\n",
      "20180180_6_66.h5\n",
      "20180270_0_119.h5\n",
      "20187003_3_53.h5\n",
      "20180276_6_161.h5\n",
      "20187008_0_12.h5\n"
     ]
    }
   ],
   "source": [
    "input_shapes = [(1, 64, 51, 1)]\n",
    "test_models_dir = '/scratch/sk7898/test_models'\n",
    "keras_models = ['test_l3_audio_8000_64_160_1024_half.h5']\n",
    "\n",
    "for input_shape, model in zip(input_shapes, keras_models):\n",
    "    input_repr = model.strip('.h5')\n",
    "    keras_model_path = os.path.join(test_models_dir, model)\n",
    "    quant_output_path = os.path.join(test_models_dir, 'quant_{}.tflite'.format(input_repr))\n",
    "\n",
    "    calibration_steps = 5\n",
    "    quant_mode = 'default'\n",
    "\n",
    "    post_training_quantization(keras_model_path, out_path=quant_output_path, quant_mode=quant_mode, \\\n",
    "                               input_shape=input_shape, calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "{'name': 'input_1', 'index': 20, 'shape': array([ 1, 64, 51,  1], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.3137255012989044, 127)}\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'audio_embedding_layer/BiasAdd', 'index': 7, 'shape': array([  1,   8,   6, 256], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (1.5971095561981201, 4)}\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(quant_output_path))\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "output_shape = output_details[0]['shape'][1:]\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
