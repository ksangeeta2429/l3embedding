{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Activation, Lambda\n",
    "import keras.regularizers as regularizers\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files(iterable):\n",
    "    lst = list(iterable)\n",
    "    random.shuffle(lst)\n",
    "    return iter(lst)\n",
    "\n",
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec\n",
    "\n",
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256,\\\n",
    "                       quant_melspec=False, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    #if quant_melspec:\n",
    "        #https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/quantization/quantize\n",
    "        #S = S.astype(np.int8)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_data_generator(data_dir, batch_size=512, samp_rate=8000, n_fft=2048, \\\n",
    "                         n_mels=64, mel_hop_length=160, hop_size=0.1, fmax=None,\\\n",
    "                         random_state=None, start_batch_idx=None, quant_input=False):\n",
    "\n",
    "    #global shortlist_files\n",
    "    if random_state:\n",
    "        random.seed(23455)\n",
    "        \n",
    "    frame_length = samp_rate * 1\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "        \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        #shortlist_files.append(data_batch_path)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = np.load(data_batch_path)\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['audio'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    #saved audio files are already in float so need not convert to float32\n",
    "                    X = [get_melspectrogram(batch[i].flatten(), n_fft=n_fft, \\\n",
    "                                            mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels,\\\n",
    "                                            quant_melspec=quant_input, fmax=fmax) for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    return batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None\n",
    "\n",
    "def single_epoch_test_data_generator(file_list, quant_input=True, batch_size=64, samp_rate=8000, fmax=None,\\\n",
    "                                     n_fft=1024, n_mels=64, mel_hop_length=160, start_batch_idx=None):\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    for fname in file_list:\n",
    "        data_batch_path = fname\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = np.load(data_batch_path)\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['audio'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    X = [get_melspectrogram(batch[i].flatten(), n_fft=n_fft, \\\n",
    "                                            mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels,\\\n",
    "                                            quant_melspec=quant_input, fmax=fmax) for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_keras_to_tflite(tflite_model_file, keras_model_path, quant_mode='default', quantized_input=False,\\\n",
    "                             n_mels=256, n_hop=242, n_dft=2048, asr=48000, halved_convs=False,\\\n",
    "                             calibrate_data_dir=None, num_calibration_steps=1024):\n",
    "\n",
    "    def representative_dataset_gen():\n",
    "            #l3_model = os.path.dirname(tflite_model_file)\n",
    "            #splits = l3_model.split('_')\n",
    "\n",
    "            print('Calibrating.........')\n",
    "            for _ in range(num_calibration_steps):\n",
    "                x = quant_data_generator(calibrate_data_dir, batch_size=1,\\\n",
    "                                         samp_rate=asr, n_fft=n_dft, n_mels=n_mels,\\\n",
    "                                         mel_hop_length=n_hop)\n",
    "                yield [np.array(x).astype(np.float32)]\n",
    "                \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\n",
    "    \n",
    "    if quant_mode == 'default':\n",
    "        if calibrate_data_dir is None:\n",
    "            raise ValueError('Quantized activation calibration needs data directory!')\n",
    "        \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        if quantized_input:\n",
    "            #converter.inference_input_type = tf.int8\n",
    "            converter.inference_output_type = tf.int8\n",
    "        #converter.default_ranges_stats = (0, 1)\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "                \n",
    "    elif quant_mode == 'size':\n",
    "        converter.post_training_quantize = True\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_model_file, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print('Tflite model saved in:', tflite_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(model_path, calibrate_data_dir, quant_mode='default', quantized_input=False,\\\n",
    "                               n_mels=256, n_hop=242, n_dft=2048, asr=48000, halved_convs=False,\\\n",
    "                               flatten=False, calibration_steps=1024):\n",
    "    \n",
    "    #1. Convert l3model to keras model for quantization (with maxpooling layer but flatten removed)\n",
    "    dir_prefix = '/scratch/sk7898/quantization/' + os.path.basename(model_path).strip('.h5')\n",
    "    \n",
    "    if not os.path.isdir(dir_prefix):\n",
    "        os.makedirs(dir_prefix)\n",
    "    \n",
    "    keras_model = keras.models.load_model(model_path)\n",
    "    #print(keras_model.summary())\n",
    "    \n",
    "    #2.1 Convert keras to tflite model\n",
    "    #2.2 Quantize model with mode 'default' for only weights quantization or 'size' for full quantization\n",
    "    #2.3 Save the quantized tflite model\n",
    "    \n",
    "    print('Quantizing keras model and saving as tflite')\n",
    "    input_type = '_int8Ip' if quantized_input else '_float32'\n",
    "    tflite_model_file = os.path.join(dir_prefix, 'full_quantized_'+ quant_mode + input_type + '.tflite')\n",
    "    \n",
    "    quantize_keras_to_tflite(tflite_model_file, model_path, quant_mode=quant_mode,\\\n",
    "                             quantized_input=quantized_input, asr=asr,\\\n",
    "                             n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, halved_convs=halved_convs, \\\n",
    "                             calibrate_data_dir=calibrate_data_dir, num_calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantize both the weights and the activations of the model**\\\n",
    "If the input is already in int8, set quantized_input = True\\\n",
    "If tflite should convert the float32 to int8 by adding a Quantize layer, quantized_input = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Quantizing keras model and saving as tflite\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 54 variables.\n",
      "INFO:tensorflow:Converted 54 variables to const ops.\n",
      "Calibrating.........\n",
      "00_002872.npz\n",
      "11_000501.npz\n",
      "27_000831.npz\n",
      "13_001380.npz\n",
      "40_000687.npz\n",
      "40_001575.npz\n",
      "32_001624.npz\n",
      "40_000428.npz\n",
      "02_002355.npz\n",
      "38_002951.npz\n",
      "27_001961.npz\n",
      "38_000322.npz\n",
      "23_001430.npz\n",
      "40_001131.npz\n",
      "31_000861.npz\n",
      "05_010416.npz\n",
      "41_002444.npz\n",
      "40_000153.npz\n",
      "27_003242.npz\n",
      "04_000128.npz\n",
      "33_002663.npz\n",
      "32_002908.npz\n",
      "04_001123.npz\n",
      "37_001644.npz\n",
      "29_001369.npz\n",
      "02_000256.npz\n",
      "06_001327.npz\n",
      "40_000525.npz\n",
      "10_002599.npz\n",
      "40_001089.npz\n",
      "40_001437.npz\n",
      "23_002593.npz\n",
      "Tflite model saved in: /scratch/sk7898/quantization/pipeline/full_quantized_default_float32.tflite\n"
     ]
    }
   ],
   "source": [
    "#model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "#model_path = '/scratch/dr2915/l3pruning/embedding/fixed/reduced_input/l3_audio_20191108201753_8000_64_160_1024_half.h5'\n",
    "model_path = '/scratch/dr2915/Nathan/pipeline.h5'\n",
    "calibrate_data_dir = '/beegfs/dr2915/sonyc_ust/frames/8KHz'\n",
    "calibration_steps = 32\n",
    "\n",
    "quant_mode='default'\n",
    "flatten=True\n",
    "quantized_input=False\n",
    "n_mels=64\n",
    "n_hop=160\n",
    "n_dft=1024\n",
    "asr=8000\n",
    "halved_convs=True if 'half' in model_path else False\n",
    "\n",
    "post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode, quantized_input=quantized_input,\\\n",
    "                           n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, asr=asr, halved_convs=halved_convs,\\\n",
    "                           flatten=flatten, calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input/Output of tflite model (Interpreter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "{'name': 'input_1', 'index': 45, 'shape': array([ 1, 64, 51,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'urban_sound_classifier/output/Sigmoid', 'index': 44, 'shape': array([1, 8], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128)}\n"
     ]
    }
   ],
   "source": [
    "output_path = '/scratch/sk7898/quantization'\n",
    "quant_model = 'pipeline/full_quantized_default_float32.tflite'\n",
    "quant_output_path = os.path.join(output_path, quant_model)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(quant_output_path))\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "output_shape = output_details[0]['shape'][1:]\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantize only the weights of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_mode = 'size'\n",
    "post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode, quantized_input=quantized_input,\\\n",
    "                           n_mels=n_mels, n_hop=n_hop, n_dft=n_dft, asr=asr, halved_convs=halved_convs,\\\n",
    "                           flatten=flatten, calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Embedding from the tflite model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softmax_batch_from_tflite(data_gen, tflite_model_file, batch_size, classes=8):\n",
    "    \n",
    "    predictions = []\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "\n",
    "    interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "                \n",
    "    #predictions per batch   \n",
    "    for idx, batch_x in enumerate(data_gen):\n",
    "        x = np.array(batch_x).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        predictions.append(output)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_softmax(tflite_model_file, file_list, samp_rate=8000,\\\n",
    "                n_mels=64, emb_len=256, mel_hop_length=160,\\\n",
    "                n_fft=1024, batch_size=64):\n",
    "    \n",
    "    output = None\n",
    "    classes = 8\n",
    "    print('Getting softmax output for downstream classes out of Quantized tflite model')\n",
    "    \n",
    "    data_gen = single_epoch_test_data_generator(file_list, batch_size=batch_size, samp_rate=samp_rate,\\\n",
    "                                                n_fft=n_fft, n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "\n",
    "    output = get_softmax_batch_from_tflite(data_gen, tflite_model_file,\\\n",
    "                                           batch_size, classes=classes)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_files(data_dir, num_files=10):\n",
    "    shortlist_files = []\n",
    "    random.seed(23455)\n",
    "    \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        shortlist_files.append(data_batch_path)\n",
    "        if len(shortlist_files) >= num_files:\n",
    "            break\n",
    "    return shortlist_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'selected_audio_files.npz'\n",
    "tflite_model_file = '/scratch/sk7898/quantization/pipeline/full_quantized_default_float32.tflite'\n",
    "\n",
    "if not os.path.exists(out_file):\n",
    "    test_data_dir = '/beegfs/dr2915/sonyc_ust/frames/8KHz'\n",
    "    shortlist_files = get_test_files(test_data_dir)\n",
    "    np.savez(out_file, x=shortlist_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/beegfs/dr2915/sonyc_ust/frames/8KHz/01_001297.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/27_002896.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/04_002755.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/03_000965.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/16_010692.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/08_001117.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/25_010315.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/06_000069.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/18_001657.npz'\n",
      " '/beegfs/dr2915/sonyc_ust/frames/8KHz/06_002320.npz']\n"
     ]
    }
   ],
   "source": [
    "files = np.load(out_file)\n",
    "shortlist_files = files['x']\n",
    "print(shortlist_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting softmax output for downstream classes out of Quantized tflite model\n",
      "== Input details ==\n",
      "{'name': 'input_1', 'index': 45, 'shape': array([64, 64, 51,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'urban_sound_classifier/output/Sigmoid', 'index': 44, 'shape': array([64,  8], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128)}\n"
     ]
    }
   ],
   "source": [
    "output = gen_softmax(tflite_model_file, shortlist_files, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_file = 'selected_audio_predictions_fp.npz'\n",
    "#np.savez(output_file, y=np.array(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(output).reshape(-1, 8)\n",
    "pred_max = np.argmax(pred, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 6 7 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 7 7 7 7 7 7 7 7 7 6 6 6 7 6 6 6 6 7 7 7 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 6 6 4 4\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 4 6 6 0 6 6 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6 4 4 0 0 6 0 0 6 6 6 0 6 6 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 4 0 0 0 0 0 0 6 0 6 6 0 6 6 0 6 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 6 0 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 6 6 6 6 6 0 0 0 0 0 0 0 0 6\n",
      " 0 0 0 0 6 0 1 6 4 6 0 0 6 6 0 6 6 6 0 6 6 6 0 0 0 0 0 0 0 6 0 6 0 0 6 6 4\n",
      " 6 6 6 6 4 4 0 0 0 0 0 6 0 6 0 6 6 6 6 6 6 6 0 6 6 6 0 6 0 6 6 1 0 0 0 0 0\n",
      " 0 0 0 0 0 4 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 0 1 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 3 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 3 1 0 1 0 1 0 3 1 0 1 0 0 1 1 1 1 1 1\n",
      " 1 1 1 3 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(pred_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = 'audio_class_pred_fp.npz'\n",
    "np.savez(pred_file, y=pred_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
