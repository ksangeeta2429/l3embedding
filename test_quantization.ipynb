{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_shuffle(iterable, shuffle=True):\n",
    "    lst = list(iterable)\n",
    "    while True:\n",
    "        yield from lst\n",
    "        if shuffle:\n",
    "            random.shuffle(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length,\\\n",
    "                                 window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_mels=n_mels, fmax=fmax,\\\n",
    "                                           power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir, batch_size=512, samp_rate=48000,\\\n",
    "                   n_fft=2048, n_mels=256, mel_hop_length=252, hop_size=0.1, fmax=None,\\\n",
    "                   random_state=20180123, start_batch_idx=None, keys=None):\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    hop_length = int(hop_size * samp_rate)\n",
    "    frame_length = samp_rate * 1\n",
    "    \n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    # Limit keys to avoid producing batches with all of the metadata fields\n",
    "    if not keys:\n",
    "        keys = ['audio', 'video', 'label']\n",
    "\n",
    "    for fname in cycle_shuffle(os.listdir(data_dir)):\n",
    "        batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        blob = h5py.File(batch_path, 'r')\n",
    "        blob_size = len(blob['label'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = {k:blob[k][blob_start_idx:blob_end_idx]\n",
    "                             for k in keys}\n",
    "                else:\n",
    "                    for k in keys:\n",
    "                        batch[k] = np.concatenate([batch[k],\n",
    "                                                   blob[k][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Preprocess video so samples are in [-1,1]\n",
    "                    batch['video'] = 2 * img_as_float(batch['video']).astype('float32') - 1\n",
    "\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch['audio'] = pcm2float(batch['audio'], dtype='float32')\n",
    "                    else:\n",
    "                        batch['audio'] = resample(pcm2float(batch['audio'], dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "                \n",
    "                    X = [get_melspectrogram(batch['audio'][i].flatten(), n_fft=n_fft, mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels) for i in range(batch_size)]\n",
    "                    \n",
    "                    batch['audio'] = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch['audio'])) #(64, 256, 191, 1)\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_data_generator(data_dir, epoch_size, **kwargs):\n",
    "    while True:\n",
    "        data_gen = data_generator(data_dir, **kwargs)\n",
    "        for idx, item in enumerate(data_gen):\n",
    "            yield item\n",
    "            # Once we generate all batches for an epoch, restart the generator\n",
    "            if (idx + 1) == epoch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_rate = 48000\n",
    "num_epochs = 1\n",
    "\n",
    "train_data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "validation_data_dir = '/beegfs/work/AudioSetSamples/music_valid'\n",
    "\n",
    "train_batch_size = 64\n",
    "validation_batch_size = 32\n",
    "\n",
    "train_epoch_size = 64\n",
    "validation_epoch_size = 64\n",
    "\n",
    "train_gen = data_generator(train_data_dir,\\\n",
    "                           batch_size=train_batch_size,\\\n",
    "                           samp_rate=samp_rate)\n",
    "\n",
    "train_gen = pescador.maps.keras_tuples(train_gen,\n",
    "                                       ['video', 'audio'],\n",
    "                                       'label')\n",
    "\n",
    "val_gen = single_epoch_data_generator(validation_data_dir,\\\n",
    "                                      validation_epoch_size,\\\n",
    "                                      batch_size=validation_batch_size,\\\n",
    "                                      samp_rate=samp_rate)\n",
    "\n",
    "val_gen = pescador.maps.keras_tuples(val_gen,\n",
    "                                     ['video', 'audio'],\n",
    "                                     'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "learning_rate = 0.00001\n",
    "model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_252_2048.h5'\n",
    "model = keras.models.load_model(model_path)\n",
    "model.compile(Adam(lr=learning_rate),\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "history = model.fit_generator(train_gen, train_epoch_size, num_epochs,\\\n",
    "                              validation_data=val_gen,\\\n",
    "                              validation_steps=validation_epoch_size,\\\n",
    "                              verbose=True,\\\n",
    "                              initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and Quantize model without melspectrogram\n",
    "out_path = \"/scratch/sk7898/quantization/\" + os.path.basename(model_path).strip('.h5') +\"/checkpoints\"\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "train_sess = tf.Session(graph=train_graph)\n",
    "\n",
    "keras.backend.set_session(train_sess)\n",
    "with train_graph.as_default():\n",
    "    model = keras.models.load_model(model_path)\n",
    "    tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\n",
    "    train_sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    model.compile(Adam(lr=learning_rate),\n",
    "                      loss=loss,\n",
    "                      metrics=metrics)\n",
    "    history = model.fit_generator(train_gen, train_epoch_size, num_epochs,\\\n",
    "                                  validation_data=val_gen,\\\n",
    "                                  validation_steps=validation_epoch_size,\\\n",
    "                                  verbose=True,\\\n",
    "                                  initial_epoch=0)\n",
    "    #save graph and checkpoints\n",
    "    #saver = tf.train.Saver()\n",
    "    #saver.save(train_sess, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Quantized model without melspectrogram \n",
    "#Save the checkpoint and eval graph proto to disk for freezing and providing to TFLite.\n",
    "with open(eval_graph_file, 'w') as f:\n",
    "    f.write(str(g.as_graph_def()))\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert keras model to tflite\n",
    "model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_252_2048.h5'\n",
    "tflite_model_file = \"/scratch/sk7898/quantization/quantized_\" + os.path.basename(model_path).strip('.h5') +\".tflite\"\n",
    "converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(model_path)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate AVC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
