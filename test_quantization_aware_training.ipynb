{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import git\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "import pescador\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import tempfile\n",
    "import librosa\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from l3embedding.training_utils import conv_dict_to_val_list, multi_gpu_model, \\\n",
    "    MultiGPUCheckpointCallback, LossHistory, GSheetLogger, TimeHistory\n",
    "from l3embedding.model import *\n",
    "from l3embedding.audio import pcm2float\n",
    "from log import *\n",
    "from kapre.time_frequency import Spectrogram, Melspectrogram\n",
    "from resampy import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger('embedding_approx_mse')\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_shuffle(iterable, shuffle=True):\n",
    "    lst = list(iterable)\n",
    "    while True:\n",
    "        yield from lst\n",
    "        if shuffle:\n",
    "            random.shuffle(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec\n",
    "\n",
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_length(model):\n",
    "    embed_layer = model.get_layer('audio_embedding_layer')\n",
    "    emb_len = tuple(embed_layer.get_output_shape_at(0))\n",
    "    return emb_len[-1]\n",
    "\n",
    "def get_embedding_key(method, batch_size, emb_len, neighbors=None, \\\n",
    "                      metric=None, min_dist=None):\n",
    "    \n",
    "    if method == 'umap':\n",
    "        if neighbors is None or metric is None or min_dist is None:\n",
    "            raise ValueError('Either neighbors or metric or min_dist is missing')\n",
    "        \n",
    "        key = 'umap_batch_' + str(batch_size) +\\\n",
    "              '_len_' + str(emb_len) +\\\n",
    "              '_k_' + str(neighbors) +\\\n",
    "              '_metric_' + metric +\\\n",
    "              '_dist|iter_' + str(min_dist)\n",
    "\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir, emb_dir, student_emb_length=None, approx_mode='umap', approx_train_size=None,\\\n",
    "                   neighbors=10, min_dist=0.3, metric='euclidean', batch_size=512, \\\n",
    "                   n_fft=2048, n_mels=256, n_hop=242, hop_size=0.1, fmax=None,\\\n",
    "                   samp_rate=16000, random_state=20180123, start_batch_idx=None, test=False):\n",
    "\n",
    "    if approx_mode != 'mse' and (student_emb_length is None or approx_train_size is None):\n",
    "        raise ValueError('Either student embedding length or reduced embedding training size was not provided')\n",
    "\n",
    "    random.seed(random_state)\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    if student_emb_length != 512:\n",
    "        emb_key = get_embedding_key(approx_mode, approx_train_size, student_emb_length, neighbors=neighbors, \\\n",
    "                                    metric=metric, min_dist=min_dist)\n",
    "    else:\n",
    "        emb_key = 'l3_embedding'\n",
    "        \n",
    "    if test:\n",
    "        print('Testing phase')\n",
    "        data_list = os.listdir(data_dir)\n",
    "    else:\n",
    "        data_list = cycle_shuffle(os.listdir(data_dir))\n",
    "        \n",
    "    for fname in data_list:\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        emb_batch_path = os.path.join(emb_dir, fname)\n",
    "\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = h5py.File(data_batch_path, 'r')\n",
    "        emb_blob = h5py.File(emb_batch_path, 'r')\n",
    "\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = {'audio': data_blob['audio'][blob_start_idx:blob_end_idx],\\\n",
    "                             'label': emb_blob[emb_key][blob_start_idx:blob_end_idx]}\n",
    "                else:\n",
    "                    batch['audio'] = np.concatenate([batch['audio'], data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "                    batch['label'] = np.concatenate([batch['label'], emb_blob[emb_key][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "                emb_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch['audio'] = pcm2float(batch['audio'], dtype='float32')\n",
    "                    else:\n",
    "                        batch['audio'] = resample(pcm2float(batch['audio'], dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "\n",
    "                    X = [get_melspectrogram(batch['audio'][i].flatten(), n_fft=n_fft, mel_hop_length=n_hop,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels) for i in range(batch_size)]\n",
    "\n",
    "                    batch['audio'] = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch['audio'])) #(64, 256, 191, 1)\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_data_generator(data_dir, emb_dir, epoch_size, **kwargs):\n",
    "    while True:\n",
    "        data_gen = data_generator(data_dir, emb_dir, **kwargs)\n",
    "        for idx, item in enumerate(data_gen):\n",
    "            yield item\n",
    "            # Once we generate all batches for an epoch, restart the generator\n",
    "            if (idx + 1) == epoch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_variables(sess):\n",
    "    if hasattr(tf, 'global_variables'):\n",
    "        variables = tf.global_variables()\n",
    "    else:\n",
    "        variables = tf.all_variables()\n",
    "\n",
    "    #print(variables)\n",
    "    uninitialized_variables = []\n",
    "    for v in variables:\n",
    "        if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n",
    "            uninitialized_variables.append(v)\n",
    "            v._keras_initialized = True\n",
    "    \n",
    "    #print(uninitialized_variables)\n",
    "    if uninitialized_variables:\n",
    "        if hasattr(tf, 'variables_initializer'):\n",
    "            sess.run(tf.variables_initializer(uninitialized_variables))\n",
    "        else:\n",
    "            sess.run(tf.initialize_variables(uninitialized_variables)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_l3_audio_model(model_path): \n",
    "    model = keras.models.load_model(model_path)\n",
    "    embed_layer = model.get_layer('audio_embedding_layer')\n",
    "    pool_size = tuple(embed_layer.get_output_shape_at(0)[1:3])\n",
    "    y_a = MaxPooling2D(pool_size=pool_size, padding='same')(model.output)\n",
    "    y_a = Flatten()(y_a)\n",
    "    model = keras.models.Model(inputs=model.input, outputs=y_a)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(model_description):\n",
    "    fmax = None\n",
    "    splits = model_description.split('_') \n",
    "    samp_rate = int(splits[3])\n",
    "    n_mels = int(splits[4])\n",
    "    n_hop = int(splits[5])\n",
    "    n_fft = int(splits[6])\n",
    "    if len(splits) == 10:\n",
    "        fmax = int(splits[-1])\n",
    "    return samp_rate, n_mels, n_hop, n_fft, fmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_save_quantized_model(model_path, output_dir):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    import keras\n",
    "\n",
    "    output_path = os.path.join(output_dir, 'frozen_model.pb')\n",
    "    eval_graph = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True    \n",
    "#     session_conf = tf.ConfigProto(device_count={'GPU' : 0},\\\n",
    "#                                   allow_soft_placement=True,\\\n",
    "#                                   log_device_placement=False)\n",
    "    eval_sess = tf.Session(config=config, graph=eval_graph)\n",
    "\n",
    "    K.set_session(eval_sess)\n",
    "\n",
    "    with eval_graph.as_default():\n",
    "        K.set_learning_phase(0)\n",
    "        model_repr = model_path.split('/')[-2]\n",
    "        eval_model = keras.models.load_model(model_path, custom_objects={'Melspectrogram': Melspectrogram})\n",
    "        tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\n",
    "        \n",
    "        eval_graph_def = eval_graph.as_graph_def()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(eval_sess, os.path.join(output_dir, model_repr))\n",
    "\n",
    "        frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "                                                                        eval_sess,\n",
    "                                                                        eval_graph_def,\n",
    "                                                                        [eval_model.output.op.name]\n",
    "                                                                        )\n",
    "\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quantized_model(train_data_dir, validation_data_dir, emb_train_dir, emb_valid_dir, output_dir, model_path,\\\n",
    "                          approx_mode='umap', approx_train_size=None, neighbors=10, min_dist=0.3, metric='euclidean', \\\n",
    "                          num_epochs=300, train_epoch_size=4096, validation_epoch_size=1024, gpus=1, \\\n",
    "                          train_batch_size=64, validation_batch_size=64, log_path=None, disable_logging=False, \\\n",
    "                          random_state=20180216,learning_rate=0.00001, verbose=True, checkpoint_interval=10,\\\n",
    "                          continue_model_dir=None, gsheet_id=None, google_dev_app_name=None,):\n",
    "    \n",
    "    K.clear_session()\n",
    "    init_console_logger(LOGGER, verbose=verbose)\n",
    "    if not disable_logging:\n",
    "        init_file_logger(LOGGER, log_path=log_path)\n",
    "\n",
    "    LOGGER.debug('Initialized logging.')\n",
    "    LOGGER.info('Embedding Reduction Mode: %s', approx_mode)\n",
    "\n",
    "    #reduced_emb_dir_train = os.path.join(reduced_emb_dir, os.path.basename(train_data_dir))\n",
    "    #reduced_emb_dir_valid = os.path.join(reduced_emb_dir, os.path.basename(validation_data_dir))\n",
    "      \n",
    "    if approx_mode == 'umap':\n",
    "        if min_dist is None:\n",
    "            min_dist = 0.3\n",
    "        if metric is None:\n",
    "            metric='euclidean'\n",
    "        model_attribute = 'quantized_umap_train_' + str(approx_train_size) + '_neighbors_' + str(neighbors)+\\\n",
    "                            '_dist_' + str(min_dist) + '_metric_' + metric\n",
    "    elif approx_mode == 'mse':\n",
    "        min_dist = 0\n",
    "        neighbors = 0\n",
    "        metric = ''\n",
    "        model_attribute = 'quantized_mse_original'\n",
    "    else:\n",
    "        raise ValueError('Approximation mode: {} not supported!'.format(approx_mode))\n",
    "    \n",
    "    \n",
    "    #train graph\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "    train_graph = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    train_sess = tf.Session(config=config, graph=train_graph)\n",
    "    K.set_session(train_sess)\n",
    "\n",
    "    with train_graph.as_default():\n",
    "        # Set up callbacks\n",
    "        if continue_model_dir:\n",
    "            model_desc = continue_model_dir.split('/')[-3]\n",
    "            latest_model_path = os.path.join(continue_model_dir, 'model_latest.h5')\n",
    "            student_base_model = keras.models.load_model(latest_model_path, custom_objects={'Melspectrogram': Melspectrogram})\n",
    "            samp_rate, n_mels, n_hop, n_dft, fmax = get_model_params(model_desc)\n",
    "\n",
    "        elif model_path:\n",
    "            model_desc = os.path.basename(model_path).strip('.h5')\n",
    "            student_base_model = load_l3_audio_model(model_path)\n",
    "            samp_rate, n_mels, n_hop, n_dft, fmax = get_model_params(model_desc)\n",
    "        else:\n",
    "            raise ValueError('Both continue_dir and model_path are not provided!')\n",
    "        \n",
    "        if 'half' in model_desc:\n",
    "            model_repr = str(samp_rate)+'_'+str(n_mels)+'_'+str(n_hop)+'_'+str(n_dft)+'_half'+'_fmax_'+str(fmax)\n",
    "        else:\n",
    "            model_repr = str(samp_rate)+'_'+str(n_mels)+'_'+str(n_hop)+'_'+str(n_dft)+'_fmax_'+str(fmax)\n",
    "\n",
    "        # Make sure the directories we need exist\n",
    "        if continue_model_dir:\n",
    "            model_dir = continue_model_dir\n",
    "        else:\n",
    "            model_dir = os.path.join(output_dir, 'embedding_approx', model_repr, model_attribute,\\\n",
    "                                     datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "        if not os.path.isdir(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        student_emb_len = get_embedding_length(student_base_model);\n",
    "        LOGGER.info('Student sampling rate: {}'.format(samp_rate))\n",
    "        LOGGER.info('Model Representation: {}'.format(model_repr))\n",
    "        LOGGER.info('Model Attribute: {}'.format(model_attribute))\n",
    "        LOGGER.info('Student Embedding Length: {}'.format(student_emb_len))\n",
    "        LOGGER.info('Model files can be found in \"{}\"'.format(model_dir))\n",
    "\n",
    "        param_dict = {\n",
    "            'username': getpass.getuser(),\n",
    "            'model_dir': model_dir,\n",
    "            'train_data_dir': train_data_dir,\n",
    "            'validation_data_dir': validation_data_dir,\n",
    "            'reduced_emb_train_dir': emb_train_dir,\n",
    "            'reduced_emb_valid_dir': emb_valid_dir,\n",
    "            'approx_mode': approx_mode,\n",
    "            'neighbors': neighbors,\n",
    "            'min_dist': min_dist,\n",
    "            'metric': metric,\n",
    "            'student_model_repr': model_repr,\n",
    "            'student_emb_len': student_emb_len, \n",
    "            'num_epochs': num_epochs,\n",
    "            'train_epoch_size': train_epoch_size,\n",
    "            'validation_epoch_size': validation_epoch_size,\n",
    "            'train_batch_size': train_batch_size,\n",
    "            'validation_batch_size': validation_batch_size,\n",
    "            'random_state': random_state,\n",
    "            'learning_rate': learning_rate,\n",
    "            'verbose': verbose,\n",
    "            'checkpoint_interval': checkpoint_interval,\n",
    "            'log_path': log_path,\n",
    "            'disable_logging': disable_logging,\n",
    "            'gpus': gpus,\n",
    "            'continue_model_dir': continue_model_dir,\n",
    "        }\n",
    "\n",
    "        LOGGER.info('Training with the following arguments: {}'.format(param_dict))\n",
    "\n",
    "        param_dict.update({\n",
    "            'latest_epoch': '-',\n",
    "            'latest_train_loss': '-',\n",
    "            'latest_validation_loss': '-',\n",
    "            'latest_train_mae': '-',\n",
    "            'latest_validation_mae': '-',\n",
    "            'best_train_loss': '-',\n",
    "            'best_validation_loss': '-',\n",
    "            'best_train_mae': '-',\n",
    "            'best_validation_mae': '-',\n",
    "        })\n",
    "\n",
    "        latest_weight_path = os.path.join(model_dir, 'model_latest.h5')\n",
    "        best_valid_mae_weight_path = os.path.join(model_dir, 'model_best_valid_mae.h5')\n",
    "        best_valid_loss_weight_path = os.path.join(model_dir, 'model_best_valid_loss.h5')\n",
    "        checkpoint_weight_path = os.path.join(model_dir, 'model_checkpoint.{epoch:02d}.h5')\n",
    "\n",
    "        # Load information about last epoch for initializing callbacks and data generators\n",
    "        if continue_model_dir is not None:\n",
    "            prev_train_hist_path = os.path.join(continue_model_dir, 'history_csvlog.csv')\n",
    "            last_epoch_idx, last_val_mae, last_val_loss = get_restart_info(prev_train_hist_path)\n",
    "\n",
    "        if gsheet_id:\n",
    "            cb.append(GSheetLogger(google_dev_app_name, gsheet_id, param_dict))\n",
    "\n",
    "        LOGGER.info('Setting up train data generator...')\n",
    "        if continue_model_dir is not None:\n",
    "            train_start_batch_idx = train_epoch_size * (last_epoch_idx + 1)\n",
    "        else:\n",
    "            train_start_batch_idx = None\n",
    "\n",
    "        # Fit the model\n",
    "        LOGGER.info('Fitting model...')\n",
    "        if verbose:\n",
    "            verbosity = 1\n",
    "        else:\n",
    "            verbosity = 2\n",
    "\n",
    "        if continue_model_dir is not None:\n",
    "            initial_epoch = last_epoch_idx + 1\n",
    "        else:\n",
    "            initial_epoch = 0\n",
    "        \n",
    "        cb = []\n",
    "        cb.append(MultiGPUCheckpointCallback(latest_weight_path,\n",
    "                                             student_base_model,\n",
    "                                             save_weights_only=False,\n",
    "                                             verbose=1))\n",
    "\n",
    "        best_val_mae_cb = MultiGPUCheckpointCallback(best_valid_mae_weight_path,\n",
    "                                                     student_base_model,\n",
    "                                                     save_weights_only=False,\\\n",
    "                                                     save_best_only=True,\\\n",
    "                                                     verbose=1,\\\n",
    "                                                     monitor='val_mean_absolute_error')\n",
    "        if continue_model_dir is not None:\n",
    "            best_val_mae_cb.best = last_val_mae\n",
    "        cb.append(best_val_mae_cb)\n",
    "\n",
    "        best_val_loss_cb = MultiGPUCheckpointCallback(best_valid_loss_weight_path,\n",
    "                                                      student_base_model,\n",
    "                                                      save_weights_only=False,\n",
    "                                                      save_best_only=True,\n",
    "                                                      verbose=1,\n",
    "                                                      monitor='val_loss')\n",
    "        if continue_model_dir is not None:\n",
    "            best_val_loss_cb.best = last_val_loss\n",
    "        cb.append(best_val_loss_cb)\n",
    "\n",
    "        checkpoint_cb = MultiGPUCheckpointCallback(checkpoint_weight_path,\n",
    "                                                   student_base_model,\n",
    "                                                   save_weights_only=False,\n",
    "                                                   period=checkpoint_interval)\n",
    "        if continue_model_dir is not None:\n",
    "            checkpoint_cb.epochs_since_last_save = (last_epoch_idx + 1) % checkpoint_interval\n",
    "        cb.append(checkpoint_cb)\n",
    "\n",
    "        history_checkpoint = os.path.join(model_dir, 'history_checkpoint.pkl')\n",
    "        cb.append(LossHistory(history_checkpoint))\n",
    "\n",
    "        history_csvlog = os.path.join(model_dir, 'history_csvlog.csv')\n",
    "        cb.append(keras.callbacks.CSVLogger(history_csvlog, append=True, separator=','))\n",
    "\n",
    "        earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10)\n",
    "        reduceLR = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "        cb.append(earlyStopping)\n",
    "        cb.append(reduceLR)\n",
    "    \n",
    "        train_gen = data_generator(train_data_dir,\n",
    "                                   emb_train_dir,\n",
    "                                   approx_mode=approx_mode,\n",
    "                                   approx_train_size=approx_train_size,\n",
    "                                   neighbors=neighbors,\n",
    "                                   min_dist=min_dist,\n",
    "                                   student_emb_length=student_emb_len,\n",
    "                                   samp_rate=samp_rate,\n",
    "                                   batch_size=train_batch_size,\n",
    "                                   n_fft=n_dft, n_mels=n_mels, n_hop=n_hop, fmax=fmax,\n",
    "                                   random_state=random_state,\n",
    "                                   start_batch_idx=train_start_batch_idx)\n",
    "\n",
    "        val_gen = single_epoch_data_generator(validation_data_dir,\n",
    "                                              emb_valid_dir,\n",
    "                                              validation_epoch_size,\n",
    "                                              approx_mode=approx_mode,\n",
    "                                              approx_train_size=approx_train_size,\n",
    "                                              neighbors=neighbors,\n",
    "                                              min_dist=min_dist,\n",
    "                                              student_emb_length=student_emb_len,\n",
    "                                              samp_rate=samp_rate,\n",
    "                                              batch_size=validation_batch_size,\n",
    "                                              n_fft=n_dft, n_mels=n_mels, n_hop=n_hop, fmax=fmax,\n",
    "                                              random_state=random_state)\n",
    "\n",
    "\n",
    "        train_gen = pescador.maps.keras_tuples(train_gen,\n",
    "                                               'audio',\n",
    "                                               'label')\n",
    "\n",
    "        val_gen = pescador.maps.keras_tuples(val_gen,\n",
    "                                             'audio',\n",
    "                                             'label')\n",
    "    \n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        #print(tf.all_variables())\n",
    "        #print(tf.get_default_graph().get_operations())\n",
    "        tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\n",
    "        initialize_uninitialized_variables(train_sess)\n",
    "        \n",
    "        #Convert the base (single-GPU) model to Multi-GPU model\n",
    "        if gpus == 1:\n",
    "            model = student_base_model\n",
    "        else:\n",
    "            model = multi_gpu_model(student_base_model, gpus=gpus)\n",
    "    \n",
    "        model.compile(optimizer,\\\n",
    "                      loss='mean_squared_error',\\\n",
    "                      metrics=['mae'])\n",
    "        \n",
    "        history = model.fit_generator(train_gen, train_epoch_size, num_epochs,\\\n",
    "                                      validation_data=val_gen,\n",
    "                                      validation_steps=validation_epoch_size,\n",
    "                                      callbacks=cb,\n",
    "                                      verbose=verbosity, initial_epoch=initial_epoch)\n",
    "\n",
    "        #save graph and checkpoints\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(train_sess, model_dir) \n",
    "    \n",
    "        LOGGER.info('Done training. Saving results to disk...')\n",
    "        # Save history\n",
    "        with open(os.path.join(model_dir, 'history.pkl'), 'wb') as fd:\n",
    "            pickle.dump(history.history, fd)\n",
    "\n",
    "    LOGGER.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-30 12:19:50,796 - embedding_approx_mse - DEBUG - Initialized logging.\n",
      "2019-09-30 12:19:50,798 - embedding_approx_mse - INFO - Embedding Reduction Mode: mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/keras/models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "2019-09-30 12:19:52,456 - embedding_approx_mse - INFO - Student sampling rate: 48000\n",
      "2019-09-30 12:19:52,458 - embedding_approx_mse - INFO - Model Representation: 48000_256_242_2048_fmax_None\n",
      "2019-09-30 12:19:52,460 - embedding_approx_mse - INFO - Model Attribute: quantized_mse_original\n",
      "2019-09-30 12:19:52,461 - embedding_approx_mse - INFO - Student Embedding Length: 512\n",
      "2019-09-30 12:19:52,463 - embedding_approx_mse - INFO - Model files can be found in \"/scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952\"\n",
      "2019-09-30 12:19:52,465 - embedding_approx_mse - INFO - Training with the following arguments: {'username': 'sk7898', 'model_dir': '/scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952', 'train_data_dir': '/beegfs/work/AudioSetSamples/music_train', 'validation_data_dir': '/beegfs/work/AudioSetSamples/music_valid', 'reduced_emb_train_dir': '/scratch/sk7898/orig_l3_embeddings/music_train', 'reduced_emb_valid_dir': '/scratch/sk7898/orig_l3_embeddings/music_valid', 'approx_mode': 'mse', 'neighbors': 0, 'min_dist': 0, 'metric': '', 'student_model_repr': '48000_256_242_2048_fmax_None', 'student_emb_len': 512, 'num_epochs': 2, 'train_epoch_size': 3, 'validation_epoch_size': 3, 'train_batch_size': 3, 'validation_batch_size': 10, 'random_state': 20180216, 'learning_rate': 1e-05, 'verbose': True, 'checkpoint_interval': 10, 'log_path': None, 'disable_logging': False, 'gpus': 1, 'continue_model_dir': None}\n",
      "2019-09-30 12:19:52,466 - embedding_approx_mse - INFO - Setting up train data generator...\n",
      "2019-09-30 12:19:52,468 - embedding_approx_mse - INFO - Fitting model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/librosa/filters.py:271: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3 [===================>..........] - ETA: 2s - loss: 1.7411 - mean_absolute_error: 1.0292 Epoch 00001: saving model to /scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952/model_latest.h5\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 1.31079, saving model to /scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952/model_best_valid_mae.h5\n",
      "Epoch 00001: val_loss improved from inf to 2.80094, saving model to /scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952/model_best_valid_loss.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.6920 - mean_absolute_error: 1.0211 - val_loss: 2.8009 - val_mean_absolute_error: 1.3108\n",
      "Epoch 2/2\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6538 - mean_absolute_error: 0.9958Epoch 00002: saving model to /scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952/model_latest.h5\n",
      "Epoch 00002: val_mean_absolute_error improved from 1.31079 to 1.25134, saving model to /scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952/model_best_valid_mae.h5\n",
      "Epoch 00002: val_loss improved from 2.80094 to 2.59357, saving model to /scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952/model_best_valid_loss.h5\n",
      "3/3 [==============================] - 2s 641ms/step - loss: 1.6114 - mean_absolute_error: 0.9851 - val_loss: 2.5936 - val_mean_absolute_error: 1.2513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-30 12:20:11,112 - embedding_approx_mse - INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    num_epochs = 2\n",
    "    learning_rate = 0.00001\n",
    "    train_batch_size = 3\n",
    "    train_epoch_size = 3\n",
    "    validation_epoch_size = 3\n",
    "    validation_batch_size = 10\n",
    "    model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "    train_data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "    validation_data_dir = '/beegfs/work/AudioSetSamples/music_valid'\n",
    "    emb_train_dir = '/scratch/sk7898/orig_l3_embeddings/music_train'\n",
    "    emb_valid_dir = '/scratch/sk7898/orig_l3_embeddings/music_valid'\n",
    "    output_dir = '/scratch/sk7898/test_quant'\n",
    "    train_quantized_model(train_data_dir, validation_data_dir, emb_train_dir, emb_valid_dir, output_dir, model_path,\\\n",
    "                          approx_mode='mse', learning_rate=learning_rate, num_epochs=num_epochs,\\\n",
    "                          train_epoch_size=train_epoch_size, validation_epoch_size=validation_epoch_size, gpus=1, \\\n",
    "                          train_batch_size=train_batch_size, validation_batch_size=validation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'batch_normalization_92/gamma:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'batch_normalization_92/beta:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'batch_normalization_92/moving_mean:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'batch_normalization_92/moving_variance:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'conv2d_71/kernel:0' shape=(3, 3, 1, 64) dtype=float32_ref>, <tf.Variable 'conv2d_71/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_93/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_93/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_93/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_93/moving_variance:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2d_72/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'conv2d_72/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_94/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_94/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_94/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'batch_normalization_94/moving_variance:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'conv2d_73/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'conv2d_73/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_95/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_95/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_95/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_95/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'conv2d_74/kernel:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'conv2d_74/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_96/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_96/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_96/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'batch_normalization_96/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'conv2d_75/kernel:0' shape=(3, 3, 128, 256) dtype=float32_ref>, <tf.Variable 'conv2d_75/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_97/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_97/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_97/moving_mean:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_97/moving_variance:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'conv2d_76/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'conv2d_76/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_98/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_98/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_98/moving_mean:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'batch_normalization_98/moving_variance:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'conv2d_77/kernel:0' shape=(3, 3, 256, 512) dtype=float32_ref>, <tf.Variable 'conv2d_77/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'batch_normalization_99/gamma:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'batch_normalization_99/beta:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'batch_normalization_99/moving_mean:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'batch_normalization_99/moving_variance:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'audio_embedding_layer/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'audio_embedding_layer/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'Adam/iterations:0' shape=() dtype=int64_ref>, <tf.Variable 'Adam/lr:0' shape=() dtype=float32_ref>, <tf.Variable 'Adam/beta_1:0' shape=() dtype=float32_ref>, <tf.Variable 'Adam/beta_2:0' shape=() dtype=float32_ref>, <tf.Variable 'Adam/decay:0' shape=() dtype=float32_ref>, <tf.Variable 'training/Adam/Variable:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_1:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_2:0' shape=(3, 3, 1, 64) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_3:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_4:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_5:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_7:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_8:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_9:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_10:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_11:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_12:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_13:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_14:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_15:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_16:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_17:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_18:0' shape=(3, 3, 128, 256) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_19:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_20:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_21:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_22:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_23:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_24:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_25:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_26:0' shape=(3, 3, 256, 512) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_27:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_28:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_29:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_30:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_31:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_32:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_33:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_34:0' shape=(3, 3, 1, 64) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_35:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_36:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_37:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_38:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_39:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_40:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_41:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_42:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_43:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_44:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_45:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_46:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_47:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_48:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_49:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_50:0' shape=(3, 3, 128, 256) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_51:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_52:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_53:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_54:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_55:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_56:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_57:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_58:0' shape=(3, 3, 256, 512) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_59:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_60:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_61:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_62:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'training/Adam/Variable_63:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'conv2d_71/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_71/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_71/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_71/act_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_72/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_72/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_72/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_72/act_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_73/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_73/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_73/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_73/act_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_74/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_74/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_74/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_74/act_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_75/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_75/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_75/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_75/act_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_76/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_76/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_76/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_76/act_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_77/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_77/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_77/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'conv2d_77/act_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'audio_embedding_layer/weights_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'audio_embedding_layer/weights_quant/max:0' shape=() dtype=float32_ref>, <tf.Variable 'audio_embedding_layer/act_quant/min:0' shape=() dtype=float32_ref>, <tf.Variable 'audio_embedding_layer/act_quant/max:0' shape=() dtype=float32_ref>]\n",
      "INFO:tensorflow:Restoring parameters from /scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 80 variables.\n",
      "INFO:tensorflow:Converted 80 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "quantize_trained_model = '/scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original/20190930121952/model_best_valid_loss.h5'\n",
    "output_path = '/scratch/sk7898/test_quant/embedding_approx/48000_256_242_2048_fmax_None/quantized_mse_original'\n",
    "restore_save_quantized_model(quantize_trained_model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFLITE CONVERTER\n",
    "usage: tflite_convert [-h] --output_file OUTPUT_FILE\n",
    "                      (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\n",
    "                      [--output_format {TFLITE,GRAPHVIZ_DOT}]\n",
    "                      [--inference_type {FLOAT,QUANTIZED_UINT8}]\n",
    "                      [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\n",
    "                      [--input_arrays INPUT_ARRAYS]\n",
    "                      [--input_shapes INPUT_SHAPES]\n",
    "                      [--output_arrays OUTPUT_ARRAYS]\n",
    "                      [--saved_model_tag_set SAVED_MODEL_TAG_SET]\n",
    "                      [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\n",
    "                      [--std_dev_values STD_DEV_VALUES]\n",
    "                      [--mean_values MEAN_VALUES]\n",
    "                      [--default_ranges_min DEFAULT_RANGES_MIN]\n",
    "                      [--default_ranges_max DEFAULT_RANGES_MAX]\n",
    "                      [--post_training_quantize] [--drop_control_dependency]\n",
    "                      [--reorder_across_fake_quant]\n",
    "                      [--change_concat_input_ranges {TRUE,FALSE}]\n",
    "                      [--allow_custom_ops]\n",
    "                      [--converter_mode {DEFAULT,TOCO_FLEX,TOCO_FLEX_ALL}]\n",
    "                      [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR]\n",
    "                      [--dump_graphviz_video]\n",
    "                    \n",
    "tflite_convert --output_file=model.tflite --graph_def_file=frozen_model.pb --mean_values=0  --input_arrays=input_13 --output_arrays=max_pooling2d_1/MaxPool  --std_dev_values=255  --default_ranges_min=0 --default_ranges_max=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
