{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_shuffle(iterable, shuffle=True):\n",
    "    lst = list(iterable)\n",
    "    while True:\n",
    "        yield from lst\n",
    "        if shuffle:\n",
    "            random.shuffle(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir, emb_dir, batch_size=512, samp_rate=48000, n_fft=2048, \\\n",
    "                   n_mels=256, mel_hop_length=242, hop_size=0.1, fmax=None,\\\n",
    "                   random_state=20180123, start_batch_idx=None, keys=None, test=False):\n",
    "\n",
    "    random.seed(random_state)\n",
    "    frame_length = samp_rate * 1\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    emb_key = 'l3_embedding'\n",
    "\n",
    "    if test:\n",
    "        print('Testing phase')\n",
    "        data_list = os.listdir(data_dir)\n",
    "    else:\n",
    "        data_list = cycle_shuffle(os.listdir(data_dir))\n",
    "        \n",
    "    for fname in data_list:\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        emb_batch_path = os.path.join(emb_dir, fname)\n",
    "\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = h5py.File(data_batch_path, 'r')\n",
    "        emb_blob = h5py.File(emb_batch_path, 'r')\n",
    "\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = {'audio': data_blob['audio'][blob_start_idx:blob_end_idx],\\\n",
    "                             'label': emb_blob[emb_key][blob_start_idx:blob_end_idx]}\n",
    "                else:\n",
    "                    batch['audio'] = np.concatenate([batch['audio'], data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "                    batch['label'] = np.concatenate([batch['label'], emb_blob[emb_key][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "                emb_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch['audio'] = pcm2float(batch['audio'], dtype='float32')\n",
    "                    else:\n",
    "                        batch['audio'] = resample(pcm2float(batch['audio'], dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "\n",
    "                    X = [get_melspectrogram(batch['audio'][i].flatten(), n_fft=n_fft, mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels) for i in range(batch_size)]\n",
    "\n",
    "                    batch['audio'] = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch['audio'])) #(64, 256, 191, 1)\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_data_generator(data_dir, emb_dir, epoch_size, **kwargs):\n",
    "    while True:\n",
    "        data_gen = data_generator(data_dir, emb_dir, **kwargs)\n",
    "        for idx, item in enumerate(data_gen):\n",
    "            yield item\n",
    "            # Once we generate all batches for an epoch, restart the generator\n",
    "            if (idx + 1) == epoch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_variables(sess):\n",
    "    if hasattr(tf, 'global_variables'):\n",
    "        variables = tf.global_variables()\n",
    "    else:\n",
    "        variables = tf.all_variables()\n",
    "\n",
    "    uninitialized_variables = []\n",
    "    for v in variables:\n",
    "        if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n",
    "            uninitialized_variables.append(v)\n",
    "            v._keras_initialized = True\n",
    "            \n",
    "    print(uninitialized_variables)\n",
    "    if uninitialized_variables:\n",
    "        if hasattr(tf, 'variables_initializer'):\n",
    "            sess.run(tf.variables_initializer(uninitialized_variables))\n",
    "        else:\n",
    "            sess.run(tf.initialize_variables(uninitialized_variables)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_l3_audio_model(model_path):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    embed_layer = model.get_layer('audio_embedding_layer')\n",
    "    pool_size = tuple(embed_layer.get_output_shape_at(0)[1:3])\n",
    "    y_a = keras.layers.MaxPooling2D(pool_size=pool_size, padding='same')(model.output)\n",
    "    y_a = keras.layers.Flatten()(y_a)\n",
    "    model = keras.models.Model(inputs=model.input, outputs=y_a)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quantized_model(model_path, output_path, train=False, train_gen=None, \\\n",
    "                          train_epoch_size=64, num_epochs=5, val_gen=None,\\\n",
    "                          validation_epoch_size=64):\n",
    "    \n",
    "    if train and (train_gen is None or val_gen is None):\n",
    "        raise ValueError('Invalid data (train/valid) generator')\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    import keras\n",
    "\n",
    "    output_path = os.path.join(output_dir, 'checkpoints')\n",
    "    \n",
    "    train_graph = tf.Graph() #tf.keras.backend.get_session().graph\n",
    "    session_conf = tf.ConfigProto(device_count={'GPU' : 0},\\\n",
    "                                  allow_soft_placement=True,\\\n",
    "                                  log_device_placement=False)\n",
    "    train_sess = tf.Session(config=session_conf, graph=train_graph)\n",
    "    keras.backend.set_session(train_sess)\n",
    "    \n",
    "    with train_graph.as_default():\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model = load_l3_audio_model(model_path)\n",
    "        tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\n",
    "        initialize_uninitialized_variables(train_sess)\n",
    "        if train:\n",
    "            model.compile(optimizer,\\\n",
    "                          loss='mean_squared_error',\\\n",
    "                          metrics=['mae'])\n",
    "\n",
    "            history = model.fit_generator(train_gen, train_epoch_size, num_epochs,\\\n",
    "                                          validation_data=val_gen,\\\n",
    "                                          validation_steps=validation_epoch_size,\\\n",
    "                                          verbose=1, initial_epoch=0)\n",
    "\n",
    "        #save graph and checkpoints\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(train_sess, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_quantized_model(model_path, output_dir):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    import keras\n",
    "\n",
    "    output_path = os.path.join(output_dir, 'frozen_model.pb')\n",
    "    eval_graph = tf.Graph()\n",
    "    session_conf = tf.ConfigProto(device_count={'GPU' : 0},\\\n",
    "                                  allow_soft_placement=True,\\\n",
    "                                  log_device_placement=False)\n",
    "    train_sess = tf.Session(config=session_conf, graph=eval_graph)\n",
    "\n",
    "    keras.backend.set_session(eval_sess)\n",
    "\n",
    "    with eval_graph.as_default():\n",
    "        keras.backend.set_learning_phase(0)\n",
    "        eval_model = load_l3_audio_model(model_path)\n",
    "        \n",
    "        tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\n",
    "        eval_graph_def = eval_graph.as_graph_def()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(eval_sess, 'checkpoints')\n",
    "\n",
    "        frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            eval_sess,\n",
    "            eval_graph_def,\n",
    "            [eval_model.output.op.name]\n",
    "        )\n",
    "\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_252_2048.h5'\n",
    "splits = os.path.basename(model_path).split('.h5')[0].split('_')\n",
    "samp_rate = int(splits[3])\n",
    "n_mels = int(splits[4])\n",
    "mel_hop_length = int(splits[5])\n",
    "n_fft = int(splits[-1])\n",
    "\n",
    "output_dir = \"/scratch/sk7898/quantization/\" + os.path.basename(model_path).strip('.h5')\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "num_epochs = 1\n",
    "learning_rate = 0.00001\n",
    "train_data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "validation_data_dir = '/beegfs/work/AudioSetSamples/music_valid'\n",
    "\n",
    "train_batch_size = 64\n",
    "train_epoch_size = 64\n",
    "\n",
    "validation_epoch_size = 3\n",
    "validation_batch_size = 10\n",
    "\n",
    "\n",
    "train_emb_dir='/scratch/sk7898/orig_l3_embeddings/music_train'\n",
    "val_emb_dir='/scratch/sk7898/orig_l3_embeddings/music_valid'\n",
    "\n",
    "train_gen = data_generator(train_data_dir, train_emb_dir, batch_size=train_batch_size, samp_rate=samp_rate,\\\n",
    "                           n_fft=n_fft, n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "\n",
    "train_gen = pescador.maps.keras_tuples(train_gen,\n",
    "                                       'audio',\n",
    "                                       'label')\n",
    "\n",
    "val_gen = single_epoch_data_generator(validation_data_dir, val_emb_dir, validation_epoch_size,\\\n",
    "                                      batch_size=validation_batch_size, samp_rate=samp_rate,\\\n",
    "                                      n_fft=n_fft, n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "\n",
    "val_gen = pescador.maps.keras_tuples(val_gen,\n",
    "                                     'audio',\n",
    "                                     'label')\n",
    "\n",
    "#Load and Quantize model without melspectrogram\n",
    "#train_quantized_model(model_path, output_dir)\n",
    "\n",
    "#restore_quantized_model(model_path, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
