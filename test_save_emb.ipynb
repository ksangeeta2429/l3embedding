{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "from l3embedding.audio import pcm2float\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teacher_embedding(audio_batch):\n",
    "    import keras\n",
    "    import tensorflow as tf\n",
    "    from kapre.time_frequency import Melspectrogram\n",
    "    \n",
    "    try:\n",
    "        with tf.Graph().as_default(), tf.Session().as_default():\n",
    "            weight_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_full_original_48000_256_242_2048.h5'\n",
    "            model = keras.models.load_model(weight_path, custom_objects={'Melspectrogram': Melspectrogram}) \n",
    "            embeddings = model.get_layer('audio_model').predict(audio_batch)\n",
    "            return embeddings\n",
    "\n",
    "    except GeneratorExit:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_h5(paths, batch, batch_size):\n",
    "    n_files = int(batch_size/1024)\n",
    "    start_idx = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        end_idx = start_idx + 1024\n",
    "            \n",
    "        with h5py.File(path, 'a') as f:\n",
    "            for key in batch.keys():\n",
    "                if key in f.keys():\n",
    "                    continue\n",
    "                f.create_dataset(key, data=batch[key][start_idx:end_idx], compression='gzip')\n",
    "            f.close()\n",
    "        start_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_embedding(data, method, emb_len=None, neighbors=10, \\\n",
    "                          metric='euclidean', min_dist=0.3, iterations=500):\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        raise ValueError('Data is empty!')\n",
    "    if emb_len is None:\n",
    "        raise ValueError('Reduced embedding dimension was not provided!')\n",
    "\n",
    "    if method == 'umap':\n",
    "        embedding = umap.umap_.UMAP(n_neighbors=neighbors, min_dist=min_dist, metric=metric, \\\n",
    "                                    n_components=emb_len).fit_transform(data)\n",
    "    elif method == 'tsne':\n",
    "        embedding = TSNE(perplexity=neighbors, n_components=emb_len, metric=metric, \\\n",
    "                         n_iter=iterations, method='exact').fit_transform(data)\n",
    "    else:\n",
    "        raise ValueError('Reduction method technique should be either `umap` or `tsne`!')\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob_keys(method, batch_size, emb_len, neighbors_list=None, metric_list=None, min_dist_list=None, tsne_iter_list=None):\n",
    "    \n",
    "    blob_keys = []\n",
    "    \n",
    "    if method == 'umap':\n",
    "        if neighbors_list is None or metric_list is None or min_dist_list is None:\n",
    "            raise ValueError('Either neighbor_list or metric_list or min_dist_list is missing')\n",
    "        \n",
    "        [blob_keys.append('umap_batch_' + str(batch_size) + \\\n",
    "                          '_len_' + str(emb_len) + \\\n",
    "                          '_k_' + str(neighbors) + \\\n",
    "                          '_metric_' + metric + \\\n",
    "                          '_dist|iter_' + str(min_dist)) \\\n",
    "         for neighbors in neighbors_list for metric in metric_list for min_dist in min_dist_list]\n",
    "                    \n",
    "    elif method == 'tsne':\n",
    "        if neighbors_list is None or metric_list is None or tsne_iter_list is None:\n",
    "            raise ValueError('Either neighbor_list or metric_list or tsne_iter_list is missing')\n",
    "        \n",
    "        [blob_keys.append('tsne_batch_' + str(batch_size) +\\\n",
    "                          '_len_' + str(emb_len) + \\\n",
    "                          '_batch_' + str(batch_size) + \\\n",
    "                          '_k_' + str(neighbors) + \\\n",
    "                          '_metric_' + metric + \\\n",
    "                          '_dist|iter_' + str(iteration)) \\\n",
    "        for neighbors in neighbors_list for metric in metric_list for iteration in tsne_iter_list]\n",
    "\n",
    "    return blob_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_generator(data_dir, output_dir, reduced_emb_len, approx_mode='umap', neighbors_list=None, \\\n",
    "                        metric_list=None, min_dist_list=None, tsne_iter_list=[500], \\\n",
    "                        batch_size=1024, random_state=20180123, start_batch_idx=None):\n",
    "\n",
    "    if data_dir == output_dir:\n",
    "        raise ValueError('Output path should not be same as data path to avoid overwriting data files!')\n",
    "        \n",
    "    if neighbors_list is None:\n",
    "        raise ValueError('Neighbor cannot be None!')\n",
    "    \n",
    "    if metric_list is None:\n",
    "        metric_list = ['euclidean']\n",
    "\n",
    "    if approx_mode == 'umap' and min_dist_list is None:\n",
    "        min_dist_list = [0.3]\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    \n",
    "    batch = None\n",
    "    blob_embeddings = dict()\n",
    "    embedding_out_paths = []\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    blob_keys = get_blob_keys(approx_mode, batch_size, reduced_emb_len,\\\n",
    "                              neighbors_list=neighbors_list, metric_list=metric_list,\\\n",
    "                              min_dist_list=min_dist_list, tsne_iter_list=tsne_iter_list)\n",
    "    \n",
    "    print('Embedding Blob Keys: {}'.format(blob_keys))\n",
    "        \n",
    "    b_idx = 0\n",
    "    for fname in os.listdir(data_dir):\n",
    "        if os.path.isdir(fname):\n",
    "            continue\n",
    "            \n",
    "        batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        blob = h5py.File(batch_path, 'r')\n",
    "        blob_size = len(blob['label'])\n",
    "        \n",
    "        embedding_out_paths.append(os.path.join(output_dir, fname))\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = {'audio':blob['audio'][blob_start_idx:blob_end_idx]}\n",
    "                else:\n",
    "                    batch['audio'] = np.concatenate([batch['audio'], blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "            \n",
    "            if blob_end_idx == blob_size:\n",
    "                blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                results = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Convert audio to float\n",
    "                    batch['audio'] = pcm2float(batch['audio'], dtype='float32')\n",
    "                    \n",
    "                    # Get the embedding layer output from the audio_model and flatten it to be treated as labels for the student audio model\n",
    "                    teacher_embedding = get_teacher_embedding(batch['audio'])\n",
    "                    \n",
    "                    if approx_mode == 'umap':\n",
    "                        n_process = len(neighbors_list) * len(metric_list) * len(min_dist_list)\n",
    "                                            \n",
    "                        results = Parallel(n_jobs=min(multiprocessing.cpu_count(), n_process))\\\n",
    "                                                    (delayed(get_reduced_embedding)(teacher_embedding, 'umap',\\\n",
    "                                                                                   emb_len=reduced_emb_len,\\\n",
    "                                                                                   neighbors=neighbors,\\\n",
    "                                                                                   metric=metric,\\\n",
    "                                                                                   min_dist=min_dist)\\\n",
    "                        for neighbors in neighbors_list for metric in metric_list for min_dist in min_dist_list)\n",
    "                                                \n",
    "                        \n",
    "                    for idx in range(len(results)):\n",
    "                        if blob_keys[idx] not in blob_embeddings.keys():    \n",
    "                            blob_embeddings[blob_keys[idx]] = np.zeros((blob_size, reduced_emb_len), dtype=np.float32)\n",
    "                            blob_embeddings[blob_keys[idx]] = results[idx]\n",
    "                        else:\n",
    "                            blob_embeddings[blob_keys[idx]] = results[idx]\n",
    "                      \n",
    "                    blob_embeddings['l3_embedding'] = teacher_embedding\n",
    "                    write_to_h5(embedding_out_paths, blob_embeddings, batch_size) \n",
    "              \n",
    "                    b_idx += 1\n",
    "                    print('File {}: {} done!'.format(b_idx, fname))\n",
    "                \n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None\n",
    "                blob_embeddings = dict()\n",
    "                embedding_out_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Blob Keys: ['umap_batch_1024_len_256_k_10_metric_correlation_dist|iter_0.3', 'umap_batch_1024_len_256_k_20_metric_correlation_dist|iter_0.3', 'umap_batch_1024_len_256_k_30_metric_correlation_dist|iter_0.3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk7898/miniconda3/envs/l3embedding-new-cpu/lib/python3.6/site-packages/librosa/filters.py:261: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n",
      "/home/sk7898/miniconda3/envs/l3embedding-new-cpu/lib/python3.6/site-packages/keras/models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1: 20180261_7_35.h5 done!\n",
      "File 2: 20180261_7_34.h5 done!\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/scratch/sk7898/temp_data'\n",
    "output_dir = '/scratch/sk7898/temp_emb/temp_data'\n",
    "reduced_emb_len = 256\n",
    "neighbors_list = [10, 20, 30]\n",
    "min_dist_list = [0.3]\n",
    "metric_list = ['correlation']\n",
    "\n",
    "embedding_generator(data_dir, output_dir, reduced_emb_len, approx_mode='umap', neighbors_list=neighbors_list, \\\n",
    "                    metric_list=metric_list, min_dist_list=min_dist_list, \\\n",
    "                    batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  /scratch/sk7898/reduced_embeddings/music_valid/0_0_22.h5\n",
      "<HDF5 dataset \"umap_batch_1024_len_256_k_10_metric_correlation_dist|iter_0.3\": shape (1024, 256), type \"<f4\">\n",
      "<HDF5 dataset \"umap_batch_1024_len_256_k_20_metric_correlation_dist|iter_0.3\": shape (1024, 256), type \"<f4\">\n",
      "<HDF5 dataset \"umap_batch_1024_len_256_k_30_metric_correlation_dist|iter_0.3\": shape (1024, 256), type \"<f4\">\n",
      "<HDF5 dataset \"umap_batch_1024_len_256_k_5_metric_correlation_dist|iter_0.3\": shape (1024, 256), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "fname = '/scratch/sk7898/reduced_embeddings/music_valid/0_0_22.h5'\n",
    "batch_path = os.path.join(fname)\n",
    "blob = h5py.File(batch_path, 'r') \n",
    "print('File: ', fname)\n",
    "for key in blob.keys():\n",
    "    print(blob[key])\n",
    "blob.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 10, 'metric': 'correlation', 'min_dist': 0.3}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 10, 'metric': 'correlation', 'min_dist': 0.5}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 10, 'metric': 'euclidean', 'min_dist': 0.3}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 10, 'metric': 'euclidean', 'min_dist': 0.5}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 20, 'metric': 'correlation', 'min_dist': 0.3}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 20, 'metric': 'correlation', 'min_dist': 0.5}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 20, 'metric': 'euclidean', 'min_dist': 0.3}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 20, 'metric': 'euclidean', 'min_dist': 0.5}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 30, 'metric': 'correlation', 'min_dist': 0.3}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 30, 'metric': 'correlation', 'min_dist': 0.5}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 30, 'metric': 'euclidean', 'min_dist': 0.3}\n",
      "<function whatever at 0x2ae23607d598> ('umap',) {'emb_len': 256, 'neighbors': 30, 'metric': 'euclidean', 'min_dist': 0.5}\n",
      "[10, 10, 10, 10, 20, 20, 20, 20, 30, 30, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "#Test the multiprocessing return\n",
    "reduced_emb_len = 256\n",
    "neighbors_list = [10, 20, 30]\n",
    "min_dist_list = [0.3, 0.5]\n",
    "metric_list = ['correlation', 'euclidean']\n",
    "\n",
    "def whatever(method, emb_len=None, neighbors=10, metric='euclidean', min_dist=0.3, iterations=500):\n",
    "    dummy = neighbors\n",
    "    return dummy\n",
    "\n",
    "functions = (delayed(whatever)('umap',\\\n",
    "                            emb_len=reduced_emb_len,\\\n",
    "                            neighbors=neighbors,\\\n",
    "                            metric=metric,\\\n",
    "                            min_dist=min_dist)\\\n",
    "for neighbors in neighbors_list for metric in metric_list for min_dist in min_dist_list)\n",
    "\n",
    "for f, args, kwargs in functions:\n",
    "    print(f, args, kwargs)\n",
    "    \n",
    "results = Parallel(n_jobs=min(multiprocessing.cpu_count(),3))(delayed(whatever)('umap',\\\n",
    "                                                                emb_len=reduced_emb_len,\\\n",
    "                                                                neighbors=neighbors,\\\n",
    "                                                                metric=metric,\\\n",
    "                                                                min_dist=min_dist)\\\n",
    "for neighbors in neighbors_list for metric in metric_list for min_dist in min_dist_list)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 12, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk7898/miniconda3/envs/l3embedding-new-cpu/lib/python3.6/site-packages/keras/models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Test the Multi-GPU callback to save single-GPU model\n",
    "import keras\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "#model_path = '/scratch/sk7898/l3pruning/embedding_approx_mse/embedding_approx/mse_original/20190912111335/model_best_valid_loss.h5'\n",
    "#model_path='/scratch/sk7898/l3pruning/embedding_approx_mse/embedding_approx/mse_original/20190912113135/model_best_valid_mae.h5'\n",
    "model = keras.models.load_model(model_path, custom_objects={'Melspectrogram': Melspectrogram})\n",
    "embedding_shape = model.get_layer('audio_embedding_layer').output_shape\n",
    "print(embedding_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
