{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from tensorflow.core.framework import types_pb2\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Activation, Lambda\n",
    "import keras.regularizers as regularizers\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_data_generator(data_dir, batch_size=512, random_state=None, start_batch_idx=None):\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "        \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        #shortlist_files.append(data_batch_path)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = np.load(data_batch_path)\n",
    "        blob_size = len(data_blob['db_mels'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['db_mels'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['db_mels'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    #saved audio files are already in float so need not convert to float32\n",
    "                    X = [batch[i] for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    return batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None\n",
    "\n",
    "def single_epoch_test_data_generator(file_list, batch_size=64, start_batch_idx=None):\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    for fname in file_list:\n",
    "        data_batch_path = fname\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = np.load(data_batch_path)\n",
    "        blob_size = len(data_blob['db_mels'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['db_mels'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['db_mels'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    X = [batch[i] for i in range(batch_size)]\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    yield batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_keras_to_tflite(tflite_model_file, keras_model, keras_model_path, quant_mode='default',\n",
    "                             quantized_input=False, target_type=None,\n",
    "                             calibrate_data_dir=None, num_calibration_steps=1024):\n",
    "\n",
    "    def representative_dataset_gen():\n",
    "            print('Calibrating.........')\n",
    "            for _ in range(num_calibration_steps):\n",
    "                x = quant_data_generator(calibrate_data_dir, batch_size=1)\n",
    "                yield [np.array(x).astype(np.float32)]\n",
    "    \n",
    "    tf_version = tf.__version__.split('.')[0]\n",
    "    \n",
    "    if tf_version == '2':\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    else:\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\n",
    "    \n",
    "    if quant_mode == 'default':\n",
    "        if calibrate_data_dir is None:\n",
    "            raise ValueError('Quantized activation calibration needs data directory!')\n",
    "        \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        if quantized_input:\n",
    "            print('Quantizing the input!')\n",
    "            converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        #converter.default_ranges_stats = (-127, 128)\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "                \n",
    "    elif quant_mode == 'size':\n",
    "        converter.post_training_quantize = True\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_model_file, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print('Tflite model saved in:', tflite_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(model_path, calibrate_data_dir=None, quant_mode='default',\n",
    "                               quantized_input=True, target_type='int8', calibration_steps=1024):\n",
    "    \n",
    "    #1. Convert l3model to keras model for quantization (with maxpooling layer but flatten removed)\n",
    "    dir_prefix = '/scratch/sk7898/quantization/' + os.path.basename(model_path).strip('.h5')\n",
    "    \n",
    "    if not os.path.isdir(dir_prefix):\n",
    "        os.makedirs(dir_prefix)\n",
    "    \n",
    "    tf_version = tf.__version__.split('.')[0]\n",
    "    \n",
    "    if tf_version=='2':\n",
    "        keras_model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        keras_model = keras.models.load_model(model_path)\n",
    "    #print(keras_model.summary())\n",
    "    \n",
    "    #2.1 Convert keras to tflite model\n",
    "    #2.2 Quantize model with mode 'default' for only weights quantization or 'size' for full quantization\n",
    "    #2.3 Save the quantized tflite model\n",
    "    \n",
    "    print('Quantizing keras model and saving as tflite')\n",
    "    tflite_model_file = os.path.join(dir_prefix, 'tf_' + str(tf_version) + '_full_quantized_'+ quant_mode + '_'+ target_type + '.tflite')\n",
    "    \n",
    "    quantize_keras_to_tflite(tflite_model_file, keras_model, model_path, quant_mode=quant_mode,\n",
    "                             quantized_input=quantized_input, calibrate_data_dir=calibrate_data_dir,\n",
    "                             target_type=target_type, num_calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantize both the weights and the activations of the model: quant_mode = 'default' else quant_mode='size'**\\\n",
    "If the input is already in int8, set quantized_input = True\\\n",
    "If tflite should convert the float32 to int8 by adding a Quantize layer, quantized_input = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Quantizing keras model and saving as tflite\n",
      "Tflite model saved in: /scratch/sk7898/quantization/pipeline_cmsis_mels/tf_2_full_quantized_size_float16.tflite\n"
     ]
    }
   ],
   "source": [
    "#model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "#model_path = '/scratch/dr2915/l3pruning/embedding/fixed/reduced_input/l3_audio_20191108201753_8000_64_160_1024_half.h5'\n",
    "model_path = '/scratch/sk7898/quantization/pipeline_cmsis/pipeline_cmsis_mels.h5'\n",
    "calibrate_data_dir = '/scratch/sk7898/cmsis_ml_data' #'/beegfs/dr2915/sonyc_ust/frames/8KHz'\n",
    "calibration_steps = 32\n",
    "quant_mode = 'size'\n",
    "target_type = 'float16'\n",
    "\n",
    "post_training_quantization(model_path, calibrate_data_dir=calibrate_data_dir, quant_mode=quant_mode,\n",
    "                           target_type=target_type, calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input/Output of tflite model (Interpreter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "{'name': 'input_1', 'index': 1, 'shape': array([ 1, 64, 51,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'Identity', 'index': 0, 'shape': array([1, 8], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "output_path = '/scratch/sk7898/quantization'\n",
    "quant_model = '/scratch/sk7898/quantization/pipeline_cmsis_mels/tf_2_full_quantized_size_float16.tflite' #'pipeline_cmsis_mels/tf_2_full_quantized_default_float32.tflite'\n",
    "quant_output_path = os.path.join(output_path, quant_model)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(quant_output_path))\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "output_shape = output_details[0]['shape'][1:]\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Embedding from the tflite model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softmax_batch_from_tflite(data_gen, tflite_model_file, batch_size, classes=8):\n",
    "    \n",
    "    predictions = []\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "\n",
    "    interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "       \n",
    "    #predictions per batch   \n",
    "    for idx, batch_x in enumerate(data_gen):\n",
    "        x = np.array(batch_x).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        predictions.append(output)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files(iterable):\n",
    "    lst = list(iterable)\n",
    "    random.shuffle(lst)\n",
    "    return iter(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_softmax(tflite_model_file, file_list, batch_size=64):\n",
    "    \n",
    "    output = None\n",
    "    classes = 8\n",
    "    print('Getting softmax output for downstream classes out of Quantized tflite model')\n",
    "    \n",
    "    data_gen = single_epoch_test_data_generator(file_list, batch_size=batch_size)\n",
    "\n",
    "    output = get_softmax_batch_from_tflite(data_gen, tflite_model_file, batch_size, classes=classes)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_files(data_dir, num_files=10):\n",
    "    shortlist_files = []\n",
    "    random.seed(23455)\n",
    "    \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        shortlist_files.append(data_batch_path)\n",
    "        if len(shortlist_files) >= num_files:\n",
    "            break\n",
    "    return shortlist_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False\n",
    "if test:\n",
    "    out_file = 'selected_audio_files_cmsis_mel.npz'\n",
    "    tflite_model_file = '/scratch/sk7898/quantization/pipeline_cmsis_mels/tf_2_full_quantized_default_float32.tflite'\n",
    "\n",
    "    test_data_dir = '/beegfs/dr2915/sonyc_ust/db_mels/test'\n",
    "    shortlist_files = get_test_files(test_data_dir)\n",
    "    np.savez(out_file, x=shortlist_files)\n",
    "\n",
    "    files = np.load(out_file)\n",
    "    shortlist_files = files['x']\n",
    "    print(shortlist_files)\n",
    "\n",
    "    output = gen_softmax(tflite_model_file, shortlist_files, batch_size=91)\n",
    "    pred = np.array(output).reshape(-1, 8)\n",
    "    pred_max = np.argmax(pred, axis=-1) \n",
    "    print(pred_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
