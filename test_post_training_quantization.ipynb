{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files(iterable):\n",
    "    lst = list(iterable)\n",
    "    random.shuffle(lst)\n",
    "    return iter(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_data_generator(data_dir, batch_size=512, samp_rate=48000, n_fft=2048, \\\n",
    "                         n_mels=256, mel_hop_length=242, hop_size=0.1, fmax=None,\\\n",
    "                         random_state=None, start_batch_idx=None):\n",
    "\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "        \n",
    "    frame_length = samp_rate * 1\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "       \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = h5py.File(data_batch_path, 'r')\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['audio'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch = pcm2float(batch, dtype='float32')\n",
    "                    else:\n",
    "                        batch = resample(pcm2float(batch, dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "\n",
    "                    X = [get_melspectrogram(batch[i].flatten(), n_fft=n_fft, mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels, fmax=fmax) for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    return batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_test_data_generator(data_dir, epoch_size, **kwargs):\n",
    "    for _ in range(epoch_size):\n",
    "        x = quant_data_generator(data_dir, **kwargs)\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_to_tflite(model_path, output_path):\n",
    "    keras.backend.clear_session()\n",
    "    l3model = keras.models.load_model(model_path)\n",
    "    embed_layer = l3model.get_layer('audio_embedding_layer')\n",
    "    pool_size = tuple(embed_layer.get_output_shape_at(0)[1:3])\n",
    "    y_a = keras.layers.MaxPooling2D(pool_size=pool_size, padding='same')(l3model.output)\n",
    "    \n",
    "    model = keras.models.Model(inputs=l3model.input, outputs=y_a)\n",
    "    model.save(output_path)\n",
    "    \n",
    "    emb_len = embed_layer.get_output_shape_at(0)[-1]\n",
    "    return model, emb_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_keras_to_tflite(tflite_model_file, keras_model_path, quant_mode='default',\\\n",
    "                             calibrate_data_dir=None, num_calibration_steps=1024):\n",
    "    \n",
    "    def representative_dataset_gen():\n",
    "            l3_model = os.path.dirname(tflite_model_file)\n",
    "            splits = l3_model.split('_')\n",
    "            samp_rate = int(splits[3])\n",
    "            n_mels = int(splits[4])\n",
    "            mel_hop_length = int(splits[5])\n",
    "            n_fft = int(splits[-1])\n",
    "\n",
    "            print('Calibrating.........')\n",
    "            for _ in range(num_calibration_steps):\n",
    "                x = quant_data_generator(calibrate_data_dir, batch_size=1,\\\n",
    "                                         samp_rate=samp_rate, n_fft=n_fft,\\\n",
    "                                         n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "                yield [np.array(x).astype(np.float32)]\n",
    "                \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\n",
    "    \n",
    "    if quant_mode == 'default':\n",
    "        if calibrate_data_dir is None:\n",
    "            raise ValueError('Quantized activation calibration needs data directory!')\n",
    "              \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        \n",
    "    elif quant_mode == 'size':\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_model_file, \"wb\") as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch_from_tflite(data_gen, tflite_model_file, epoch_size, batch_size, emb_len=512):\n",
    "    \n",
    "    predictions = np.zeros(shape=(epoch_size, batch_size, emb_len))\n",
    "    #original_embeddings = np.zeros(shape=(epoch_size, batch_size, emb_len))\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "\n",
    "    interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "                \n",
    "    #predictions per batch   \n",
    "    for idx, batch_x in enumerate(data_gen):\n",
    "        x = np.array(batch_x).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        flattened_output = np.reshape(output, (output.shape[0], output.shape[-1]))\n",
    "        predictions[idx, :, :] = flattened_output\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(model_path, calibrate_data_dir, quant_mode='default',\\\n",
    "                               calibration_steps=1024, batch_size=64, epoch_size=1024):\n",
    "    output = None\n",
    "    \n",
    "    #1. Convert l3model to keras model for quantization (with maxpooling layer but flatten removed)\n",
    "    dir_prefix = '/scratch/sk7898/quantization/' + os.path.basename(model_path).strip('.h5')\n",
    "    \n",
    "    if not os.path.isdir(dir_prefix):\n",
    "        os.makedirs(dir_prefix)\n",
    "    \n",
    "    print('Saving keras model for Quantization')\n",
    "    keras_model_path = os.path.join(dir_prefix, 'for_quantization.h5')    \n",
    "    keras_model, embedding_length = keras_to_tflite(model_path, keras_model_path)\n",
    "    print(keras_model.summary())\n",
    "    print(embedding_length)\n",
    "    \n",
    "    #2.1 Convert keras to tflite model\n",
    "    #2.2 Quantize model with mode 'default' for only weights quantization or 'size' for full quantization\n",
    "    #2.3 Save the quantized tflite model\n",
    "    \n",
    "    print('Quantizing keras model and saving as tflite')\n",
    "    tflite_model_file = os.path.join(dir_prefix, 'quantized_model_'+ quant_mode + '.tflite')\n",
    "    \n",
    "    quantize_keras_to_tflite(tflite_model_file, keras_model_path, quant_mode=quant_mode,\\\n",
    "                             calibrate_data_dir=calibrate_data_dir, num_calibration_steps=calibration_steps)\n",
    "    \n",
    "    print('Getting embedding out of Quantized tflite model')\n",
    "    #3. Predict output i.e. embeddings out of the quantized model\n",
    "    splits = os.path.basename(model_path).strip('.h5').split('_')\n",
    "    samp_rate = int(splits[3])\n",
    "    n_mels = int(splits[4])\n",
    "    mel_hop_length = int(splits[5])\n",
    "    n_fft = int(splits[-1])\n",
    "    \n",
    "    #data_gen = single_epoch_test_data_generator(calibrate_data_dir, epoch_size,\\\n",
    "    #                                            batch_size=batch_size, samp_rate=samp_rate,\\\n",
    "    #                                            n_fft=n_fft, n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "\n",
    "    #output = get_embeddings_batch_from_tflite(data_gen, tflite_model_file,\\\n",
    "    #                                          epoch_size, batch_size, emb_len=embedding_length)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving keras model for Quantization\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 256, 199, 1)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 256, 199, 1)       4         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 256, 199, 64)      640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 256, 199, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 256, 199, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 256, 199, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 256, 199, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 256, 199, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 128, 99, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 128, 99, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 128, 99, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 128, 99, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 128, 99, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 128, 99, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 128, 99, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 64, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 64, 49, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 64, 49, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 64, 49, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 64, 49, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 64, 49, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 64, 49, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 32, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 32, 24, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 32, 24, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 32, 24, 512)       0         \n",
      "_________________________________________________________________\n",
      "audio_embedding_layer (Conv2 (None, 32, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 4,689,860\n",
      "Trainable params: 4,687,042\n",
      "Non-trainable params: 2,818\n",
      "_________________________________________________________________\n",
      "None\n",
      "512\n",
      "Quantizing keras model and saving as tflite\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "INFO:tensorflow:Froze 48 variables.\n",
      "INFO:tensorflow:Converted 48 variables to const ops.\n",
      "Getting embedding out of Quantized tflite model\n"
     ]
    }
   ],
   "source": [
    "model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "calibrate_data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "calibration_steps = 1024\n",
    "#For embedding extraction\n",
    "batch_size = 2\n",
    "epoch_size = 2\n",
    "quant_mode = 'size'\n",
    "\n",
    "embeddings = post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode, \\\n",
    "                                        calibration_steps=calibration_steps,\\\n",
    "                                        batch_size=batch_size, epoch_size=epoch_size)\n",
    "#print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
