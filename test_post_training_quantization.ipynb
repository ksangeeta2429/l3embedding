{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from l3embedding.audio import pcm2float\n",
    "from resampy import resample\n",
    "import pescador\n",
    "from skimage import img_as_float\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files(iterable):\n",
    "    lst = list(iterable)\n",
    "    random.shuffle(lst)\n",
    "    return iter(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cnn_L3_melspec2_spec_model(n_mels=256, n_hop = 242, n_dft = 2048,\n",
    "                                         fmax=None, asr = 48000, halved_convs=False, audio_window_dur = 1):\n",
    "    \n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    n_frames = 1 + int((asr * audio_window_dur) / float(n_hop))\n",
    "    x_a = Input(shape=(n_mels, n_frames, 1), dtype=np.float32)\n",
    "    y_a = BatchNormalization()(x_a)\n",
    "\n",
    "    # CONV BLOCK 1\n",
    "    n_filter_a_1 = np.uint8(64)\n",
    "    if halved_convs:\n",
    "        n_filter_a_1 //= 2\n",
    "\n",
    "    filt_size_a_1 = (np.uint8(3), np.uint8(3))\n",
    "    pool_size_a_1 = (np.uint8(2), np.uint8(2))\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_1, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 2\n",
    "    n_filter_a_2 = np.uint8(128)\n",
    "    if halved_convs:\n",
    "        n_filter_a_2 //= 2\n",
    "\n",
    "    filt_size_a_2 = (np.uint8(3), np.uint8(3))\n",
    "    pool_size_a_2 = (np.uint8(2), np.uint8(2))\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = MaxPooling2D(pool_size=pool_size_a_2, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 3\n",
    "    n_filter_a_3 = np.uint16(256)\n",
    "    if halved_convs:\n",
    "        n_filter_a_3 //= 2\n",
    "\n",
    "    filt_size_a_3 = (np.uint8(3), np.uint8(3))\n",
    "    pool_size_a_3 = (np.uint8(2), np.uint8(2))\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    \n",
    "    if y_a.shape[-2] >= 2:\n",
    "        y_a = MaxPooling2D(pool_size=pool_size_a_3, strides=2)(y_a)\n",
    "\n",
    "    # CONV BLOCK 4\n",
    "    n_filter_a_4 = np.uint16(512)\n",
    "    if halved_convs:\n",
    "        n_filter_a_4 //= 2\n",
    "\n",
    "    filt_size_a_4 = (3, 3)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    y_a = BatchNormalization()(y_a)\n",
    "    y_a = Activation('relu')(y_a)\n",
    "    y_a = Conv2D(n_filter_a_4, filt_size_a_4,\n",
    "                 kernel_initializer='he_normal',\n",
    "                 name='audio_embedding_layer', padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay))(y_a)\n",
    "    \n",
    "    m = Model(inputs=x_a, outputs=y_a)\n",
    "    m.name = 'audio_model'\n",
    "\n",
    "    return m, x_a, y_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_data_generator(data_dir, batch_size=512, samp_rate=48000, n_fft=2048, \\\n",
    "                         n_mels=256, mel_hop_length=242, hop_size=0.1, fmax=None,\\\n",
    "                         random_state=None, start_batch_idx=None):\n",
    "\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "        \n",
    "    frame_length = samp_rate * 1\n",
    "\n",
    "    batch = None\n",
    "    curr_batch_size = 0\n",
    "    batch_idx = 0\n",
    "       \n",
    "    for fname in shuffle_files(os.listdir(data_dir)):\n",
    "        print(fname)\n",
    "        data_batch_path = os.path.join(data_dir, fname)\n",
    "        blob_start_idx = 0\n",
    "\n",
    "        data_blob = h5py.File(data_batch_path, 'r')\n",
    "        blob_size = len(data_blob['audio'])\n",
    "\n",
    "        while blob_start_idx < blob_size:\n",
    "            blob_end_idx = min(blob_start_idx + batch_size - curr_batch_size, blob_size)\n",
    "\n",
    "            # If we are starting from a particular batch, skip computing all of\n",
    "            # the prior batches\n",
    "            if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                if batch is None:\n",
    "                    batch = data_blob['audio'][blob_start_idx:blob_end_idx]\n",
    "                else:\n",
    "                    batch = np.concatenate([batch, data_blob['audio'][blob_start_idx:blob_end_idx]])\n",
    "\n",
    "            curr_batch_size += blob_end_idx - blob_start_idx\n",
    "            blob_start_idx = blob_end_idx\n",
    "\n",
    "            if blob_end_idx == blob_size:\n",
    "                data_blob.close()\n",
    "\n",
    "            if curr_batch_size == batch_size:\n",
    "                X = []\n",
    "                # If we are starting from a particular batch, skip yielding all\n",
    "                # of the prior batches\n",
    "                if start_batch_idx is None or batch_idx >= start_batch_idx:\n",
    "                    # Convert audio to float\n",
    "                    if(samp_rate==48000):\n",
    "                        batch = pcm2float(batch, dtype='float32')\n",
    "                    else:\n",
    "                        batch = resample(pcm2float(batch, dtype='float32'), sr_orig=48000,\n",
    "                                                  sr_new=samp_rate)\n",
    "\n",
    "                    X = [get_melspectrogram(batch[i].flatten(), n_fft=n_fft, mel_hop_length=mel_hop_length,\\\n",
    "                                            samp_rate=samp_rate, n_mels=n_mels, fmax=fmax) for i in range(batch_size)]\n",
    "\n",
    "                    batch = np.array(X)[:, :, :, np.newaxis]\n",
    "                    #print(np.shape(batch)) #(64, 256, 191, 1)\n",
    "                    return batch\n",
    "\n",
    "                batch_idx += 1\n",
    "                curr_batch_size = 0\n",
    "                batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_test_data_generator(data_dir, epoch_size, **kwargs):\n",
    "    for _ in range(epoch_size):\n",
    "        x = quant_data_generator(data_dir, **kwargs)\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_for_tflite(model_path, output_path):\n",
    "    keras.backend.clear_session()\n",
    "    keras.backend.set_learning_phase(0)\n",
    "    \n",
    "    l3model = keras.models.load_model(model_path)\n",
    "    n_model = construct_cnn_L3_melspec2_spec_model(n_mels=256, n_hop=242, n_dft=2048,\\\n",
    "                                                 fmax=None, asr=48000, halved_convs=False, audio_window_dur=1)\n",
    "    n_model.set_weights(l3model.get_weights)\n",
    "    \n",
    "    embed_layer = l3model.get_layer('audio_embedding_layer')\n",
    "    pool_size = tuple(embed_layer.get_output_shape_at(0)[1:3])\n",
    "    \n",
    "    y_a = keras.layers.MaxPooling2D(pool_size=pool_size, padding='same')(n_model.output)    \n",
    "    model = keras.models.Model(inputs=n_model.input, outputs=y_a)\n",
    "    model.save(output_path)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_keras_to_tflite(tflite_model_file, keras_model_path, quant_mode='default',\\\n",
    "                             calibrate_data_dir=None, num_calibration_steps=1024):\n",
    "\n",
    "    def representative_dataset_gen():\n",
    "            l3_model = os.path.dirname(tflite_model_file)\n",
    "            splits = l3_model.split('_')\n",
    "            samp_rate = int(splits[3])\n",
    "            n_mels = int(splits[4])\n",
    "            mel_hop_length = int(splits[5])\n",
    "            n_fft = int(splits[-1])\n",
    "\n",
    "            print('Calibrating.........')\n",
    "            for _ in range(num_calibration_steps):\n",
    "                x = quant_data_generator(calibrate_data_dir, batch_size=1,\\\n",
    "                                         samp_rate=samp_rate, n_fft=n_fft,\\\n",
    "                                         n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "                yield [np.array(x).astype(np.float32)]\n",
    "                \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\n",
    "    \n",
    "    if quant_mode == 'default':\n",
    "        if calibrate_data_dir is None:\n",
    "            raise ValueError('Quantized activation calibration needs data directory!')\n",
    "        \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.default_ranges_stats = (0, 1)\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        \n",
    "    elif quant_mode == 'size':\n",
    "        converter.post_training_quantize = True\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized Quantization mode!')\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_model_file, \"wb\") as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch_from_tflite(data_gen, tflite_model_file, epoch_size, batch_size, emb_len=512):\n",
    "    \n",
    "    predictions = np.zeros(shape=(epoch_size, batch_size, emb_len))\n",
    "    #original_embeddings = np.zeros(shape=(epoch_size, batch_size, emb_len))\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "\n",
    "    interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "                \n",
    "    #predictions per batch   \n",
    "    for idx, batch_x in enumerate(data_gen):\n",
    "        x = np.array(batch_x).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        flattened_output = np.reshape(output, (output.shape[0], output.shape[-1]))\n",
    "        predictions[idx, :, :] = flattened_output\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding(model_path, tflite_model_file, data_dir, quant_mode='default',\\\n",
    "                  emb_len=512, batch_size=64, epoch_size=1024):\n",
    "    \n",
    "    output = None\n",
    "    print('Getting embedding out of Quantized tflite model')\n",
    "    splits = os.path.basename(model_path).strip('.h5').split('_')\n",
    "    samp_rate = int(splits[3])\n",
    "    n_mels = int(splits[4])\n",
    "    mel_hop_length = int(splits[5])\n",
    "    n_fft = int(splits[-1])\n",
    "    \n",
    "    data_gen = single_epoch_test_data_generator(data_dir, epoch_size,\\\n",
    "                                                batch_size=batch_size, samp_rate=samp_rate,\\\n",
    "                                                n_fft=n_fft, n_mels=n_mels, mel_hop_length=mel_hop_length)\n",
    "\n",
    "    output = get_embeddings_batch_from_tflite(data_gen, tflite_model_file,\\\n",
    "                                              epoch_size, batch_size, emb_len=emb_len)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quantization(model_path, calibrate_data_dir, quant_mode='default',\\\n",
    "                               calibration_steps=1024):\n",
    "    \n",
    "    #1. Convert l3model to keras model for quantization (with maxpooling layer but flatten removed)\n",
    "    dir_prefix = '/scratch/sk7898/quantization/' + os.path.basename(model_path).strip('.h5')\n",
    "    \n",
    "    if not os.path.isdir(dir_prefix):\n",
    "        os.makedirs(dir_prefix)\n",
    "    \n",
    "    #print('Saving keras model for Quantization')\n",
    "    keras_model_path = os.path.join(dir_prefix, 'for_quantization.h5')    \n",
    "    #keras_model = keras_for_tflite(model_path, keras_model_path)\n",
    "    \n",
    "    #2.1 Convert keras to tflite model\n",
    "    #2.2 Quantize model with mode 'default' for only weights quantization or 'size' for full quantization\n",
    "    #2.3 Save the quantized tflite model\n",
    "    \n",
    "    print('Quantizing keras model and saving as tflite')\n",
    "    tflite_model_file = os.path.join(dir_prefix, 'quantized_model_copy_'+ quant_mode + '.tflite')\n",
    "    \n",
    "    quantize_keras_to_tflite(tflite_model_file, keras_model_path, quant_mode=quant_mode,\\\n",
    "                             calibrate_data_dir=calibrate_data_dir, num_calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing keras model and saving as tflite\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 48 variables.\n",
      "INFO:tensorflow:Converted 48 variables to const ops.\n",
      "Calibrating.........\n",
      "20185094_2_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/librosa/filters.py:284: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20187019_3_42.h5\n",
      "20180202_4_118.h5\n",
      "20180145_3_35.h5\n",
      "20183056_0_11.h5\n",
      "20180258_4_165.h5\n",
      "20187045_1_32.h5\n",
      "20180158_0_107.h5\n",
      "20180190_0_164.h5\n",
      "20180177_3_77.h5\n"
     ]
    }
   ],
   "source": [
    "model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "calibrate_data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "calibration_steps = 10\n",
    "quant_mode = 'default'\n",
    "\n",
    "post_training_quantization(model_path, calibrate_data_dir, quant_mode=quant_mode, \\\n",
    "                           calibration_steps=calibration_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embedding out of Quantized tflite model\n",
      "== Input details ==\n",
      "{'name': 'input_13', 'index': 33, 'shape': array([ 64, 256, 199,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'max_pooling2d_1/MaxPool', 'index': 34, 'shape': array([ 64,   1,   1, 512], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "20183047_3_31.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/librosa/filters.py:284: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    }
   ],
   "source": [
    "model_path = '/scratch/sk7898/l3pruning/embedding/fixed/reduced_input/l3_audio_original_48000_256_242_2048.h5'\n",
    "tflite_model_file = '/scratch/sk7898/quantization/l3_audio_original_48000_256_242_2048/quantized_model_default.tflite'\n",
    "data_dir = '/beegfs/work/AudioSetSamples/music_train'\n",
    "\n",
    "embeddings = gen_embedding(model_path, tflite_model_file, data_dir, quant_mode='default',\\\n",
    "                           batch_size=64, epoch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
